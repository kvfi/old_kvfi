---
layout: post
title: Notes and 
slug: Notes
subtitle: Miscellaneous
published: 2008-03-09
updated: 2016-12-02
progress: continious
epistemic_state: log
difficulty: 2
category: misc
tags:
 - personal
 - log
toc: true
online: true
---
"Some say that a god lives on in the faith and memory of its believers. They point to computers and say, 'Behold, they need but think all together in a particular & precise mode, and from nowhere appear things real and greater than any they thought. Might not the same be true of humans, who are so much greater?' But this is no more true than a painting of a flower the flower itself."

#### Long term investment

> "That is, from January 1926 through December 2002, when holding periods were 19 years or longer, the cumulative real return on stocks was never negative..."

How does one engage in extremely long investments? On a time-scale of centuries, investment is a difficult task, especially if one seeks to avoid erosion of returns by the costs of active management.

'Unit Investment Trust (UIT) is a US investment company offering a fixed (unmanaged) portfolio of securities having a definite life.'

'A closed-end fund is a collective investment scheme with a limited number of shares'

In long-term investments, one must become concerned about biases in the data used to make decisions. Many of these biases fall under the general rubric of "observer biases" - the canonical example being that stocks look like excellent investments if you only consider America's stock market, where returns over long periods have been quite good. For example, if you had invested by tracking the major indices any time period from January 1926 through December 2002 and had held onto your investment for at least 19 years, you were guaranteed a positive real return. Of course, the specification of place (America) and time period (before the Depression and after the Internet bubble) should alert us that this guarantee may not hold elsewhere. Had a long-term investor in the middle of the 19th century decided to invest in a large up-and-coming country with a booming economy and strong military (much like the United States has been for much of the 20th century), they would have reaped excellent returns. That is, until the hyperinflation of the Weimar Republic. Should their returns have survived the inflation and imposition of a new currency, then the destruction of the 3rd Reich would surely have rendered their shares and Reichmarks worthless. Similarly for another up-and-coming nation - Japan. Mention of Russia need not even be made.

Clearly, diversifying among companies in a sector, or even sectors in a national economy is not enough. Disaster can strike an entire nation. Rosy returns for stocks quietly ignore those bloody years in which exchanges plunged thousands of percentage points in real terms, and whose records burned in the flames of war. Over a timespan of a century, it is impossible to know whether such destruction will be visited on a given country or even whether it will still exist as a unit. How could Germany, the preeminent power on the Continent, with a burgeoning navy rivaling Britain's, with the famous Prussian military and Junkers, with an effective industrial economy still famed for the quality of its mechanisms, and with a large homogeneous population of hardy people possibly fall so low as to be utterly conquered? And by the United States and others, for that matter? How could Japan, with its fanatical warriors and equally fanatical populace, its massive fleet and some of the best airplanes in the world - a combination that had humbled Russia, that had occupied Korea for nigh on 40 years, which easily set up puppet governments in Manchuria and China when and where it pleased - how could it have been defeated so wretchedly as to see its population literally decimated and its governance wholly supplanted? How could a god be dethroned?

It is perhaps not too much to say that investors in the United States, who say that the Treasury Bond has never failed to be redeemed and that the United States can never fall, are perhaps overconfident in their assessment. Inflation need not be hyper to cause losses. Greater nations have been destroyed quickly. Who remembers the days when the Dutch fought the English and the French to a standstill and ruled over the shipping lanes? Remember that Nineveh is one with the dust.

In short, our data on returns is biased. This bias indicates that stocks and cash are much more risky than most people think, and that this risk inheres in exogenous shocks to economies - it may seem odd to invest globally, in multiple currencies, just to avoid the rare black swans of total war and hyperinflation. But these risks are catastrophic risks. Even one may be too many.

This risk is more general. Governments can die, and so their bonds and other instruments (such as cash) rendered worthless; how many governments have died or defaulted over the last century? Many. The default assumption must be that the governments with good credit, who are not in that number, may simply have been lucky. And luck runs out.

In general, entities die unpredictably, and one has no guarantee that a, say, 1500 year old Korean construction company will honor its bills in another 500 years because all it takes is one bubble to drive it into bankruptcy. When one looks at securities turning into money, of course all you see are ones for those entities which survived. This is 'survivorship bias'; our observations are biased because we aren't looking at all of the past, but the present. This can be exploited, however. Obviously if an entity perishes, it has no need for assets.

Suppose one wishes to make a very long-term investment. One groups with a large number of other investors who wish to make similar investments, in a closed-end mutual fund with a share per investor, which is set to liquidate at some remote period. This fund would invest in assets all over the world and of various kinds, seeking great diversification. The key ingredient would be that shares are not allowed to be transferred. Should an investor perish, the value of their share would be split up amongst the other investors' shares (a percentage could be used to pay for management, perhaps). Because of this ingredient, the expected return for any individual investor would be extremely high - the potential loss is 100%, but the investor by definition will never be around for that loss. Because the identity and number of investments is fixed, potential control of the assets could be dispersed among the investors so as to avoid the situation where war destroys the headquarters of whomever is managing the assets. The technical details are unimportant; cryptography has many ingenious schemes for such [secret sharing](!Wikipedia) (one can easily heavily encrypt a file as usual, and then using eg. [Shamir's Secret Sharing](!Wikipedia), create & distribute _n_ keys where any chosen number of keys up to all _n_ are needed to decrypt the file).

From Eliezer Yudkowsky's ["The Apocalypse Bet"](http://lesswrong.com/lw/ie/the_apocalypse_bet/)

> Suppose you think that gold will become worthless on April 27th, 2020 at between four and four-thirty in the morning. I, on the other hand, think this event will not occur until 2030. We can sign a contract in which I pay you one ounce of gold per year from 2010 to 2020, and then you pay me two ounces of gold per year from 2020 to 2030. If gold becomes worthless when you say, you will have profited; if gold becomes worthless when I say, I will have profited. We can have a prediction market on a generic apocalypse, in which participants who believe in an earlier apocalypse are paid by believers in a later apocalypse, until they pass the date of their prediction, at which time the flow reverses with interest. I don't see any way to distinguish between apocalypses, but we can ask the participants why they were willing to bet, and probably receive a decent answer.

Or [Garett Jones](http://econlog.econlib.org/archives/2012/09/how_to_bet_on_b.html "How to bet on bad futures")

> How can these prophets of doom cash in on their confidence? After all, they think that the state of the world where they're proven right is a state of the world where nobody can reward them for being right. Aside from the self-congratulation, how can they benefit? By signing a contract right now.  If you're reasonably sure Treasuries will be worthless in a dozen years, you should find somebody who disagrees, and convince them to give you \$1 today. If you're right, then 12 years later you get to keep the money.  If you're wrong, you have to pay the other party the normal rate of return on the \$1, plus a little extra. \$2 should be plenty if you can back the contract with some collateral; maybe \$4 otherwise. Notice what you're doing here: You're writing an uninsurance policy. You get the premiums up front, and you pay out only if things turn out fine. Since the other party is pretty sure things will turn out fine, the deal you're offering from their point of view is about the same as any other investment. That's why you only have to offer about the normal rate of return. The hard part here, of course, is convincing the other party you'll repay in the future--your barrier to riches isn't the apocalypse, it's *your own [trustworthiness](http://econlog.econlib.org/archives/2012/09/trustworthiness.html)*. That's where attorneys and insurance companies come in. [Lloyd's](!Wikipedia "Lloyd's of London") is happy to offer [hole-in-one insurance](http://www.lloyds.com/News-and-Insight/News-and-Features/Archive/2007/07/A_hole_new_insurance_policy), so the industry has no problem writing policies that pay out upon joyous events.

#### American light novels

I think one of the more interesting trends in anime is the massive number of adaptations of light novels done in the '90s and 00s; it is interesting because no such trend exists in American media as far as I can tell (the closest I can think of are comic book adaptations, but of course those are analogous to the many mangas -> animes). Now, American media absolutely adapts many novels, but they are all normal Serious Business Novels. We do not seem to even have the light novel media - young adult novels do not cut the mustard. light novels are odd as they are kind of like speculative fiction novellas. The success of comic book movies has been much noted - could *comic books* be the American equivalent of light novels? There are attractive similarities in subject matter and even medium, light novels including a fair number of color manga illustrations.

- Question for self: if America doesn't have the light novel category, is that a claim that the _Twilight_ novels, and everything published under the James Patterson brand, are regular novels?

    Answer: The _Twilight_ novels are no more light novels than the _Harry Potter_ novels were. The Patterson novels may fit, however; they have some of the traits such as very short chapters, simple literary style, and very quick moving plots, even though they lack a few less important traits (such as including illustrations). It might be better to say that there is no recognized and successful light novel *genre* rather than individual light novels - there are only unusual examples like the Patterson novels and other works uncomfortable listed under the Young Adult/Teenager rubric.

#### Cultural growth through diversity

Leaving aside the corrosive effects on social solidarity documented by Putnam and Amy Chua's 'market minorities', I've wondered about the *artistic* consequences of substantial diversity to a country or perhaps civilization. In Charles Murray's _[Human Accomplishment](http://www.amazon.com/Human-Accomplishment-Pursuit-Excellence-Sciences/dp/B000A17728/)_, one of the strongest indicators for genius is contact with a foreign culture. This foreign contact can be pretty minimal - Thomas Malthus drew on threadbare descriptions of China's teeming population, and the French _philosophes_ had little more to go on when drawing inspiration in Confucianism, as did the later rococo and _chinoiserie_ artists; much of American design and art traces back to interpretations of East Asian art based on few works, and the sprawling American cults or New Age movements and everything that umbrella term influenced post-'60s were not based on deep scholarship. They did much with little, one might say. This seems fairly true of many fertile periods: the foreigners make up, at most, a few percentage points of the population.

For example, Japanese visual art is pretty mediocre from the 900s to the 1600s - but great between then and the Meiji era. What happened? They shut off access to the outside world, and that's apparently how we got ukiyo-e. But what did the Meiji era, when the doors were flung open to the accumulated treasures of the Western world, ever produce? And to go the other direction: the Impressionists and other artists of the time received trickles of Asian artwork which apparently inspired them (my own single favorite block-print by Hiroshige also exists - as a Van Gogh painting!), but what has happened since as masses and masses of artwork became available?

However, the modern era which is likely the most globalized era ever for cultural products, in which population movements are so much vaster and in which English-speakers have access to primary sources like they have never had before (compare how much classic Japanese & Chinese literature has been translated and stored in libraries as of 2009 to what was available when Waley began translating _Genji Monogatari_ in 1921!). This would seem to be something of a contradiction: if a little foreign contact was enough to inspire all the foregoing, then why wouldn't all the Asian immigrants and translations and economic contact to America spark even greater revolutions? There has been influence, absolutely; but the influence is striking for how a little bit helped (how many haiku did the Imagists have access to?) and a lot done not much more, and perhaps even less. There's no obvious reason that more would not be better, and obvious reasons why it would be (less overhead and isolation for the foreigners; sheer better odds of getting access to the right master or specialist that a promising native artist needs). But nevertheless, I seem to discern a U-shaped curve.

> "We are doubtless deluding ourselves with a dream when we think that equality and fraternity will some day reign among human beings without compromising their diversity. However, if humanity is not resigned to becoming the sterile consumer of values that it managed to create in the past...capable only of giving birth to bastard works, to gross and puerile inventions, [then] it must learn once again that all true creation implies a certain deafness to the appeal of other values, even going so far as to reject them if not denying them altogether. For one cannot fully enjoy the other, identify with him, and yet at the same time remain different. When integral communication with the other is achieved completely, it sooner or later spells doom for both his and my creativity. The great creative eras were those in which communication had become adequate for mutual stimulation by remote partners, yet was not so frequent or so rapid as to endanger the indispensable obstacles between individuals and groups or to reduce them to the point where overly facile exchanges might equalize and nullify their diversity." --[Claude Levi-Strauss](!Wikipedia), [_The View from Afar_](http://www.amazon.com/The-View-Afar-Claude-Levi-Strauss/dp/0226474747/) pg 23 (quoted in Clifford Geertz's ["The Uses of Diversity"](https://web.archive.org/web/20130128123347/http://tannerlectures.utah.edu/lectures/documents/geertz86.pdf) [Tanner Lecture](!Wikipedia))

And so on. It seems cultures benefit most from cross-pollination when there's only a little, but globalization is forcing contact way beyond that, if you follow me.

In schools, one sees students move in cliques and especially so with students who share a native language and are non-native English speakers - one can certainly understand why they would do such a thing, or why immigrants would congregate in ghettos or Chinatowns or Koreatowns where they can speak freely and talk of the old country; perhaps this homophily drives the reduced cross-fertilizing by reducing the chances of crossing paths. (If one is the only Yid around, one must interact with many goyim, but not so if there are many others around.) Is this enough? It doesn't seem like enough to me.

This is a little perplexing. What's the explanation? Could it be that as populations build up, all the early artists sucked out the novelty available in hybridizing native material with the foreign material? Or is there something stimulating about having only a few examples - does one draw faulty but fruitful inferences based on idiosyncrasies of the small data set? In machine learning, the more data available, the less wild the guesses are, but in art, wildness is a way of jumping out of a local minima to somewhere new. If Yeats had available the entire Chinese corpus, would he produce better new English poems than when he pondered obsessively a few hundred verses, or would he simply produce better English pastiches of Chinese poems? Knowledge can be a curse by making it difficult or impossible to think new thoughts and see new angles. Or perhaps the foreign material is important only as a *hint* to what the artist was trying already to achieve; in psychology, there is an interesting variant on the [cocktail party effect](!Wikipedia) 'key' effect where one hears only static noise in a recording, is given a hint at the sentence spoken in the recording, and then one can suddenly hear it through the noise. Perhaps the original aims are entirely unimportant, and what is at play is a sort of [pareidolia](!Wikipedia) or [apophenia](!Wikipedia) (akin to [electronic voice phenomenon](!Wikipedia), to continue the sound metaphor).

> `Question:` "How much effort should go into library work?"
>
> `Hamming:` "It depends upon the field. I will say this about it. There was a fellow at Bell Labs, a very, very, smart guy. He was always in the library; he read everything. If you wanted references, you went to him and he gave you all kinds of references. But in the middle of forming these theories, I formed a proposition: there would be no effect named after him in the long run. He is now retired from Bell Labs and is an Adjunct Professor. He was very valuable; I'm not questioning that. He wrote some very good Physical Review articles; but there's no effect named after him because he read too much. If you read all the time what other people have done you will think the way they thought. If you want to think new thoughts that are different, then do what a lot of creative people do - get the problem reasonably clear and then refuse to look at any answers until you've thought the problem through carefully how you would do it, how you could slightly change the problem to be the correct one. So yes, you need to keep up. You need to keep up more to find out what the problems are than to read to find the solutions. The reading is necessary to know what is going on and what is possible. But reading to get the solutions does not seem to be the way to do great research. So I'll give you two answers. You read; but it is not the amount, it is the way you read that counts."^[Richard Hamming, ["You and Your Research"](http://www.cs.virginia.edu/~robins/YouAndYourResearch.html)]

#### Decluttering

[Ego depletion](!Wikipedia):

> Ego depletion refers to the idea that self-control and other mental processes that require focused conscious effort rely on energy that can be used up. When that energy is low (rather than high), mental activity that requires self-control is impaired. In other words, using one's self-control impairs the ability to control one's self later on. In this sense, the idea of (limited) willpower is correct.

Wonder whether this has any connection with minimalism? Clutter might damage [executive functions](!Wikipedia); [Killingsworth & Gilbert 2010](http://www.wjh.harvard.edu/~dtg/KILLINGSWORTH%20&%20GILBERT%20(2010).pdf "A Wandering Mind Is An Unhappy Mind") correlated distraction with later unhappiness, and from ["Henry Morton Stanley's Unbreakable Will"](http://www.smithsonianmag.com/history-archaeology/Henry-Morton-Stanleys-Unbreakable-Will.html?c=y&story=fullstory), Roy F. Baumeister and John Tierney:

> You might think the energy spent shaving in the jungle would be better devoted to looking for food. But [Stanley's](!Wikipedia "Henry Morton Stanley") belief in the link between external order and inner self-discipline has been confirmed recently in studies. In one experiment, a group of participants answered questions sitting in a nice neat laboratory, while others sat in the kind of place that inspires parents to shout, "Clean up your room!" The people in the messy room scored lower self-control, such as being unwilling to wait a week for a larger sum of money as opposed to taking a smaller sum right away. When offered snacks and drinks, people in the neat lab room more often chose apples and milk instead of the candy and sugary colas preferred by their peers in the pigsty.
>
> In a similar experiment online, some participants answered questions on a clean, well-designed website. Others were asked the same questions on a sloppy website with spelling errors and other problems. On the messy site, people were more likely to say that they would gamble rather than take a sure thing, curse and swear, and take an immediate but small reward rather than a larger but delayed reward. The orderly websites, like the neat lab rooms, provided subtle cues guiding people toward self-disciplined decisions and actions helping others.

Paul Graham, ["Stuff"](http://www.paulgraham.com/stuff.html):

> For example, in my house in Cambridge, which was built in 1876, the bedrooms don't have closets. In those days people's stuff fit in a chest of drawers. Even as recently as a few decades ago there was a lot less stuff. When I look back at photos from the 1970s, I'm surprised how empty houses look. As a kid I had what I thought was a huge fleet of toy cars, but they'd be dwarfed by the number of toys my nephews have. All together my Matchboxes and Corgis took up about a third of the surface of my bed. In my nephews' rooms the bed is the only clear space. Stuff has gotten a lot cheaper, but our attitudes toward it haven't changed correspondingly. We overvalue stuff.
>
> ...And unless you're extremely organized, a house full of stuff can be very depressing. A cluttered room saps one's spirits. One reason, obviously, is that there's less room for people in a room full of stuff. But there's more going on than that. I think humans constantly scan their environment to build a mental model of what's around them. And the harder a scene is to parse, the less energy you have left for conscious thoughts. A cluttered room is literally exhausting. (This could explain why clutter doesn't seem to bother kids as much as adults. Kids are less perceptive. They build a coarser model of their surroundings, and this consumes less energy.)...Another way to resist acquiring stuff is to think of the overall cost of owning it. The purchase price is just the beginning. You're going to have to *think* about that thing for years-perhaps for the rest of your life. Every thing you own takes energy away from you. Some give more than they take. Those are the only things worth having.

Michael Lewis, ["Obama's Way"](https://web.archive.org/web/20130201053308/http://www.vanityfair.com/politics/2012/10/michael-lewis-profile-barack-obama):

> This time he covered a lot more ground and was willing to talk about the mundane details of presidential existence. "You have to exercise," he said, for instance. "Or at some point you'll just break down." You also need to remove from your life the day-to-day problems that absorb most people for meaningful parts of their day. "You'll see I wear only gray or blue suits," he said. "I'm trying to pare down decisions. I don't want to make decisions about what I'm eating or wearing. Because I have too many other decisions to make." He mentioned research that shows the simple act of making decisions degrades one's ability to make further decisions. It's why shopping is so exhausting. "You need to focus your decision-making energy. You need to routinize yourself. You can't be going through the day distracted by trivia."

It's striking how cluttered a big city is when you visit them from a rural area; it's also striking how mental disease seems to [correlate with cities](The Melancholy of Subculture Society#fn29) and how mental performance improves with natural vista and not urban vistas.

See also [latent inhibition](!Wikipedia):

> Latent inhibition is a process by which exposure to a stimulus of little or no consequence prevents conditioned associations with that stimulus being formed. The ability to disregard or even inhibit formation of memory, by preventing associative learning of observed stimuli, is an automatic response and is thought to prevent information overload. Latent inhibition is observed in many species, and is believed to be an integral part of the observation/learning process, to allow the self to interact successfully in a social environment.

> Most people are able to shut out the constant stream of incoming stimuli, but those with low latent inhibition cannot. It is hypothesized that a low level of latent inhibition can cause either psychosis, a high level of creativity[1] or both, which is usually dependent on the subject's intelligence.[2][3] Those of above average intelligence are thought to be capable of processing this stream effectively, an ability that greatly aids their creativity and ability to recall trivial events in incredible detail and which categorizes them as almost creative geniuses. Those with less than average intelligence, on the other hand, are less able to cope, and so as a result are more likely to suffer from mental illness.

Interesting decluttering approach: "100 Things Challenge"

* <https://web.archive.org/web/20100301094213/http://www.guynameddave.com/100-thing-challenge.html>
* <http://www.time.com/time/magazine/article/0,9171,1812048,00.html>
* <http://www.denverpost.com/room/ci_8060057>

#### _The Count of Zarathustra_

The count of Monte Cristo as a Nietzschean hero?

#### Title

Good poem title: 'The Scarecrow Appeals to Glenda the Good'

#### Idea for Twitter SF

novel idea: an ancient British family has a 144 character (no spaces) string which encodes the political outcomes of the future eg. the restoration, the Glorious Rebellion, Napoleon, Nazis etc. thus the family has been able to pick the winning side every time and maintain its place. but they cannot interpret the remaining characters pertaining to our time. they hire researcher/librarians to crack it. one of them is our narrator. in the course of figuring it out, he becomes one of the sides mentioned. possible plot device: he has a corrupted copy?

#### Somatic genetic engineering

What's the killer app for non-medical genetic engineering in humans?

How about germ-line engineering of hair color? think about it. hair color is controlled by relatively few, and well-understood, genes. hair color is a dramatic change. there is massive demand for hair dye as it is, even with the extra effort and impermanence and unsatisfactory results. how many platinum blonds would jump at the chance to have kids who are *truly* enviably blond? or richly red-headed (and not washed-out Irish red)? A heck of a lot, I'd say. The health risks need not be enormous - aside from the intervention itself, what risk could swapping a brunette gene for blond cause? (There apparently is just 1 relevant gene: "Frost's theory is also backed up by a separate scientific analysis of north European genes carried out at three Japanese universities, which has isolated the date of the genetic mutation that resulted in blond hair to about 11,000 years ago." --["Corrected-Cavegirls were first blondes to have fun"](https://web.archive.org/web/20070206034450/http://www.timesonline.co.uk/tol/news/uk/article735078.ece))

What sort of market could we expect? [Demographics of the United States](!Wikipedia) lists 103,129,321 women between 15 and 64; these are women who could be using dye themselves, so appreciate the benefit, and are of child-bearing years.

Likely, the treatment will only work if there's natural variation to begin with - that is, for Caucasians only. We'll probably want to exclude Hispanics and Latin Americans, who are almost as homogeneous in hair color as blacks and Asians, so that leaves us 66% of the total US population. 66% * 103,129,321 will get us a rough estimate of 6.806535186e7 or 68,065,351.

<http://www.isteve.com/blondes.htm> claims that "One study estimated that of the 30% of North American women who are blonde, 5/6^ths^ had some help from a bottle." (0.3 * (5/6) = 0.25 or 25%) says
[Demographics of Mexico](!Wikipedia) says 53,013,433 females
[Canada 2006 Census#Age and sex](!Wikipedia) 16,136,925

or 172,279,679 when you sum up Mexico/Canada/USA (the remaining NA states are too small to care about); 25% of 172,279,679 is 43,069,919. 43 million dye users.

Here's a random report <http://www.researchandmarkets.com/reportinfo.asp?report_id=305358> saying hair dye is worth 1 billion USD a year. Let's assume that this is all consumed domestically by women. (So 1,000,000,000 / 43,069,919 per year is 23)

A woman using hair dye on a permanent basis will be dying every month or so, or 12 times a year. Assume that one dye job is ~20 USD* (she's not doing it herself); then ((1b / 20) / 12) gives us ~4,166,666 women using hair dye, or 1/24 or 4.1% of eligible women. This seems rather low to me, based on observations, but I suppose it may be that elderly women do not use much hair dye, or the trend to using highlights and less-than-complete dye jobs. But 4% seems like a rather safe lower end. That's a pretty large market - 4 million potential customers, who are regularly expressing their financial commitment to their desire to have some hair color other than their natural one.

If each is spending even 100$ a year on it, a genetic engineering treatment could pay for itself very quickly. At 1000$, just 10 years. (And women can expect to live ~80). Not to mention, one would expect the natural hair to simply look better than the dye job.

There's a further advantage to this: it seems reasonable to expect that early forms of this sort of therapy will simply not work for minorities such as blacks or Hispanics - their markets wouldn't justify the research to make it work for them; their dark hair colors seem to be very dominant genetically, and likely the therapy would be working with recessive alleles (at least, it seems intuitively plausible that there is less 'distance' between making a Caucasian embryo, who might even have a recessive blonde allele already, fully blond, as compared to making a black baby, who would never ever come anywhere near a non-black hair color, blond). So marketing would benefit from an implicit racism and classism: racism in that one might need to be substantially Caucasian to benefit, and classism to be able to pony up the money up front.

* I think this price is a low-ball estimate by at least 50%; hopefully it will give us a margin of error, since I'm not sure how often dye-jobs need to be done.

#### The Camel Has Two Humps

Why does the camel have 2 humps? ["The camel has two humps (working title)"](http://wiki.t-o-f.info/uploads/EDM4600/The%20camel%20has%20two%20humps.pdf "Dehnadi & Bornat 2006"): "All teachers of programming find that their results display a 'double hump'. It is as if there are two populations: those who can, and those who cannot, each with its own independent bell curve."

Attractive as this idea is, Alan Kay seems [a little skeptical](http://www.secretgeek.net/camel_kay.asp "Alan Kay on 'The Camel has Two Humps'") and replications of the test have had issues; from ["Mental models, Consistency and Programming Aptitude"](http://crpit.com/confpapers/CRPITV78Bornat.pdf "Bornat et al 2008"):

> We now report that after six experiments, involving more than 500 students at six institutions in three countries, the predictive effect of our test has failed to live up to that early promise.

And ["Mental Models and Programming Aptitude"](http://www.daimi.au.dk/~mec/publications/conference/23--iticse2007.pdf "Caspersen et al 2007") & ["Meta-analysis of the effect of consistency on success in early learning of programming"](http://www.eis.mdx.ac.uk/research/PhDArea/saeed/SD_PPIG_2009.pdf "Dehnadi et al 2009")

> A test was designed that apparently examined a student's knowledge of assignment and sequence before a first course in programming but in fact was designed to capture their reasoning strategies. An experiment found two distinct populations of students: one could build and consistently apply a mental model of program execution; the other appeared either unable to build a model or to apply one consistently. The first group performed very much better in their end-of-course examination than the second in terms of success or failure. The test does not very accurately predict levels of performance, but by combining the result of six replications of the experiment, five in UK and one in Australia. We show that consistency does have a strong effect on success in early learning to program but background programming experience, on the other hand, has little or no effect.

Nevertheless, I think there's something to this: your first computer language is really hard no matter your experience but the second is almost trivial, unless it's a truly alien paradigm. (This is interestingly different from growing up learning a natural language, where the first is very easy but the second is very difficult unless also learned in childhood.)

This suggests some questions to me. Obviously on a raw information level, a natural language is *much* more complex than a computer language (the former are almost indefinitely complex with vocabulary, and the latter are engineered to be simple). Is it, relatively speaking, easier to learn a second natural language, or a second computer language? That is, if the difficulty of learning a second computer language is perhaps 10% of learning the first language, is that better or worse than the difficulty of learning a second natural language after one's native language? My own impression is that learning Haskell after I knew some Java was a lot easier than my first attempt at learning Haskell; when I learned some French after learning Haskell, it seemed easier than before but not *that* much easier. If this is so, it suggests that computer languages share more deep similarities than natural languages. What is the knack of programming? Why do people never seem to cease being programmers - what irreversible paradigm shift happens in their heads?

#### The advantage of an uncommon name

Theory: as time passes, it becomes more and more costly to have a 'common' name: a name which frequently appears either in history or in born-digital works. In the past, having a name like 'John Smith' may have not been a disadvantage - connections were personal, no one confused one John Smith with another, textual records were only occasionally used. It might sometimes be an issue with bureaucracy such as taxes or the legal system, but nowhere else.

But online, it is important to be findable. You want your friends on Facebook to find you with the first hit. You want potential employers doing surreptitious Google searches before an interview to see your accomplishments and not others' demerits; you do not want, as Abigail Garvey discovered when she married a Wilson, [employers thinking your resume fraudulent](http://online.wsj.com/article/SB117856222924394753.html) because you are no longer ranking highly in Google searches. As Kevin Kelly has since [put it](http://www.kk.org/thetechnium/archives/2011/02/google-unique_n.php):

> With such a common first/last name attached to my face, I wanted my children to have unique names. They were born before Google, but the way I would put it today, I wanted them to have Google-unique names.

[Vladimir Nesov](http://squid314.livejournal.com/317320.html?thread=2319752#t2319752) termed having a common given and surname like "John Smith" as being "Google Stupid". [Clive Thompson](!Wikipedia) [says](http://www.collisiondetection.net/mt/archives/2007/05/_in_the_age_of.php) that search rankings were why he originally started blogging:

> Today's search engines reward people who have online presences that are well-linked-to. So the simplest way to hack Google to your advantage is to blog about something you find personally interesting, at which point other people with similar interests will begin linking to you - and the upwards cascade begins.
>
> This is precisely one of the reasons I started Collision Detection: I wanted to 0wnz0r the search string "Clive Thompson". I was sick of the British billionaire and Rentokil CEO Lord Clive Thompson getting all the attention, and, frankly, as a freelance writer, it's crucially important for anyone who wants to locate me - a source, an editor, old friends - to be able to do so instantly with a search engine. Before my blog, a search for "Clive Thompson" produced a blizzard of links dominated by the billionaire; I appeared only a few times in the first few pages, and those were mostly just links to old stories I'd written that didn't have current email addresses. But after only two months of blogging, I had enough links to propel my blog onto the first page of a Google search for my name.

This isn't obvious. It's easy to raise relatively rare risks as objections (but how many cases of identity theft are made possible solely by a relatively unique name making a person google-able? Surely few compared to the techniques of mass identity theft: corporate espionage, dumpster diving, cracking, skimming etc.) To appreciate the advantages, you have to be a 'digital native'. Until you've tried to Google friends or acquaintances, the hypothesis that unique names might be important will never occur to you. Until then, as long as your name was unique inside your school classes, or your neighborhood, or your section of the company, you would never notice. Even researchers spend their time researching unimportant correlations like people named Baker becoming bakers more often, or people tending to move to a state whose name they share (like Georgia).

What does one do? One avoids as much as possible choosing any name which is in the say, top 100 most popular names. People with especially rare surnames may be able to get away with common personal names, but not the Smiths. (It's easy to check how common names are with [online tools](http://howmanyofme.com/search/) drawing on US Census data. My own name pair is unique at the expense of the Dutch surname being 12 letters long, and difficult to remember.)

But one doesn't wake up and say "I will name myself 'Zachariah' today because 'John' is just too damn common". After 20 years or more, one is heavily invested in one's name. It's acceptable to change one's surname (women do it all the time), but not the first name.

One *does* decide the first name of one's children, though, and it's iron tradition that one does so. So we can expect digital natives to shy away from common names when naming their kids. But remember who are the 'digital natives' - kids and teenagers of the '00s, at the very earliest. If they haven't been on, say, Facebook for years, they don't count. Let's say their ages are 0-20 during 2008 when Facebook really picked up steam in the non-college population; and let's say that they won't have kids until ~30. The oldest of this cohort will reach child-bearing age at around 2018, and every one after that can be considered a digital native from osmosis if nothing else. If all this is true, then beginning with 2018, we will see a growing '[long tail](!Wikipedia "Heavy-tailed distribution")' of baby names.

So this is a good story: we have a suboptimal situation (too many collisions in the new global namespace of the Internet) and a predicted adjustment with specific empirical consequences.

But there are issues.

- Rare names may come with comprehensibility issues; [Zooko's triangle](!Wikipedia) in cryptography says that names cannot be unique, globally valid, *and* short or human-meaningful. You have to compromise on some aspect.
- There's already a decline in popular names, according to [Wikipedia](!Wikipedia "Given names#Popularity distribution of given names"):

    > Since about 1800 in England and Wales and in the U.S., the popularity distribution of given names has been shifting so that the most popular names are losing popularity. For example, in England and Wales, the most popular female and male names given to babies born in 1800 were Mary and John, with 24% of female babies and 22% of male babies receiving those names, respectively. In contrast, the corresponding statistics for in England and Wales in 1994 were Emily and James, with 3% and 4% of names, respectively. Not only have Mary and John gone out of favor in the English speaking world, also the overall distribution of names has changed [substantially] over the last 100 years for females, but not for males.

    (The female trend has continued through to 2010: "The 1,000 top girl names accounted for only 67% of all girl names last year, down from 91% in 1960 and compared with 79% for boys last year."^[["Say Goodnight, Grace (and Julia and Emma, too)"](http://www.nytimes.com/2011/06/26/magazine/the-state-of-babies-names-hello-jayden-goodbye-hannah.html). _New York Times Magazine_]) The theory could probably be rescued by saying that the advantage of having a unique given name (and thus a relatively unique full name) goes that far back, but then we would need to explain why the advantage would be there for women, but *not* men. On the other hand, Social Security data seems to indicate both a 2-century long decline in the popularity of the top ten names and also a convergence of top-ten name rarity; from [Andrew Gelman](http://andrewgelman.com/2013/01/where-36-of-all-boys-end-up-nowadays/):

    ![Total popularity of top ten names each year, by sex; Source: Social Security Administration, courtesy of Laura Wattenberg](/images/2013-01-toptenusanames.png "http://andrewgelman.com/wp-content/uploads/2013/01/topten.png")
- Pop culture is known to have a very strong influence on baby names (cf. the popularity of _Star Wars_ and the subsequent massive spike in 'Luke'). The counter-arguments to [The Long Tail](!Wikipedia) marketing theory say that pop culture is becoming ever more monolithic and hit-driven. The fewer hits, and the more mega-hits, the more we could expect a few names to spike and drive down the rate of other names. The effect on a rare name can be incredible even from relatively small hits (the song in question was only a Top 10):

    > Kayleigh became a particularly popular name in the United Kingdom following the release of a song by the British rock group Marillion. Government statistics in 2005 revealed that 96% of Kayleighs were born after 1985, the year in which Marillion released "Kayleigh".^[Wikipedia again]
- Given names follow a power-law distribution already where a few names dominate, and so small artifacts can make it appear that there is a shift towards unpopular names. Immigration or ethnic groups can distort the statistics and make us think we see a decline in popular names when we're actually seeing an increase in popular names elsewhere - imagine all the Muhammeds and Jesuses we might see in the future. Those will show up as decreases in the percentages of 'John' or 'James' or 'Emily' or 'William', and fool us, even though Muhammed and Jesus are 2 of the most popular names in the world.
- [One informal analysis](http://blog.theladders.com/research-2/3556/ "On a First-name Basis with Success? Your Mom Chose Your Name Wisely") suggests short first names are strongly correlated with higher salaries.
- the impacts of names can be hard to predict and subtle (see some examples cited in Alter's ["The Power of Names"](http://www.newyorker.com/online/blogs/elements/2013/06/psychology-language-power-of-names.html))

(Much of the above appears to be pretty common knowledge among people interested in baby names and onomastics in general; for example, a _Washington Post_ editorial by Laura Wattenberg, "Are our unique baby names that unique?", 16 Sunday May 2010, argues much of the above.)

#### Optimizing the alphabet

Here's an interesting idea: the glyphs of the Phoenician-style alphabet are not optimized in any sense. They are bad in several ways, and modern glyphs are little better. For example, v and w, or m and n. People confuse them all the time, both in reading and in writing.

So that's one criterion: glyphs should be as distinct from all the rest as possible.

What's a related criterion? m and w are another pair which seem suboptimal, yet they are as dissimilar as, say, a and b, under many reasonable metrics. m and w are related via *symmetry*. Even though they share relatively few pixels, they are still identical under rotation, and we can see that. We could confuse them if we were reading upside down, or at an angle, or just confuse them period.

So that's our next criterion: the distinctness must also hold when the glyph is rotated by any degree and then compared to the rest.

OK, so we now have a set of unique and dissimilar glyphs that are unambiguous about their orientation. What else? Well, we might want them to be easy to write as well as read. How do we define 'easy to write'? We could have a complicated physiological model about what strokes can easily follow what movements and so on, but we will cop out and say: it is made of as few straight lines and curves as possible. Rather than unwritable pixels in a grid, our primitives will be little geometric primitives.

The fewer the primitives and the closer to integers or common fractions the positioning of said primitives, the simpler and the better.

We throw all these rules in, add a random starting population or better yet a population modeled after the existing alphabet, and begin our genetic algorithm. What 26 glyphs will we get?

Problem: our current glyphs may be optimal in a deep sense:

> Dehaene describes some fascinating and convincing evidence for the first kind of innateness. In one of the most interesting chapters, he argues that the shapes we use to make written letters mirror the shapes that primates use to recognize objects. After all, I could use any arbitrary squiggle to encode the sound at the start of "Tree" instead of a T. But actually the shapes of written symbols are strikingly similar across many languages.
> It turns out that T shapes are important to monkeys, too. When a monkey sees a T shape in the world, it is very likely to indicate the edge of an object - something the monkey can grab and maybe even eat. A particular area of its brain pays special attention to those important shapes. Human brains use the same area to process letters. Dehaene makes a compelling case that these brain areas have been "recycled" for reading. "We did not invent most of our letter shapes," he writes. "They lay dormant in our brains for millions of years, and were merely rediscovered when our species invented writing and the alphabet."
<http://www.nytimes.com/2010/01/03/books/review/Gopnik-t.html>

#### Meta

A: But who is to say that a butterfly could not dream of a man? You are not the butterfly to say so!

B: No. Better to ask what manner of beast could dream of a man dreaming a butterfly, and a butterfly dreaming a man.

#### Why IQ doesn't matter and how points mislead

One common anti-IQ arguments is that IQ does nothing and may be actively harmful past 120 or 130 or so; the statistical evidence is there to support a loss of correlation with success, and commentators can adduce [William Sidis](!Wikipedia) if they don't themselves know any such 'slackers', or the [Terman report](!Wikipedia)'s [similar findings](http://www.iza.org/conference_files/CoNoCoSk2011/gensowski_m6556.pdf) (viz. that personality factors matter more after ~130+).

This is a reasonable objection. But it is rarely proffered by people really familiar with IQ, who also rarely respond to it. Why? I believe they have an intuitive understanding that IQ is a *percentile ranking*, not an *absolute measurement*. (IQ is ordinal, not cardinal.)

It is plausible that the 20 points separating 100 and 120 represents far more cognitive power and ability than that separating 120 and 140, or 140 and 160. To move from 100 to 120 with a standard deviation of 15, one must surpass 40% of the population; to move from 120 to 140 requires surpassing a smaller percentage (~8.7%), and 140-160 smaller yet - which makes sense, since the higher the IQ, the smaller the percentage of the overall population to begin with!

Similarly it should make us wonder how much absolute ability is being measured at the upper ranges when we reflect that, while normal (relatively low) adult IQs are stable over years, they are unstable in the short-term and test results can vary dramatically even if there is no distorting factors like emotional disturbance or varying caffeine consumption. If one question at the end of an IQ test is the difference between an IQ of 170 and 160, wouldn't one expect a great deal of variance and reduced reliability? (I'm not familiar with the high-normed IQ literature; this may be utterly obvious and well-supported experimentally.)

Another thought: are the kids in your local [special ed](!Wikipedia) program mentally closer to chimpanzees, or to Albert Einstein/[Terence Tao](!Wikipedia)? Pondering all the things we expect even special ed kids to learn or already know (vision, natural language, eye-hand coordination - all the stuff of [Moravec's paradox](!Wikipedia)), I think those kids are vastly closer to Einstein than monkeys.

And if retarded kids are closer to Einstein that the smartest non-human animal, that indicates human intelligence is very 'narrow', and that there is a vast spectrum of stupidity stretching below us all the way down to viruses (which only 'learn' through evolution). (Current IQ tests are designed for, tested against, and normed on fine distinctions among humans. It is [very hard](http://www.nytimes.com/books/first/b/budiansky-lion.html) to test animal intelligence because of differing incentives and sensory systems, but *if* one deals with those problems, there ought to be some general intelligence of prediction and problem solving; the approach I favor is [AIXI-style IQ tests](http://lesswrong.com/lw/42t/aixistyle_iq_tests/).)

A gap like 20 points looks very impressive from our narrow compressed human perspective, but it reflects very little *absolute* difference; to a sheep, other sheep are each distinctive. In [Big O](!Wikipedia) computer terms, we might say that geniuses are a [constant factor](!Wikipedia) faster than their dimmer brethren, but not [asymptotically](!Wikipedia) faster.

It is expected then, that someone measured at 180 doesn't make the rest of us look like a nigh-comatose retard of 20 IQ points. To be so smart requires thousands of factors (mental & biological) to click just right (genetically correlating with [thousands of variations](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3182557/ "'Genome-wide association studies establish that human intelligence is highly heritable and polygenic', Davies et al 2011"), and not a few master genes); if ordinary people luck out on 900 factors, then those geniuses' scores are trying to secern differences of 2 or 3 factors. The practical impact of a few factors out of thousands may be minimal, and explain the findings without denying the existence of such differences.

#### Backups: life and death

Consider the plight of an upload - a human mind running on a computer rather than a brain. It has the advantage of all digital data: perfect fidelity in replication, fast replication - replication period. An upload could well be immortal. But an upload is also very fragile. It needs storage at every instance of its existence, and it needs power for every second of thought. It doesn't carry with it any reserves - a bit is a bit, there are no bits more durable than other bits, nor bits which carry small batteries or [UPSes](!Wikipedia "Uninterruptable power supply") with themselves.

So reliable backups are literally life and death for uploads.

But backups are a double-edged sword for uploads. If I backup my photos to [Amazon S3](!Wikipedia) and a bored employee pages through them, that's one thing; annoying or career-ending as it may be, pretty much the worst thing that could happen is that I get put in jail for a few decades for child pornography. But for an upload? If an enemy got a copy of its full backups, the upload has essentially been kidnapped. The enemy can now run copies and torture them for centuries, or use them to attack the original running copy (as hostages, in [false flag attacks](!Wikipedia), or simply to understand & predict what the original will do). The negative consequences of a leak are severe.

So backups need to be both reliable and secure. These are conflicting desires, though.

One basic principle of long-term storage is '[LOCKSS](!Wikipedia)': "lots of copies keeps stuff safe". Libraries try to distribute copies of books to as many holders as possible, on the premise that each holder's failure to preserve a copy is a random event independent of all the other holders; thus, increasing the number of holders can give arbitrarily high assurances that *a* copy will survive. But the more copies, the more risk one copy will be misused. That's fine if 'misuse' of a book is selling it to a book collector or letting it rot in a damp basement; but 'misuse' of a conscious being is unacceptable.

Suppose one encrypts the copies? Suppose one uses a [one-time pad](!Wikipedia), since one worries that an encrypted copy which is bullet-proof *today* may be copied and saved for centuries until the encryption has been broken, and is perfectly certain the backups are 'secure'. Now one has 2 problems: making sure the backups survive until one needs them, and making sure the one-time pad survives as well! If the future upload is missing either one, nothing works.

The trade-off is unfortunate, but let's consider secure backups. The first and most obvious level is physical security. Most systems are highly vulnerable to attackers who have physical access; desktop computers are trivially hacked, and [DRM](!Wikipedia) is universally a failure.

Any backup ought to be as inaccessible as possible. [Security through obscurity](!Wikipedia) might work, but let's imagine *really* inaccessible backups. How about hard drives in orbit? No, that's too close: commercial services can reach orbit easily, to say nothing of governments. And orbit doesn't offer too much hiding space. How about orbit not around the Earth, but around the Solar System? Say, past the orbit of Pluto?

That offers an enormous volume: the Kuiper Belt is roughly ~1.95x10^30^ cubic kilometers[^volume]. The lightspeed delay is at least 20 minutes, but [latency](!Wikipedia) isn't an issue; a backup protocol on Earth could fire off one request to an orbiting device and the device would then transmit back everything it stored without waiting for any replies or confirmations (somewhat like [UDP](!Wikipedia)).

10^30^ cubic kilometers is more than enough to hide small stealthy devices in. But once it sends a message back to Earth, its location has been given away - the Doppler effect will yield its velocity and the message gives its location at a particular time. This isn't enough to specify its orbit, but it cuts down where the device could be. 2 such messages and the orbit is known. A restore would require more than 2 messages.

The device could self-destruct after sending off its encrypted payload. But that is very wasteful. We want the orbit to change unpredictably after each broadcast.

If we imagine that at each moment the device chooses between firing a thruster to go 'left' or 'right', then we could imagine the orbit as being a message encrypted with a one-time pad - a one-time pad, remember, being a string of random bits. The message is the original orbit; the one-time pad is a string of random bits shared by Earth and the device. Given the original orbit, and knowing when and how many messages have been sent by the device, Earth can compute what the new orbit is and where the device will be in the future. ('It started off on this orbit, then the random bit-string said at time X to go left, then at X+1, go left again, then at X+Y, go right; remembering how fast it was going, that means it should now be... there in the constellation of Virgo.')

The next step up is a symmetric cipher: a shared secret used not to determine future orbit changes, but to send messages back and forth - 'go this way next; I'm going this way next; start a restore' etc. But an enemy can observe where the messages are coming from, and can work out that 'the first message must've been X, since if it was at point N and then showed up at point O, only one choice fits, which means this encrypted message meant X, which lets me begin to figure out the shared secret'.

A public-key system would be better: the device encrypts all its messages against Earth's private key, and vice versa. Now the device can randomly choose where to go and tell Earth its choice so Earth knows where to aim its receivers and transmitters next.

But can we do better?

[^volume]: The area of a sphere is given by the equation: $\frac{4}{3} \times \pi \times r^3$\
       1 AU = $149.60 \times 10^6$ kilometers\
       30 AU = $30 \times 149.60 \times 10^6$, or $4.488 \times 10^9$ km\
       55 AU = $55 \times 149.60 \times 10^6$, or $8.228 \times 10^9$ km\
       So the shell is the volume of the outer sphere minus the inner sphere:\
       $(\frac{4}{3} \times \pi \times (8.228 \times 10^9)^3) - (\frac{4}{3} \times \pi \times (4.488 \times 10^9)^3)$, or $1.9546466984296578 \times 10^{30}$.

#### A secular humanist reads _The Tale of Genji_

After several years, I finished reading Edward Seidensticker's translation of _[The Tale of Genji](!Wikipedia)_. Many thoughts occurred to me towards the end, when the novelty of the Heian era began to wear off and I could be more critical.

The prevalence of poems & puns is quite remarkable. It is also remarkable how tired they all feel; in _Genji_, poetry has lost its magic and has simply become another stereotyped form of communication, as codified as a letter to the editor or small talk. I feel fortunate that my introductions to Japanese poetry have usually been small anthologies of the greatest poets; had I first encountered court poetry through _Genji_, I would have been disgusted by the mawkish sentimentality & repetition.

The gender dynamics are remarkable. Toward the end, one of the two then main characters becomes frustrated and casually has sex with a serving lady; it's mentioned that he liked sex with her better than with any of the other servants. Much earlier in _Genji_ (it's a good thousand pages, remember), Genji simply rapes a woman, and the central female protagonist, Murasaki, is kidnapped as a girl and he marries her while still what we would consider a child. (I forget whether Genji sexually molests her before the _pro forma_ marriage.) This may be a matter of non-relativistic moral appraisal, but I get the impression that in matters of sexual fidelity, rape, and children, Heian-era morals were not much different from my own, which makes the general immunity all the more remarkable. (This is the 'shining' Genji?) The double-standards are countless.

The power dynamics are equally remarkable. Essentially every speaking character is nobility, low or high, or Buddhist clergy (and very likely nobility anyway). The characters spend next to no time on 'work' like running the country, despite many main characters ranking high in the hierarchy and holding minister-level ranks; the Emperor in particular does nothing except party. All the households spend money like mad, and just expect their land-holdings to send in the cash. (It is a signal of their poverty that the Uji household ever even mentions how less money is coming from their lands than used to.) The Buddhist clergy are remarkably greedy & worldly; after the death of the father of the Uji household, the abbot of the monastery he favored sends the grief-stricken sisters a note - which I found remarkably crass - reminding them that he wants the customary gifts of valuable textiles[^Seidensticker-Genji].

[^Seidensticker-Genji]: pg814 of my Seidensticker e-book:

    > ...Because the prince had gone there for his retreats, an occasional messenger came down from the monastery and, rarely, there was a note from the abbot himself, making general inquiries about their health. He no longer had reason to call in person. Day by day the Uji villa was lonelier. It was the way of the world, but they were sad all the same. Occasionally one or two of the village rustics would look in on them. Such visits, beneath their notice while their father was alive, became breaks in the monotony. Mountain people would bring in firewood and nuts, and the abbot sent charcoal and other provisions.
    >
    > "One is saddened to think that the generous flow of gifts may have ceased forever",  said the note that came with them.
    >
    > It was a timely reminder: their father had made it a practice to send the abbot cottons and silks against the winter cold. The princesses made haste to do as well.

    Rereading this passage, I think it could be defended as not crass based on bit about how "the abbot sent charcoal and other provisions" - it could be that the cottons & silks are part of a barter exchange because they all are too elegant and high-class to engage in such dclass merchant-like 'buying' or 'selling'. I am not sure how realistic this is, given that textiles pre-Industrial-Revolution were extremely expensive goods, and even now a silk garment would cost many kilograms of charcoal (charcoal seems to cost ~\$0.5/kg, while silk robes seem to rarely be <\$50).

The medicinal practices are utterly horrifying. They seem to consist, one and all, of the following algorithm: 'while sick, pay priests to chant.' If chanting doesn't work, hire more priests. (One freethinker suggests that a sick woman eat more food.) Chanting is, at least, not outright harmful like bloodletting, but it's still sickening to read through *dozens* of people dying amidst chanting. In comparison, the bizarre superstitions (such as trapping them in houses on inauspicious days) that guide many characters' activities are unobjectionable.

The 'ending' is so abrupt, and so clearly unfinished; many chapters have been spent on the 3 daughters of the Uji householder, 2 are disposed of, and the last one has just been discovered in her nunnery by 1 of the 2 protagonists (and the other protagonist suspects). The arc is not over until the would-be nun has been confronted, yet the book ends. Given that [Murasaki Shikibu](!Wikipedia) was writing an episodic entertainment for her court friends, and the overall lack of plot, I agree with Seidensticker that the abrupt mid-sentence ending is due either to Shikibu dying or abandoning her tale - not to any sort of deliberate plan.

#### Measuring multiple times in a sandglass

How does one make a sand hourglass measure multiple times?

One could just watch it and measure fractions by eye - when a 10-minute timer is down to 1/2, it has measured 5 minutes. One could mark the outside and measure fractions that way.

Or perhaps one could put in two-toned sand - when the white has run out and there's only black sand, then 5 minutes has passed.

But the sand would inevitably start to mix, and then you just have a 10-minute timer with grey sand. Perhaps some sort of plastic sheet separating them? But it would get messed up when it passes through the funnel.

Then, perhaps the black sand could be magnetically charged positively, and the white sand negatively? But magnetism attracts *unlike*. If the black is positive and white negative, they'll clump together even more effectively than random mixing would.

We can't make a color homogeneous in charge. Perhaps we could charge just black negative, and put positive magnets at the roof and floor? The bias might be enough over time to counteract any mixing effect - the random walk of grains would have a noticeable bias for black. But if the magnet is strong, then some black sand would never move, and if it's weak, then most of the sand will never be affected; either way, it doesn't work well.

Perhaps we could make half the black sand positive and half negative, while all white is neutral? Black will clump to black everywhere in the hourglass, without any issues about going through the funnel or affecting white.

How might this fail? Well, why would there be only *2* layers? There could be several alternating layers of black and white, and this be a stable system.

We might be able to remedy this by combining magnetized black sand with magnets on the roof/floor, imparting an overall bias - the layers form, but slowly get compacted together.

The real question is whether strong enough magnetism to usefully sort is also so strong to clump together and defeat the gravity-based timing.

#### Measuring social trust by offering free lunches

People can be awfully suspicious of free lunches. I'd like to try a little experiment or stunt sometime to show this. Here's how it'd go.

I'd grab myself a folding table, make a big poster saying 'Free Money! \$1 or \$2' and in fine print, 'one per person per day'. Then, anyone who came up and asked would get \$2. Eventually, someone would ask for \$1 - they would get it, but be asked first *why* they declined the larger amount.

I think their answers would be interesting.

Even funner would be giving the \$2 as a 2-dollar bill, and not 2 dollar bills. They're rare enough that it would be quite a novelty to people.

#### Leaf burgers

One thing I was known for in Boy Scouts (or so I thought) was my trick of cooking hamburgers with leaves rather than racks or pans. I had learned it long ago at a campboree, and made a point of cooking my hamburger that way and not any other.

The way it works is you take several large green leafs straight from the tree, and sandwich your burger. Ideally you only need 2, one leaf on top and the other on bottom. (I was originally taught using just one leaf, and carefully flipping the burger on top of its leaf, but that's error prone - one bad flip and your burger is irretrievably dirty and burned.) Then you put your green sandwich on top of a nice patch of coals - no flames! - and flip it in 10 minutes or so.

You'll see it smoke, but not burn. The green leaves themselves don't want to burn, and the hamburger inside is giving off lots of water, so you don't need to worry unless you're overcooking it. At about 20 minutes, the leaves should have browned and you can pull it out and enjoy.

What's the point of this? Well, it saves one dishes. Given how difficult it is to clean dishes out there where there are no dishwashers or sinks, this should not be lightly ignored. It cooks better: much more evenly and with less char or burning of the outside. Given many scouts' cooking skills, this is no mean consideration either. It's a much more interesting way to cook. And finally, the hamburger ends up with a light sort of 'leafy' taste on the outside, which is quite good and not obtainable any way else.

#### Arthur Moulton

My grandparents have long been friends with an old bachelor named Arthur Moulton (into his 80s by the time I knew him). He's long been a mysterious man to us.

I've long prided myself on my search skills, and reminded of Arthur by his declining health, I thought on 15 June 2013 it might be an interesting exercise to dig up everything I could from public online sources, while relying on only the vaguest details about him (full name and age). The family was interested and as it turned out, the information could be useful for his obituary. I managed to find a fair bit of information on him and his brother, Roger. (As it happens, [reading his family's obituary for him](http://smnewsnet.com/archives/89255 "Arthur B. Moulton, 93") I noticed I had missed a few things, but overall, did fairly well.)

I give my process & results below, with my reasoning as best as I can reconstruct it.

##### Background

Arthur Bernard Moulton; American Caucasian male, ~92 (~1921 - 20 December 2013), retired, lived in Maryland, believed to have worked for intelligence or defense agencies in unknown but possibly classified capacities during the Cold War (claims to have witnessed several atomic "shots" including at White Sands and in the south Pacific but not at Bikini Atoll).

##### General

I start searching general databases with a query like `"Arthur Bernard Moulton" OR "Arthur B Moulton" OR "Arthur Moulton"`:

- PACER: no hits in Maryland district or bankruptcy court
- JSTOR: nothing
- EBSCOhost Academic Search Complete: nothing
- Web of Science: nothing
- Google News Archives: nothing
- _New York Times_: nothing
- [Unz.org](http://www.unz.org/Pub/AllPublications): nothing

##### Intelligence?

Leaning on the intelligence possibility, I check [NameBase](!Wikipedia) and find [a hit](http://www.namebase.org/cgi-bin/nb04?FE):

> Association of Former Intelligence Officers, McLean Office Building, 6723 Whittier Ave., Suite 303A, McLean, VA 22101, Tel: 703-790-0320. Membership Directory. 1983. 103 pages. AFIO is a national organization of about 2500 members, with a smaller number active at national conventions and local chapters. It is not necessary to be a former intelligence officer to join, as long as you support the principles of the organization. Some journalists have joined just to get the membership directory, hoping to find retired officers willing to grant an occasional interview or comment on various issues.
>
> ...
>
> - MOULTON ARTHUR B
> - MOULTON ROGER D

As NameBase cautions, we cannot infer much from the inclusion, but it makes a useful start because it gives us another person to look for. Googling this "Roger D. Moulton", I find his [WWII enlistment record](http://www.mainegenealogy.net/individual_enlistment_record.asp?id=48022) which tells me the following useful information: Roger Moulton was born in 1921, was a white male with a highschool education, and enlisted 1943 in Bangor, Maine. Is this Arthur's younger brother?

##### Census

We can find out by looking at US Census data which is released after 72 years; so luckily, the 1940 census was released in 2012. (This explains my failure to find this data ~2009.). Arthur's [1940 Census entry](https://familysearch.org/pal:/MM9.1.1/KMMP-TWX) tells us that in 1940 he was aged 19 (b. 1921, New Hampshire); he had a widowed mother [Ida D](https://familysearch.org/pal:/MM9.1.1/KMMP-TW6) (b. 1884, Michigan), a brother [Roger D](https://familysearch.org/pal:/MM9.1.1/KMMP-TWF) (b. 1922, Maine), & a sister [Jean R](https://familysearch.org/pal:/MM9.1.1/KMMP-TWN) (b. 1927,  Minnesota). (Family legend confirms that Arthur's mother was indeed widowed; supposedly, his father was working with some other men on some sort of electrified wire and they all were electrocuted while Arthur watched as a young boy.) We get more detailed information by reading the scanned census form: Ida, Arthur, & Roger are listed as having graduated highschool but Jean had completed only her first year of highschool. The 3 siblings had attended school at some time 'since March 1940'. No one in the family, including the "lodger" Richard Brant, was employed at the time (except for Ida doing housework). Issues here include: why does the census birth-year 1922 for Roger differ from the enlistment record 1921? He was 22 when he enlisted (1943 - 1921 = 22) so there was no need to lie. How did Arthur's father die? Was one lodger really enough to pay their bills? What did Arthur and Roger do in between 1940 and 1943, given that they had apparently graduated highschool but were not in the military (the US officially entered WWII on 11 December 1941) and Roger's "Civilian Occupation" is "Undefined Code" in the enlistment paper?

##### Patents

Google Scholar's patent search turns up 2 patents and a citation to possibly a third patent:

1. filed 6 April 1954, [#2858477 "Ring Circuits"](http://www.google.com/patents/US2858477) to Arthur B. Moulton, San Diego CA; not assigned

    > This invention is in [ring circuits](!Wikipedia) and specifically is ring circuit utilizing gas filled tubes only. One object of the invention is to provide a ring one gas filled tube for each stage. Another object of the invention is to provide a of the nature mentioned requiring no source such as is usually required. Other objects will be apparent from a reading of specification and claims. The drawing is a schematic diagram of a ring to my invention.
2. filed 5 March 1956, [#2933682 "Frequency Measuring Apparatus"](http://www.google.com/patents/US2933682) to Arthur B. Moulton, San Diego CA, and Joseph A. Webb, La Mesa CA; patent was assigned to General Dynamics Corporation, San Diego

    > This invention relates to electrical measuring apparatus, and, more particularly, to apparatus for measuring a difference in frequency between two alternating voltages.
3. filed 21 June 1971, [#3676802 "Submarine Propeller Cavitation Noise Simulator"](http://www.google.com/patents/US3676802) to Francis J. Murphree & Paul S. Catano (Winter Park & Orlando of Florida, respectively); assigned to "The United States of America as represented by the Secretary of the Navy"

    > Abstract: Simulation of submarine propeller cavitation as it varies with speed and depth of submergence is effected by feeding a frequency proportional to blade rate to a counter which is periodically read out and reset at a rate proportional with the square root of pressure. The read out is used to control noise attenuator means including a one of N decoder and N attenuators scaled to produce relative noise according to a curve characteristic of the submarine to be simulated. The noise output is modulated in pulse width and repetition rate by a function generator controlled by the counter read-out.
    >
    > ... [pg6] Conversely, of course, the pulses go from wide to narrow as cavitation decreases. To this end, the blade rate analog input on line 22 is also applied via line 22_a_ to a voltage to frequency convert 104, the output of which is fed via line 106 as the triggering input to the one-shot 100. The one-shot 100, may conveniently be of the type described in U.S. Pat. No. application Ser. No. 50,259 of Arthur B. Moulton, assigned to the assignee hereof, and is adapted to have its unstable or triggered period controlled in duration by the voltage input from the function generator 96...

    I am unable to find anything about that application. It may be classified but the Navy, [as of 2012](https://www.fas.org/sgp/othergov/invention/stats.html), holds only 29 secret patents (out of a total of 5321 such patents, out of >6 million patents ever issued), so it's possible that the patent application was simply rejected or abandoned.

Patent #2 is interesting for being assigned to [General Dynamics Corporation](!Wikipedia), what is now one of the largest defense conglomerates in the world. But in 1956 it was much smaller, had recently bought the airplane manufacturer Canadair and had only adopted that name in 1952, then purchasing [Convair](!Wikipedia) in 1953; they began building all sorts of military airplanes, the Atlas ICBM, and civilian airliners. The original Convair plant was located in San Diego, where Arthur is listed as filing the patent from in 1954. In that year, Convair was either manufacturing or developing the [Convair B-36](!Wikipedia), [Convair F-102 Delta Dagger](!Wikipedia), [Convair B-58 Hustler](!Wikipedia), and the [SM-65 Atlas](!Wikipedia) (substantially modified partway through the design process due to successful H-bomb tests). The patent seems like it could've been useful for any of these.

##### Google
###### Books

Google Books turns up 6 potentially useful hits; in chronological order:

1. the 1946 _Annual Report_ of the [Portland, Maine](!Wikipedia) public library on pg114 mentions

    > ...Medbury, Mrs. Arthur B. Moulton, Miss Marcia Merrill, Mrs. Mary Mulkern...

    The names are listed in an unknown context (the book is still in copyright); while he was single in the 1940 census, in 1946, Arthur would have been ~25 and so could be married, and he is not due in California for another 8 years. This is may be another red herring: the [Maine Marriage History](https://portal.maine.gov/marriage/archdev.marriage_archive.search_form) archive turns up 3 marriages for a "MOULTON ARTHUR B" in 1903 and 1933, which obviously cannot be this Arthur (who was not even alive in 1903) but the women involved are well within living distance of 1946.
2. Under [_Electronics_](!Wikipedia "Electronics (magazine)") (ISSN 0883-4989), volume 63 (1963), "FREE REPRINT of the MONTH", pg123:

    > "Chart Gives RLC Values for Critical Damping" by Arthur B. Moulton, P.O. Box 24, Livermore, California
    >
    > This article is a reprint from the May 31, 1963 issue of _electronics_. Selecting component values for generating a critically damped transient in a simple RLC circuit is a cut-and-try under conditions frequently not in practice. Component selection is made easier by the normalized graphs given in this article. One copy per person only, if more are required regular reprint costs apply.

    Arthur would be ~42. A family story reports him working at the Lawrence Livermore National Laboratory, as might be expected given his previous career and the location, but a search of the LLNL site failed to turn up anything. Archives of the defunct magazine are not available online, but a request was successful in [producing a scanned copy](https://dl.dropboxusercontent.com/u/182368464/1963-moulton.pdf); unfortunately, it includes no useful details beyond what the snippet provided.
3. _Professional Engineer_, volume 43 (1973) includes 3 advertisements (pg64, 71, 76) for

    > Arthur B. Moulton, P.E.
    >
    > Consultant
    >
    > Electronic Design, Integrated Circuit Systems, Analog Computers, Radio, Dc. Motors, Reports, Photography
    >
    > P.O. Box 149, Laurel, Md. 20810

    [Laurel, Maryland](!Wikipedia), incidentally, is located very near the NSA's headquarters. Arthur would be ~52. There turns out to be a possible explanation for this: in the '70s, Arthur's brother Roger was working for the NSA as a contractor on an unusual pair of computers (see later section), and so it would make sense for him to be living near the headquarters such/ as the town Laurel, and if Roger is there, then there are many reasons Arthur might be: renting a room from Roger, wishing to be near Roger, drawing on connections for work, etc.
4. _Atlas World Press Review_, volume 25 (1978) includes 2 advertisements (pg57) for

    > LIBRARY RESEARCH, any subject. Three great libraries this area. Arthur B. Moulton, P.O. Box 149, Laurel, Md. 20810

    Arthur would be ~57.
5. The final 2 hits turn out to be useless without any snippets: _History of the town of Arlington, Massachusetts_, pg339, Cutter & Cutter 1988 (likely useless, hard to see how Arthur could figure in it); _Ossipee, New Hampshire, vital records, 1887-2001_, Roberts 2002, pg399 (possibly a birth entry)

###### Search

A general Google search is not very fruitful, initially turning up only the global-warming denialist ["Global Warming Petition Project"](http://www.petitionproject.org/signers_by_last_name.php?run=M) includes an Arthur B. Moulton; it restricts signatures to "Americans with university degrees in science", but a degree in electrical engineering would presumably count. A more intensive trawl through every available hit for a heavily filtered search (`"Arthur Bernard Moulton" OR "Arthur B Moulton" OR "Arthur Moulton" -lawyer -bishop -"Moulton Simpson" -"William Arthur" -"Jr." -"Moulton Allen" -"Arthur H" -"J. Arthur" -Foweraker -"Charles Arthur" -"Edward George"`) turned up nothing useful except what is probably his phone number and the existence of a [ham radio operator](http://www.interceptradio.com/ham.php?call=WA3UVQ) also named Arthur B Moulton. (I misread the page initially: when I saw "Scotland", I assumed it was a ham radio in, well, Scotland; but if you read the page carefully, you see "Scotland" is in the *City* column, and the *state* is "MD" or Maryland - he lived in [Scotland, Maryland](!Wikipedia). Most confusing.)

###### University of Maine

On an intuition, I append "electrical engineering" to the search and immediately turn up a perfect `e-yearbook.com` hit which lists "ARTHUR B. MOULTON Electrical Engineering" - from the University of Maine, 1943, with a Roger D. Moulton. Bingo! The site demands an absurd fee for the original image, but scans of the [university yearbook](http://www.library.umaine.edu/yearbooks/Forties.htm) are available and it is relatively trivial to download, find the relevant page, extract it from the PDF, and upload it:

![pg114 of the 1943 University of Maine yearbook, with the entries for Arthur B. Moulton & Roger D. Moulton; Arthur is below the lad with glasses](/images/1943-umaineyearbook-themoultons.jpg)

We find as predicted:

> Arthur B. Moulton
>
> Electrical Engineering / York Village
>
> Dean's List 2a.

And likewise his busy brother:

> Roger D. Moulton
>
> Electrical Engineering / York Village
>
> Main Masque 2,3; Scabbard and Blade 3; M.O.C. 2; A.I.E.E. 3; Radio Club 3; Vice President, I.S.O. 3; Men's Student Senate 3; Dean's List 1b, 2a, 2b.

###### Roger Moulton

This unexpected re-appearance of his brother Roger prompted me to search for Roger some more, and I hit pay dirt. It turns out that the people who worked on two early NSA supercomputers, the [IBM 7030 Stretch](https://en.wikipedia.org/wiki/IBM_7030) and the [Harvest](https://en.wikipedia.org/wiki/IBM_7950_Harvest), have [held reunions](http://users.bestweb.net/~collier/sh/). In the collated [org chart](http://users.bestweb.net/~collier/sh/orgchart.html) and the apparently-current [contact information](http://users.bestweb.net/~collier/sh/contact.html), we find the entry "Roger D. Moulton  org  nsa harvest silo memory contract".

Unfortunately, further details seem hard to find. Aside from a mention on a [York High School reunion](http://www.ccstrat.com/yhs2011reunion/regrets2011.html) page, the hits for Roger D. Moulton are contaminated by some chemist's papers/citations/patents.

I have to admit defeat at this point - I've checked all the databases I can easily think of, so for followup, I created a Google Alert in case additional materials surface on the public web (`"Arthur Bernard Moulton" OR "Arthur B Moulton" OR "Arthur B. Moulton" OR "Roger D. Moulton" OR "Roger D Moulton"`). We'll see. But by 28 December 2013, nothing useful had appeared.

#### Night watch

> The gloom of dusk. \
> An ox from out in the fields \
> comes walking my way; \
> and along the hazy road \
> I encounter no one.^[Shtetsu; 59 'An Animal in Spring'; [_Unforgotten Dreams: Poems by the Zen monk Shtetsu_](http://www.amazon.com/Unforgotten-Dreams-Steven-D-Carter/dp/0231105762/); trans. Steven D. Carter, ISBN 0-231-10576-2]

Night watch is not devoid of intellectual interest. The night is quite beautiful in its own right, and during summer, I find it superior to the day. It is cooler, and often windier. Contrary to expectation, it is less buggy than the day. Fewer people are out, of course.

My own paranoia surprises me. At least once a night, I hear noises or see light, and become convinced that someone is prowling or seeks to break in. Of course, there is no one there. This is true despite it being my 4th year. I reflect that if it is so for me, then what might it be like for a primitive heir to millennia of superstition? There is a theory that spirits and gods arise from overly active imaginations, or pattern-recognition as it is more charitably termed. My paranoia has made me more sympathetic to this theory. I am a staunch atheist, but even so!

The tempo at night varies as well. It seems to me that the first 2 years, cars were coming and going every night. Cars would meet, one would stay and the other go; or a car would enter the lot and not leave for several days (with no one inside); or they would simply park for a while. School buses would congregate, as would police-cars, sometimes 4 or 5 of them. In the late morning around 5 AM, the tennis players would come. Sometimes when I left at 8 AM, all 4 or 5 courts would be busy - and some of the courts hosted 4 players. I would find 5 or 6 tennis balls inside the pool area, and would see how far I could drop-kick them. Now, I hardly ever find tennis balls, since I hardly ever see tennis players. A night in which some teenagers congregate around a car and smoke their cigarettes is a rarity. Few visit my lot.

I wonder, does this have to do with the recession which began in 2008?

##### Fiction

> Another year gone by \
> And still no spring warms my heart. \
> It's nothing to me \
> But now I am accustomed \
> To stare at the sky at dawn.^[[Fujiwara no Teika](!Wikipedia); pg 663 of [Donald Keene](!Wikipedia) (1999), [_Seeds in the Heart: Japanese Literature from Earliest Times to the Late Sixteenth Century_](http://www.amazon.com/Seeds-Heart-Japanese-Literature-Sixteenth/dp/0231114419/), Columbia University Press, ISBN 0-231-11441-9]

The night has, paradoxically, sights one cannot see during the day. What one can see takes on greater importance, becoming new and fresh. I recall one night long ago; on this cool dark night, the fogs lay heavy on the ground, light-grey and densely soupy. In the light, one could watch banks of fog swirl and mingle in myriads of meetings and mutations; it seemed a thing alive. I could not have seen this under the sun. It has no patience for such ethereal and undefinable things. It would have burned off the fog, driven it along, not permitted it to linger. And even had it existed and been visible, how could I have been struck by it if my field of view were not so confined?

One feels an urge to do strange things. The night has qualities all its own, and they demand a reflection in the night watcher. It is strange to be awake and active in the wrong part of the day, and this strangeness demands strangeness on one's own part. Often when doing my rounds I have started and found myself perched awkwardly on a bench or fence. I stay for a time, ruminating on nothing in particular. The night is indefinite, and my thoughts are content to be that way as well. And then something happens, and I hop down and continue my rounds.

For I am the sole inhabitant of this small world. The pool is bounded by blackened fences, and as it lies prostrate under tall towers bearing yellowed flood-lights. The darkness swallows all that is not pool, and returns a feeling of isolation. As if nothing besides remains. I circumnambulate to recreate the park, to assure me it abides, that it is yet there to meet my eyes - a sop to conscience, a token of duty; an act of creation.

I bring the morning.

#### Two cows: philosophy

Philosophy [two-cows](!Wikipedia "You have two cows") jokes:

- Free will: you have 2 cows; in an event entirely independent of all previous events & predictions, they devour you alive; this makes no sense as cows are herbivores, but you are no longer around to notice this.
- Fatalism: you have 2 cows; whether they survive or not is entirely up to the inexorable and deterministic course of the universe, and what you do or not likewise, so you don't feed your cows and they starve to death; you reflect that the universe really has it in for you.
- Compatibilism: you have 1 cow which is free and capable of making decisions, and 1 cow that is determined and bound to follow the laws of physics; they are the same cow. But you get 2 cows' worth of milk anyway.
- Existentialism: You have two cows; one is a metaphor for the human condition. You kill the other and in bad faith claim hunger made you do it.
- Ethics: You have two cows, and as a Utilitarian, does it suit the best interests of yourself and both cows to milk them, or could it be said that the interests of yourself, as a human, come above those of the cows, who are, after all, inferior to the human race? Aristotle would claim that this is correct, although Peter Singer would disagree.
- Sorites: you have 2 cows who produce a bunch of milk; but if you spill a drop, it's still a bunch of milk; and so on until there's no more milk left. Obviously it's impossible to have a bunch of milk, and as you mope over how useless your cows are, you die of thirst.
- Nagarjuna: You have 2 cows; they are 'empty', of course, since they are dependent on grass; you milk them and get empty-milk (dependent on the cow), which tastes empty; you sell them both and go get some real cows. _Moo mani hum_...
- Descartes: You have 2 cows, therefore you are (since deceive me howbeit the demon may, he can never make it so that I have 2 cows yet am not); further, there are an infinite #### of 2-cows jokes, and where could this conception of infinity have come from but God? Therefore he exists. You wish you had some chocolate milk.
- Bentham: no one has a natural right to anything, since that would be '2 cows walking upon stilts'; everything must be decided by the greatest good for the greatest number; you get a lobotomy and spend the rest of your life happily grazing with your 2 cows.
- Tocqueville: Cows are inevitable, so we must study the United Cows of America; firstly, we shall take 700 pages to see how this nation broke free of the English Mooarchy, and what factors contributes to their present demoocracy...
- Gettier: You see 2 cows in your field - actually, what you see is 2 cow-colored mounds of dirt, but there really are 2 cows over there; when you figure this out, your mind is blown and >2000 years of epistocowlogy shatters.
- Heidegger: [dasein](!Wikipedia) dasein apophantic being-in cow being-in-world milk questioning proximate science thusly Man synthesis time, thus, 2 cows.
- Husserl: You have 2 cows, but do you really *see* them?

#### Waking up

In neuroscience, there's a model of consciousness called the 'workspace' model. The idea is that the various modules in the brain, like the auditory or visual or long-term memory modules normally operate on their own, doing their things, predicting & perceiving what they can; but sometimes something goes wrong: the predictions are suddenly all wrong, or there's unusual & urgent input. The modules panic and emit a summary of the situation over to the single global workspace, where it sits side by side with all the other summaries, and the slow linear prefrontal cortex ponders all the situations & weighs their importance (perhaps issuing some requests to various memories) & sends out orders. In other words, one is only conscious when there is conflict between modules; otherwise, one is unconscious and the modules continue their work. When carrying a dish from the kitchen to the table, one is largely unconscious - one isn't really thinking, one can't remember much, because not much is happening in consciousness. But if the plate is burning hot? Then all of a sudden there is conflict: the arm neurons are frantically trying to execute the 'flinch' reflex, another part is frantically saying don't drop it we're almost there! and the multiple summaries arrive in consciousness, one suddenly 'wakes up' and decides to drop it or not to drop it, and the deed is done.

Why do people ride roller-coasters? Why do they go into haunted houses? They say it makes them feel alive, that it's vivid and unusual, that it's very exciting.. That it wakes them up.

<http://www.rifters.com/crawl/?p=791>

#### _Full Metal Alchemist_

TODO: there's some general essay I could write about FMA, especially the manga versus anime+movie

> What do you think about Mustang using the philosopher's stone entirely to get his vision back?

Kosher. Mustang didn't ask to see the Gate, and that stone would otherwise have been wasted. He didn't merit his punishment.

> About him not taking the seat as the fuhrer?

With the corrupt establishment toppled, there's no longer any compelling reason for him to be fuhrer. Indeed, his personal failings may mean that it's better for him to not be fuhrer. (What would he do?)

> Ed transmuting the literal gate to break the rules?

That wasn't rule-breaking; that was awesome. It was tremendously satisfying.

One of FMA's running themes was the narrowness of those interested in alchemy. They were interested in it, in using it, in getting more of it. Obviously folks like Shou Tucker or Kimbly sold their soul for alchemy, but less obviously, the other alchemists have been corrupted to some degree by it. Even heroes like Izumi or the Elrics transgressed. Consider Mustang; his connection with Hawkeye was alchemy-based, and only after years did the connection blossom. Consider how little time he spent with Hughes, in part due to his alchemy-based position. Mustang didn't learn until Hughes was gone just how much his friends meant.

Similarly, Greed. His epiphany at the end hammers in the lesson about the value of friends. How did he lose *his* friends? By pursuit of alchemy-based methods of immortality.

That is why Ed was the real hero. Because he realized the Truth of FMA: your relationships are what really matter. No alchemist ever escaped the Gate essentially intact before he did. Why? Because it would never even occur to them to give up their alchemy or what they learned at the Gate.

Have you ever heard of a monkey trap made of a hole and a collar of spikes sticking down? The monkey reaches in and grabs the fruit inside, but his fist is too big to pass back out. If only the stupid monkey would let go of the fruit, he could escape. But he won't. And then the hunter comes.

The alchemists are the monkey, alchemy is the fruit, and the Truth is the hunter. The monkeys put the fruit above their lives, because they think they can have it all. Ed doesn't.

Were there things I disliked? Yes, the whole god thing struck me as strange and ill-thought out. I also disliked the mechanism for alchemy - some sort of Earth energy. I thought that the movie's idea that alchemy was powered by deaths in an alternate Earth to really fit the whole theme of Equivalent Exchange - TANSTAAFL. It's good that the Amestrian alchemy turns out to be [powered by human sacrifice](http://fma.wikia.com/wiki/Alchemy#Secret_Origins) (TANSTAAFL), but that turns out to be due to the Father character blocking the 'real' alchemy, and so, [non-Amestrian alchemy](http://fma.wikia.com/wiki/Alkahestry) turns out to be a free lunch!

#### Fake explanation of cats

    gwern> I often think that cat psychology is harder than dog psychology
    gwern> then I reflect that dogs have co-evolved with us for much longer than cats,
           and dogs have bigger brains as well
    cwillu> no, the dominance hierarchy is firmly established
    gwern> so maybe I only think I understand dogs
    gwern> perhaps under the evolved tricks like eye-following or pointing-understand lies
           a psychology as or more alien than cats
    gwern> *pointing-understanding
    AngryParsley> cat psychology is that they do whatever the hell they want
    fake explanation ^ <http://lesswrong.com/lw/ip/fake_explanations/>

Cats mimic children: <http://news.discovery.com/animals/cats-humans-pets-relationships-110224.html> <http://www.cell.com/current-biology/retrieve/pii/S0960982209011683>

#### Let's nuke Africa

w/r/t existential threats, when is it a better idea to bomb failed nations/continents back to the stone age? <http://lesswrong.com/lw/1qf/the_craigslist_revolution_a_realworld_application/1lud?c=1>

> The absence of rule of law, democratic checks on the military, continual conflict and overall incompetence also increases the chances lab error or misuse of high tech weaponry as technology become more accessible while social, economic and political conditions do not improve.

I just had a fun idea: take this premise, and the demonstrated difficulty of improving Africa, and the idea that the development vs. likeliness-to-screw-everybody-over-with-WMDs curve would be an inverted U, and calculate the point at which it would be better to cut off all aid & begin bombing Africa into (or within) the Stone Age.

> There a high moral cost to beginning bombing Africa.

There is no moral cost by definition; at the point at which we would want to start bombing, the immoral thing is to not bomb. We've bombed many countries for far less than existential threats (arguably, every US bombing campaign back to WWII).

Further, I think you drastically overestimate the chances of homegrown terrorism. Vietnam was long ago. Reports like millions of Iraqi refugees or hundreds of thousands of excess Iraqi deaths merely spark muted partisan arguments about whether the Lancet's statistics are right or not. It's a long way to Tipperary.

> The Global economy would tailspin and the existential risk situation would get a lot worse as a result.

I think you badly overestimate how important Africa is. Even assuming resources cannot be extracted while also bombing the place, Africa isn't that important.

The continental GDP is just 2.7 trillion. Several percentage points of that is foreign aid ([Economy of Africa](!Wikipedia)) and their exports to the rest of the world are small enough that their balance of payments (with the rest of the world) is negative by billions (http://www.africaneconomicoutlook.org/en/data-statistics/).

Now, if Africa disappeared or was suddenly destroyed, I would expect the global financial markets to drop considerably; but they are so skittish they drop at the fall of a hat. The long-term economic impact wouldn't be so bad outside of commodities like coltan. Certainly not so bad as some grey goo getting loose.

(I'd count things like AIDS as further debits to Africa, but obviously that's a sunk cost as far as this suggestion is concerned.)

#### Geneva culinary crimes tribunal

'King Krryllok stated that Crustacistan had submitted a preliminary indictment of Gary Krug, "the butcher of Boston", laying out in detail his systematic genocide of lobsters, shrimp, and others conducted in his Red Lobster franchisee; international law experts predicted that Krug's legal team would challenge the origin of the records under the poisoned tree doctrine, pointing to news reports that said records were obtained via industrial espionage of Red Lobster Inc. When reached for comment, Krug evinced confusion and asked the reporter whether he would like tonight's special on fried scallops'

#### Multiple interpretations theory of humor

My theory is that humor is when there is a connection between the joke & punchline which is obvious to the person in retrospect, but not initially.

Hence, a pun is funny because the connection is unpredictable in advance, but clear in retrospect; Eliezer's joke about the motorist and the asylum inmate is funny because we were predicting some other response other than the logical one; similarly for 'why did the duck cross the road? to get to the other side' is not funny to someone who has never heard any of the road jokes, but to someone who has and is thinking of zany explanations, the reversion to normality is unpredicted.

Your theory doesn't work with absurdist humor. There isn't initially 1 valid decoding, much less 2.

Mm. This might work for some proofs - Lewis Carroll, as we all know, was a mathematician - but a proof for something you already believe that is conducted via tedious steps is not humorous by anyone's lights. Proving P/=NP is not funny, but proving 2+2=3 is funny.

'A man walks into a bar and says "Ow."'

> How many surrealists does it take to change a lightbulb? Two. One to hold the giraffe, and one to put the clocks in the bathtub.

Exactly. What are the 2 valid decodings of that? I struggle to come up with just 1 valid decoding involving giraffes and bathtubs; like the duck crossing the road, the joke is the frustration of our attempt to find the connection.

#### Mr. T(athagata)

idea: Mr. T as modern Bodhisattva. He remains in the world because he pities da fools trapped in the Wheel of Reincarnation.

#### Lip reading website

As far as I can tell, there is no free resource for learning how to [lip read](!Wikipedia), much less free online resources.

Learning to lip read is basically:

1. watch a video with obscured audio
2. guess what you think they said
3. be corrected
4. go to #1

This is eminently doable as a website: YouTube for hosting videos, [Amazon Mechanical Turk](!Wikipedia) or similar services for generating Free videos, and perhaps a [SRS algorithm](Spaced repetition) for scheduling periodic reviews videos of particular words or sentences.

Lip reading is useful to know. There are roughly [28 million](http://www.audiologyonline.com/articles/pf_article_detail.asp?article_id=1204) people in the US with hearing issues and as the [Baby Boomers](!Wikipedia) age and lose hearing, many will want to learn; estimates of Baby Boomers who will have any degree of hearing loss range from 20-60%. See:

- <http://articles.baltimoresun.com/1999-09-26/news/9909280440_1_hearing-loss-cells-hearing-problems>
- <http://www.iltsource.com/customer-downloads/Baby_Boomer_Study.pdf> / <http://iltsource.com/customer-downloads/Clarity_Boomer_2006.pdf>
- <http://www.healthyhearing.com/articles/45443-hearing-loss-baby-boomers>
- <http://archives.cnn.com/2000/HEALTH/aging/04/19/hearing.loss.wmd/index.html>

For older Americans, the rate is [63%](http://www.nytimes.com/2011/03/08/health/research/08aging.html).

Nor is the loss limited to Baby Boomers; a report in the August 2010 issue of the American Medical Association estimated that over the 15 years from 1995 to 2010, teen hearing loss rate increased 30%^[Cited in _The Futurist_ November 2010], to a total of all teens with detectable hearing loss of 19.5% of 12-19 year olds (and a similar increase in the number with mild hearing loss).[^nytimesloss]

[^nytimesloss]: ["Childhood: Hearing Loss Grows Among Teenagers"](http://www.nytimes.com/2010/08/24/health/research/24child.html), _New York Times_:

    > The new study, published Wednesday in _The Journal of the American Medical Association_, analyzed data on about 1,771 youngsters aged 12 to 19 who participated in the National Health and Nutrition Examination Survey of 2005-6, and compared the prevalence of hearing loss with that of youngsters who took part in the survey in 1988-94. The percentage with at least slight hearing loss increased by 30%, to 19.5% from 14.9% in the earlier study. For most the hearing loss is slight enough they may not even notice.

The small industry of lip reading and the international scattering of lip reading classes shows that people will pay hundreds of dollars and go places to learn it.

##### Costs

Optimistically, one-time >100$ for content, 20$ / month then on, and a substantial time investment in putting together a site and a process for acquiring or creating video.

###### Technical
####### Hosting

Assuming videos are hosted on YouTube or [Amazon S3](!Wikipedia), the website would require extremely little bandwidth and accommodate >1000 users at ~20$ a month:

Assume a webpage requires 100KB to be loaded (very pessimistic), and that a user spends 1 hour a day using the website (30 hours a month), going to a new page every minute. That user will use $100\text{KB} \times (30 \times 60)$, or 180000KB, or 180MB of bandwidth. [Linode's](http://www.linode.com/) cheapest offering at \$20/month pays for 200GB of bandwidth; $\frac{200\text{GB}}{180\text{MB}}$ = 1112 users. A domain name costs ~$10 a year, or ~$1 a month.

More reasonable would be assuming 10KB per pageload, and 10 hours a month, cutting the per-user bandwidth down to $10 \times 10 \times 60$, or 6MB, and assuming fewer than 1000 users; then hosting could be even cheaper. [DreamHost](http://www.dreamhost.com/hosting.html) is known for screwing over its more-demanding customers, but should be reliable enough here; their hosting is \$9 a month.

####### Coding

Obviously a site custom-made for lip reading & very user-friendly doesn't exist. I'd have to code one or reuse some framework, though offhand I don't know of any really suited for the task. It'd be a big coding task - at least dozens of hours to learn the specific technologies and build a prototype. But then, I can't really count my own time as a cost - I'd just spend the time reading elsewise.

###### Marketing

Unknown. These sorts of sites seem to do best with word of mouth marketing, so who knows? Maybe just time.

###### Lipreading content

The content is the wildcard. There are a couple possible sources:

- There's a cottage industry of books and occasional CDs/DVDs, whose copyright obviously would be far too expensive to purchase.
- Hiring professionals to make lip movements also is obviously right out. To make it worth their while and to get at least 10 hours of material would take thousands of dollars.
- Online freelancing sites. I have a theory that one doesn't *want* professionals because one intends to use lip reading in real life, to read the lips of the 'amateurs' one interacts with.
     - I mentioned Mechanical Turk, but that may not be appropriate; many Turkers do not have cameras or webcams, and it may not be doable to ask them to submit videos through Amazon, but Turkers could definitely be used to verify that the person in a clip is saying the things they are supposed to say. (This would cost ~10 per review, and usually one double-checks with multiple Turkers, so 20 a clip.)
    - Other freelancing sites like guru.com list video/photo people working for \$20-40 an hour. I figure that means that amateurs in both department will be no more than half that, \$10/hour. 10 hours of content then would be ~$100.

##### Revenue

Ads, obviously. A competitor would be lipreading.com; so that's a reasonable starting point. With zero effort at doing anything other than selling a DVD, some estimates of its ad revenue are [44](http://www.cubestat.com/www.lipreading.com) to [$2.22](http://www.websiteoutlook.com/www.lipreading.com) a day. At hosting costs of \$21 a month or <75 a day, the site could at least pay its on-going expenses.

##### Links

Random links that may be of interest:

- <http://deafness.about.com/cs/communication/a/lipreading.htm>
- <https://web.archive.org/web/20131227150022/http://www.hearinglossweb.com/Issues/OralCommunications/Strategies/sphrd/allen.htm>
- <http://www.usnews.com/mobile/articles_mobile/computers-might-make-learning-lip-reading-easier/index.html>
- <https://web.archive.org/web/20130728045156/http://lipread.com.au/Products.html>
- <http://www.bellaonline.com/subjects/2664.asp>
- <http://www.deafcando.org.au/index.php?/deafcando/services/C81/>
- <http://www.adcohearing.com/bvs_hear_loss.html>

#### Venusian Revolution

> "Venus is a great example. It does pretty well in the equation, and actually gets a value of about one and a half quadrillion dollars if you tweak its reflectivity a bit to factor in its bright clouds. This echoes what unfolded for Venus in the first half of the 20th century, when astronomers saw these bright clouds and thought they were water clouds, and that it was really humid and warm on the surface. It gave rise to this idea in the 1930s that Venus was a jungle planet. So you put this in the formula, and it has an explosive valuation. Then you'd show up and face the reality of lead melting on the surface beneath sulfuric-acid clouds, and everyone would want their money back!
>
> If Venus is valued using its actual surface temperature, it's like 10^-12^ of a single cent. @home.com was valued on the order of a billion dollars for its market cap, and the stock is now literally worth zero. Venus is unfortunately the @home.com of planets.
>
> It's tragic, amazing, and extraordinary, to think that there was a small window, in 1956, 1957, when it wasn't clear yet that Venus was a strong microwave emitter and thus was inhospitably hot.
>
> The scientific opinion was already going against Venus having a clement surface, but in those years you could still credibly imagine that Venus was a habitable environment, and you had authors like Ray Bradbury writing great stories about it. At the same time, the ability to travel to Venus was completely within our grasp in a way that, shockingly, it may not be now. Think what would have happened, how history would've changed, if Venus had been a quadrillion-dollar world, we'd have had a virgin planet sitting right next door. Things would have unfolded in an extremely different way. we'd be living in a very different time." --Greg Laughlin, interviewed in ["Cosmic Commodities: How much is a new planet worth?"](http://www.boingboing.net/2011/02/03/cosmic-commodities-h.html)

Sounds like a good alternate history novel. The space race heats up in the 1950s, with a new planet at stake. Of course, along the lines of [Peter Thiel's reasoning](http://www.hoover.org/publications/policy-review/article/5646) about France & John Law & the Louisiana Territory, the 'winner' of such a race would probably suffer the [winner's curse](!Wikipedia). (Don't go mine for gold yourself; sell pick-axes to the miners instead.)

#### Pseudonymity

'Gwern Branwen' is the pseudonym I use to traffick online; I was long paranoid and sought to cleanly separate my online and offline life. (With the exception of commercial transactions - I didn't particularly mind if Amazon.com was able to link my credit card under my real name with my email account.) This was a good idea because I picked up the occasional online enemy who could annoy me.

As Maru Dubshinki, I made an enemy of Daniel Brandt; and more recently, there was an attempted harassment of me in real life, /b/-style, over Neil Gaiman's [Scientology connections](!Wikipedia "Neil Gaiman#Early life") which failed miserably when their investigation dead-ended at calling up [RIT](!Wikipedia) and asking whether a Gwern Branwen happened to work there. Which of course he did not. My biggest mistake, when I still cared about solid pseudonymity, was occasionally joining the [#wikipedia IRC channel](irc://irc.freenode.net#wikipedia) without my [IRC cloak](!Wikipedia) and thereby exposing my IP address; Daniel Brandt was able to narrow me to someone living in [Suffolk County, New York](!Wikipedia).

So, I had always expected a break in my pseudonymity to come from the online direction. I hadn't expected it to come from the other direction. Then one day in 2008 or 2009, it did...

I was idling in #wikipedia, discussing Wikipedia matters, as one does on the weekend, when Werdna & FlyingToaster happened to also be hanging out with my sister at CMU; they fell to discussing Wikipedia and she mentioned that her younger brother did an awful lot of Wikipedia editing (as I [do](Wikipedia resume)), and Boriss inquired as to who I was when I was not at home. It was some strange nickname she couldn't remember. But she *did* remember that I frequently edited Japanese literature articles and the nick started with 'g' or something. (I'm particularly proud of [Fujiwara no Teika](!Wikipedia), incidentally.)

Well, she says, I don't know many Japanese literature editors; but I do chat with this guy named Gwern on IRC who does a lot of that sort of thing. That's it!, my sister says (as she casually destroys my pseudonymity), the odd nickname was 'Gwern'! They both have a good laugh about what a small world it is, and then they go on IRC and knock me for a loop by 'guessing' personal details until they finally reveals that my 'hot sister' is there providing information. And of course since my sister now works in San Francisco, and Garrett is a PHP developer contracted to the Wikimedia Foundation & Boriss a developer for the Mozilla Foundation (both headquartered in or near SF), she or he occasionally visits & stays with my sister and I have to hear about it online from them. Oy vey.

#### Efficient natural language

A single English character can be expressed (in [ASCII](!Wikipedia)) using a byte, or ignoring the wasted high-order bit, a full 7 bits. But English is pretty predictable, and isn't using those 7 bits to good effect. [Claude Shannon](!Wikipedia) found that each character was carrying more like 1 (0.6-1.3) bit of unguessable information[^Shannon] (differing from genre to genre[^Kolmogorov]); Hamid Moradi found 1.62-2.28 bits on various books[^Moradi]; [Brown et al 1992](http://www.aclweb.org/anthology/J/J92/J92-1002.pdf "An estimate of an upper bound for the entropy of English") found <1.72 bits; Teahan & Cleary 1996 got 1.46[^Teahan-PPM]; Cover & King 1978 came up with 1.3 bits[^cover]; and Behr et al 2002 found 1.6 bits for English and that compressibility was similar to this when using translations in Arabic/Chinese/French/Greek/Japanese/Korean/Russian/Spanish (with Japanese as an outlier)[^Behr-ppm].  In practice, existing algorithms can make it down to just 2 bits to represent a character, and theory suggests the true entropy was around 0.8 bits per character.[^Grassberger] (This, incidentally implies that the highest bandwidth human speech can attain is around 55 bits per second.[^rapping]) Languages can vary in how much they convey in a single 'word' - ancient Egyptian conveying ~7 bits per word and modern Finnish around 10.4[^PLOS] (and word ordering adding another at least 3 bits over most languages); but we'll ignore those complications.

[^Shannon]: Claude E. Shannon, ["Prediction and entropy of printed English"](http://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf), _Bell Systems Technical Journal_, pp. 50-64, Jan. 1951
[^Kolmogorov]: ["The Man Who Invented Modern Probability"](http://nautil.us/issue/4/the-unlikely/the-man-who-invented-modern-probability), _Nautilus_ issue 4:

    > To measure the artistic merit of texts, Kolmogorov also employed a letter-guessing method to evaluate the entropy of natural language. In information theory, entropy is a measure of uncertainty or unpredictability, corresponding to the information content of a message: the more unpredictable the message, the more information it carries. Kolmogorov turned entropy into a measure of artistic originality. His group conducted a series of experiments, showing volunteers a fragment of Russian prose or poetry and asking them to guess the next letter, then the next, and so on. Kolmogorov privately remarked that, from the viewpoint of information theory, Soviet newspapers were less informative than poetry, since political discourse employed a large number of stock phrases and was highly predictable in its content. The verses of great poets, on the other hand, were much more difficult to predict, despite the strict limitations imposed on them by the poetic form. According to Kolmogorov, this was a mark of their originality. True art was unlikely, a quality probability theory could help to measure.
[^cover]: T. M. Cover, ["A convergent gambling estimate of the entropy of English"](http://www-isl.stanford.edu/~cover/papers/transIT/0413cove.pdf), _IEEE Trans. Information Theory_, Volume IT-24, no. 4, pp. 413-421, 1978
[^Teahan-PPM]: ["The entropy of English using PPM-based models"](/docs/statistics/1996-teahan.pdf)
[^Behr-ppm]: ["Estimating and comparing entropy across written natural languages using PPM compression"](http://vuz.zaznai.ru/tw_files2/urls_5/27/d-26772/7z-docs/1.pdf)
[^Moradi]: H. Moradi, ["Entropy of English text: Experiments with humans and a machine learning system based on rough sets"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.5610&rep=rep1&type=pdf), _Information Sciences, An International Journal_ 104 (1998), 31-47
[^Grassberger]: Peter Grassberger, ["Data Compression and Entropy Estimates by Non-sequential Recursive Pair Substitution"](http://arxiv.org/abs/physics/0207023) (2002)
[^rapping]: Rapper Ricky Brown apparently set a rapping speed record in 2005 with ["723 syllables in 51.27 seconds"](http://sparkplugged.net/2007/10/outsider-speed-rap-extraordinaire/), which is 14.1 syllables a second; if we assume that a syllable is 3 characters on average, and go with an estimate of 1.3 bits per character, then the bits per second (b/s) is $14.1 \times 3 \times 1.3$, or 55 b/s. This is something of a lower bound; Korean rapper [Outsider](!Wikipedia "Outsider (rapper)") claims [17](http://www.arirang.co.kr/TV2/Star_Focus.asp?code=Ki3&F_KEY=612) syllables, which would be 66 b/s.
[^PLOS]: See ["Universal Entropy of Word Ordering Across Linguistic Families"](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0019875), Montemurro 2011

Whatever the true entropy, it's clear existing English spelling is pretty wasteful. How many characters could we get away with? We could ask, how many bits does it take to uniquely specify 1 out of, say, 100,000 words?  Well, _n_ bits can uniquely specify 2^_n_^ items; we want at least 100,000 items covered by our bits, and as it happens, 2^17^ is 131072, which gives us some room to spare. (2^16^ only gives us 65536, which would be enough for a pidgin or something.) We already pointed out that a character can be represented by 7 bits (in ASCII), so each character accounts for 7 of those 17 bits. 7+7+7 > 17, so 3 characters. In this encoding, one of our 100,000 words would look like 'AxC' (and we'd have 30,000 unused triplets to spare). That's not so bad.

But as has often been [pointed out](http://www.zompist.com/kitlong.html#howmany), one of the advantages of our verbose system which can take as many as 9 characters to express a word like 'advantage' is that the waste also lets us understand partial messages. The example given is a disemvoweled sentence: 'y cn ndrstnd Nglsh txt vn wtht th vwls'. Word lengths themselves correspond roughly to frequency of use^[Which would be a sort of [Huffman encoding](!Wikipedia); see also ["Entropy, and Short Codes"](http://lesswrong.com/lw/o1/entropy_and_short_codes/).] or average information content.^[["Word lengths are optimized for efficient communication"](http://m.pnas.org/content/108/9/3526.short): "We demonstrate a substantial improvement on one of the most celebrated empirical laws in the study of language, [Zipf's](!Wikipedia "Zipf's law") 75-y-old theory that word length is primarily determined by frequency of use. In accord with rational theories of communication, we show across 10 languages that average information content is a much better predictor of word length than frequency. This indicates that human lexicons are efficiently structured for communication by taking into account interword statistical dependencies. Lexical systems result from an optimization of communicative pressures, coding meanings efficiently given the complex statistics of natural language use."]

The answer given when anyone points out that a compressed file can be turned to nonsense by a single error is that errors aren't that common, and the 'natural' redundancy is *very* inefficient in correcting for errors[^vowel], and further, while there are some reasons to expect languages to have evolved towards efficiency, we have at least 2 arguments that they may yet be very inefficient:

1. natural languages differ dramatically in almost every way, as evidence by the difficulty Chomskyians have in finding the [deep structure](!Wikipedia) of language; for example, average word length differs considerably from language to language. (Compare German and English; they are closely related, yet one is shorter.)

    And specifically, natural languages seem to vary considerably in how much they can convey in a given time-unit; speakers make up for low-entropy syllables by speaking faster (and vice-versa), but even after multiply the number of syllables by rate, the languages still differ by as much as 30%[^idr].
2. speakers may prefer a concise short language with powerful error-detecting and correction, since speaking is so tiring and metabolically costly; but listeners would prefer not to have to think hard and prefer that the speaker do all the work for them, and would thus prefer a less concise language with less powerful error-detection and correction[^Cancho]

[^idr]: An early study found that reading speed in Chinese and English were similar when the information conveyed was similar (["Comparative patterns of reading eye movement in Chinese and English"](/docs/1985-sun.pdf "Sun et al 1985")); ["A cross-language perspective on speech information rate"](http://www.ddl.ish-lyon.cnrs.fr/fulltext/pellegrino/Pellegrino_to%20appear_Language.pdf) investigated exactly how a number of languages traded off number of syllables versus talking speed by recording a set of translated stories by various native speakers, and found that the two parameters did not counter-balance exactly:

    > Information rate is shown to result from a density/rate trade-off illustrated by a very strong negative correlation between the ID~L~ and SR~L~. This result confirms the hypothesis suggested fifty years ago by Karlgren (1961:676) and reactivated more recently (Greenberg and Fosler-Lussier (2000); Locke (2008)): 'It is a challenging thought that general optimalization rules could be formulated for the relation between speech rate variation and the statistical structure of a language. Judging from my experiments, there are reasons to believe that there is an equilibrium between information value on the one hand and duration and similar qualities of the realization on the other' (Karlgren 1961). However, IR~L~ exhibits more than 30% of variation between Japanese (0.74) and English (1.08), invalidating the first hypothesis of a strict cross-language equality of rates of information.
[^Cancho]: ["Least effort and the origins of scaling in human language"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC298679/), Cancho 2002. From the abstract:

    > In this article, the early hypothesis of Zipf of a principle of least effort for explaining the law is shown to be sound. Simultaneous minimization in the effort of both hearer and speaker is formalized with a simple optimization process operating on a binary matrix of signal-object associations. Zipf's law is found in the transition between referentially useless systems and indexical reference systems. Our finding strongly suggests that Zipf's law is a hallmark of symbolic reference and not a meaningless feature. The implications for the evolution of language are discussed

One interesting natural experiment in binary encoding of languages is the [Kele language](!Wikipedia "Kele language (Congo)"); its high and low tones add 1 bit to each syllable, and when the tones are translated to drumbeats, it takes about 8:1 repetition:

> Kele is a tonal language with two sharply distinct tones. Each syllable is either low or high. The drum language is spoken by a pair of drums with the same two tones. Each Kele word is spoken by the drums as a sequence of low and high beats. In passing from human Kele to drum language, all the information contained in vowels and consonants is lost...in a tonal language like Kele, some information is carried in the tones and survives the transition from human speaker to drums. The fraction of information that survives in a drum word is small, and the words spoken by the drums are correspondingly ambiguous. A single sequence of tones may have hundreds of meanings depending on the missing vowels and consonants. The drum language must resolve the ambiguity of the individual words by adding more words. When enough redundant words are added, the meaning of the message becomes unique.
>
> ...She [his wife] sent him a message in drum language...the message needed to be expressed with redundant and repeated phrases: "White man spirit in forest come come to house of shingles high up above of white man spirit in forest. Woman with yam awaits. Come come." Carrington heard the message and came home. On the average, about eight words of drum language were needed to transmit one word of human language unambiguously. Western mathematicians would say that about one eighth of the information in the human Kele language belongs to the tones that are transmitted by the drum language.^[["How We Know"](http://www.nybooks.com/articles/archives/2011/mar/10/how-we-know/?pagination=false), by [Freeman Dyson](!Wikipedia) in _[The New York Review of Books](!Wikipedia)_ (review of James Gleick's [_The Information: A History, a Theory, a Flood_](http://www.amazon.com/The-Information-History-Theory-Flood/dp/1400096235/))]

With a good [FEC](!Wikipedia "Forward error correction"), you can compress and eat your cake too. Exactly how much error we can detect or correct is given by the [Shannon limit](!Wikipedia):

[^vowel]: If we argue that vowels are serving a useful purpose, then there's a problem. There are only 3 vowels and some semi-vowels, so we have at the very start given up at least 20 letters - tons of possibilities. To make a business analogy, you can't burn 90% of your revenue on booze & parties, and make it up on volume. Even the most trivial error-correction is better than vowels. For example, the last letter of every word could specify how many letters there were and what fraction are vowels; 'a' means there was 1 letter and it was a vowel, 'A' means 1 consonant, 'b' means 2 vowels, 'B' means 2 consonants', 'c' means 1 vowel & 1 consonant (in that order), 'C' means the reverse, etc. So if you see 'John looked _tc Julian', the trailing 'c' implies the missing letter is a vowel, which could only be 'a'.

    This point may be clearer if we look at systems of writing. Ancient Hebrew, for example, was an [abjad](!Wikipedia) script, with vowel-indications (like the [niqqud](!Wikipedia)) coming much later. Ancient Hebrew is also a dead language, no longer spoken in the vernacular by its descendants until the [Revival of the Hebrew language](!Wikipedia) as [Modern Hebrew](!Wikipedia), so oral traditions would not help much. But nevertheless, the Bible is still very well-understood, and the lack of vowels rarely an issue; even the complete absence of modern punctuation didn't cause very many problems. The examples I know of are striking for their unimportance - the exact pronunciation of the [Tetragrammaton](!Wikipedia) or whether the [thief crucified](!Wikipedia "Saint Dismas#Today... in paradise") with Jesus immediately went to heaven.

> $\frac{\text{channelCapacity}}{1 - (-(\text{mistakeRate} \times log_2(\text{mistakeRate}) + (1 - \text{mistakeRate}) \times log_2(1 - \text{mistakeRate})))}$

If we suppose that each word is 3 characters long, and we get 1 error every 2 words on average, our channel capacity is 6 characters' of bits (or 7*6, or 42), and our mistake rate 1/6 of the characters (or 7/42), substituting in we get:

> $\frac{42}{1 - (-(\frac{1}{6} \times log_2(\frac{1}{6}) + (1 + \frac{1}{6}) \times log_2(1 - \frac{1}{6})))}$

Or in Haskell, we evaluate (using [logBase](!Hoogle) because [log](!Hoogle) is the natural logarithm, not the binary logarithm used in information theory):

~~~~{.haskell}
42 / 1 - (-(1/6 * logBase 2 (1/6) + (1 - 1/6) * logBase 2 (1 - 1/6)))
~~~~

Which evaluates to ~41. In other words, we started with 42 bits of possibly corrupted information, assumed a certain error rate, and asked how much could we communicate given that error rate; the difference is whatever we had to spend on ECC - 1 bit. Try comparing that to a vowel-scheme. The vowel would not guarantee detection or correction (you may be able to decade 'he st' as 'he sat', but can you decode 'he at' correctly?), and even worse, vowels demand an entire character, a single block of 7/8 bits, and can't be subtly spread over all the characters. So if our 2 words had one vowel, we just blew 7 bits of information on that and that alone, and if there were more than 1 vowel...

Of course, the Shannon limit is the theoretical asymptotic ideal and requires complex solutions humans couldn't mentally calculate on the fly. In reality, we would have to use something much simpler and hence couldn't get away with devoting just 1 bit to the FEC. But hopefully it demonstrates that vowels are a really atrocious form of error-correction. What would be a good compromise between humanly possible simplicity and inefficiency (compared to the Shannon limit)? I don't know.

The existence of similar neuronal pathways [across languages & cultures for reading](http://www.researchgate.net/publication/5882262_Cultural_recycling_of_cortical_maps/file/72e7e52a0c385073b5.pdf "'Cultural Recycling of Cortical Maps', Dehaene & Cohen 2007") suggests that there could be hard limits on what sorts of languages can be efficiently understood - for example, characters in all alphabets taking on average taking 3 strokes to write. [Richard Hamming](!Wikipedia), who invented much of the early error-correcting codes, once devised a scheme for IBM (similar to the [ISBN check-digit](!Wikipedia "International Standard Book Number#Check digits")); number letters or characters from 1 to 37, and add them all up modulo 37, which is the new prefix to the word. This checksum handles what Hamming considered the most common human errors like repeating or swapping digits.[^hamming] A related idea is encoding bits into audible words which are as phonetically distant as possible, so a binary string (such as a cryptographic hash) can be spoken and heard with minimum possibility of error; see [PGP word list](!Wikipedia) or the 32-bit [Mnemonic encoder](http://web.archive.org/web/20101031205747/http://www.tothink.com/mnemonic/) scheme.

[^hamming]: from "Coding Theory II" in [_The Art of Doing Science and Engineering_](http://www.amazon.com/The-Doing-Science-Engineering-ebook/dp/B000P2XFPA/), Richard W. Hamming 1997:

    > I was once asked by AT&T how to code things when humans were using an alphabet of 26 letter, ten decimal digits, plus a 'space'. This is typical of inventory naming, parts naming, and many other naming of things, including the naming of buildings. I knew from telephone dialing error data, as well as long experience in hand computing, humans have a strong tendency to interchange adjacent digits, a 67 is apt to become a 76, as well as change isolated ones, (usually doubling the wrong digit, for example a 556 is likely to emerge as 566). Thus single error detecting is not enough...Ed Gilbert, suggested a weighted code. In particular he suggested assigning the numbers (values) 0, 1, 2, ..., 36 to the symbols 0,1,..., 9, A, B, ..., Z, space.
    >
    > ...To encode a message of n symbols leave the first symbol, _k_=1, blank and whatever the remainder is, which is less than 37, subtract it from 37 and use the corresponding symbol as a check symbol, which is to be put in the first position. Thus the total message, with the check symbol in the first position, will have a check sum of exactly 0. When you examine the interchange of any two different symbols, as well as the change of any single symbol, you see it will destroy the weighted parity check, modulo 37 (provided the two interchanged symbols are not exactly 37 symbols apart!). Without going into the details, it is essential the modulus be a prime number, which 37 is.
    >
    > ...If you were to use this encoding, for example, for inventory parts names, then the first time a wrong part name came to a computer, say at transmission time, if not before (perhaps at order preparation time), the error will be caught; you will not have to wait until the order gets to supply headquarters to be later told that there is no such part or else they have sent the wrong part! Before it leaves your location it will be caught and hence is quite easily corrected at that time. Trivial? Yes! Effective against human errors (as contrasted with the earlier white noise), yes!

##### External links

- ["One world, how many bytes?"](http://itre.cis.upenn.edu/~myl/languagelog/archives/002379.html)
- ["Comparing communication efficiency across languages"](http://itre.cis.upenn.edu/~myl/languagelog/archives/005526.html); ["Mailbag: comparative communication efficiency"](http://itre.cis.upenn.edu/~myl/languagelog/archives/005530.html)
- ["The Entropy of English vs. Chinese"](http://lingpipe-blog.com/2008/04/11/the-entropy-of-english-vs-chinese/)

#### Powerful natural languages

> Designed formal notations & distinct vocabularies are often employed in STEM fields, and these specialized languages are credited with greatly enhancing research & communication. Many philosophers and other thinkers have attempted to create more generally-applicable designed languages for use outside of specific technical fields to enhance human thinking, but the empirical track record is poor and no such designed language has demonstrated substantial improvements to human cognition such as resisting cognitive biases or logical fallacies. I suggest that the success of specialized languages in fields is inherently due to encoding large amounts of previously-discovered information specific to those fields, and this explains their inability to boost human cognition across a wide variety of domains.

Alongside the axis of efficiency in terms of time or words or characters to convey a certain amount of information, one might think about natural languages in terms of some sort of 'power' metric, akin to the [Sapir-Whorf hypothesis](!Wikipedia), in which some languages are better at allowing one to think or express important thoughts.
For example, Chinese is sometimes said to be too 'concrete' and makes scientific thought difficult compared to some other languages; less controversially, mathematicians & physicists unanimously agree that notation is extremely important and that a good notation for a topic can make routine results easier to create & understand and also can enable previously unthinkable thoughts and suggest important research directions - Arabic numerals vs Roman numerals, Newton's calculus notation versus Leibniz's, [Maxwell's equations](!Wikipedia) etc.

This has been taken to extremes: if good notation can help, surely there is some designable ideal language in which all thoughts can be expressed most perfectly in a way which quickly & logically guides thoughts to their correct conclusions without falling prey to fallacies or cognitive biases, and make its users far more intelligent & effective than those forced to use natural languages.
The usual example here being Leibniz's proposed logical language in which philosophical disputes could be resolved simply by calculation, or [Wilkins's "Real character"](!Wikipedia "An Essay towards a Real Character and a Philosophical Language").
The hope that better languages can lead to better human thought has also been a motivation for developing [conlangs](!Wikipedia "Constructed language") like [Lojban](!Wikipedia), with rationalized vocabulary & grammars.
Conlangs, and less radical linguistic innovations like [E-Prime](!Wikipedia) (and coining neologisms for new concepts) show up occasionally in movements like [General Semantics](!Wikipedia) and [quite often in science fiction](http://www.projectrho.com/public_html/rocket/futurelang.php); and of course many political or social or philosophical movements have explicitly wanted to change languages to affect thought (from Communism, memorably dramatized in _[Nineteen Eighty-Four](!Wikipedia)_, to feminism, post-modernisms, [logical positivism](!Wikipedia), or [Ukrainian nationalists](http://www.newyorker.com/magazine/2012/12/24/utopian-for-beginners "Utopian for Beginners: An amateur linguist loses control of the language [Ithkuil] he invented"); I've noticed that this trend seems particularly pronounced in the mid-20th century).

This is sensible, since it seems clear that our language influences our thoughts (for better or worse), that there's no particularly reason to expect evolved languages like English to be all that great (they clearly are brutally irregular and inefficient in ways that have no possible utility, and comparisons of writing systems are equally clear that some systems like [hangul](!Wikipedia) are just plain better), and if better notation and vocabulary can be so helpful in STEM areas, can constructed languages be helpful in general?

I would have to say that the answer appears to be *no*.
First, the Sapir-Whorf hypothesis has largely panned out as trivialities: there are few or no important cross-cultural differences ascribable to languages' grammars or vocabularies.
Nor do we see huge differences between populations speaking radically different languages when they learn different natural languages - Chinese people might find it somewhat easier to think scientifically in English than in Mandarin, but it hardly seems to make a huge difference.
Secondly, looking over the history of such attempts, there does not appear to be any noticeable gain from switching to E-Prime or Lojban etc.
Movements like General Semantics have not demonstrated notable additional real-world success in their adherents (although I do think that such practices as E-Prime do improve philosophical writing)
Attempts at general-purpose efficient languages arguably have been able to decrease the learning time for conlangs and provide modest educational advantages; [domain-specific languages](!Wikipedia) for particular fields like chemistry & physics & computer science (eg [regular expressions](!Wikipedia), [BackusNaur form](!Wikipedia), [SQL](!Wikipedia)) have continued to demonstrate their tremendous value; but constructed general-purpose languages have not made people either more rational or more intelligent.

Why not?
Thinking about it from an [algorithmic information theory](!Wikipedia) sense, all Turing-complete computational languages are equivalently expressive, because any program that can be written in one language can be written in another by first writing an interpreter for the other and then running the program; so with a constant length penalty (the interpreter), all languages are about equivalent.
The constant penalty might be large & painful, though, as a language might be a [Turing tarpit](!Wikipedia), where everything is difficult to say.
Or from another information theory perspective, most bitstrings are uncompressible, most theorems are unprovable, most programs are undecidable etc; a compression or prediction algorithm only works effectively on a very small subset of possible inputs (while working poorly or not at all on random inputs).
So from that perspective, any language is on average just as good as any other - giving a [no free lunch theorem](!Wikipedia).
The goal of research, then, is to find algorithms which trade off performance on data that does not occur in the real world in exchange for performance on the kinds of regularities which do turn up in real-world data.

So if all languages are equivalent, why *do* domain-specific languages work?
Well, they work because they are inherently not *general*: they encode domain knowledge.
One could write the equivalent of a regular expression in [Brainfuck](!Wikipedia), but it would take a lot longer than writing a regular regular expression, because the regular expressions privilege a small subset of possible kinds of text matches & transformations and are not nearly as general as a Brainfuck program could be.
Similarly, any mathematical language privileges certain approaches and theorems, and this is why they are helpful: they assign short symbol sequences to common operations, while uncommon things become long & hard.

This makes sense historically, as few notations are dropped from the sky in finished form and only then assisting major discoveries.
Leibniz's calculus notation evolved from its origins; syntax for logics developed considerably (the proofs of [Frege](!Wikipedia "Begriffsschrift") are much harder to read than contemporary notation, and presentations of logic are considerably aided by innovations such as [truth tables](!Wikipedia)); the [history of Maxwell's equations](!Wikipedia "History of Maxwell's equations") is one of constant simplification & recasting into new notation/mathematics, where they were first published in a form far more baroque of 20 equations involving [quaternions](!Wikipedia) and slowly evolved by [Oliver Heaviside](!Wikipedia) & others into the familiar 4 differential equations written using hardly 23 characters ("Simplicity does not precede complexity, but follows it."); programming languages have also evolved from their often-bizarre beginnings like [Plankalkl](!Wikipedia) to far more readable programming languages like Python & Haskell.
So the information embedded in the languages did not come from nowhere - the creation & evolution of languages reflects the hard-earned knowledge of practitioners & researchers about what common uses are and what should be made easy to say, and what abstract patterns can be generalized and named and made [first-class](!Wikipedia "First-class citizen") citizens in a language.
The effectiveness of these languages, in usual practice and in assisting major discoveries, then comes from the notation reducing friction in executing normal research (eg one could multiply any Roman numerals, but it is much easier to multiply Arabic numerals), and from suggesting areas which logically follow from existing results by symmetry or combinations but which are currently gaps.

To be very effective, a general-purpose language would have to encode some knowledge or algorithm which offers considerable gains across many human domains.
There doubtless are such pieces of knowledge, but it would appear that humans learn them adequately without a language encoding them; and on the other hand, there are countless extremely important pieces of knowledge for individual domains, which would be worth encoding into a language, except most people have no need to work in those small domains, so... we get jargon and domain-specific languages, but not General Semantics superheros speaking a powerful conlang.

General-purpose languages would only encode general weak knowledge, such as (taking an AI perspective) properties like object-ness and a causally sparse world in which objects and agents can be meaningfully described with short descriptions & the intentional stance, and vocabulary encodes a humanly-relevant description of human life (eg the [lexical hypothesis](!Wikipedia) in which regularities in human personality are diffusely encoded into thousands of words - if personality became a major interest, people would likely start using a much smaller & more refined set of personality words like the [Big Five facets](!Wikipedia "Big Five personality traits")).
As a specific domain develops domain-specific knowledge which would be valuable to have encoded into a language, it gradually drifts from the general-purpose language and, when the knowledge is encoded as words, becomes jargon-dense, and when it can be encoded in the syntax & grammar of symbols, a formal notation.
("Everything should be built top-down, except the first time.")
Much like how scientific fields fission as they develop. (["There will always be things we wish to say in our programs that in all known languages can only be said poorly."](http://www.cs.yale.edu/homes/perlis-alan/quotes.html "'Epigrams in Programming', Perlis"))

The universal natural language serves as a 'glue' or 'host' language for communicating things not covered by specific fields and for combining results from domain-specific languages, and are jack of all trades: not good at anything in particular, but, shorn of most accidental complexity like grammatical gender or totally randomized spelling ("Fools ignore complexity. Pragmatists suffer it. Some can avoid it. Geniuses remove it."), about good as any competitor at everything.
("Optimization hinders evolution.")
And to the extent that the domain-specific languages encode anything generally important into vocabulary, the host general-purpose language can try to steal the idea (shorn of its framework) as isolated vocabulary words.

So the general-purpose languages generally remain equivalently powerful, and are *reasonably* efficient enough that switching to conlangs does not offer a sufficient advantage.

This perspective explains why we see powerful special-purpose languages and weak general-purpose languages: the requirement of encoding important information forces general languages into becoming ever narrower if they want to be powerful.

It also is an interesting perspective to take on intellectual trends, ideological movements, and magical/religious thinking.
Through history, many major philosophical/religious/scientific thinkers have been deeply fascinated by etymology, philology, and linguistics, indeed, to the point of basing major parts of their philosophies & ideas on their analysis of words.
(It would be invidious to name specific examples.)
These linguistically-based claims are frequently among the most bizarre & deeply wrong parts of their beliefs, and it would be reasonable to say that they have been blinded by a love of words - forgetting that a word is merely a word, and the map is not the territory.
What are the problems there? [What is wrong with our thoughts and uses of words?](http://web.maths.unsw.edu.au/~jim/wrongthoughts.html "'What is Wrong with Our Thoughts? A Neo-Positivist Credo', Stove 1991")
At least one problem is the implicit belief that linguistic analysis can tell us anything more than some diffuse population beliefs about the world or the relatively narrow questions of formal linguistics (eg about history or language families), the belief that linguistic analysis can reveal deep truths about the meaning of concepts, about the nature of reality, the gods, moral conduct, politics, etc - that we can *get out* of words far more than anyone ever *put in*.
Since words & languages have evolved through a highly random natural process filled with arbitrariness, contingency, and constant mutation/decay, there is not much information in them, there is no one who has been putting information in them, and to the extent that anyone has, their deep thoughts & empirical evidence are better accessed through their explicit writings, and in any case, are likely superseded.
So, any information a natural language encodes is impoverished, weak, and outdated; and unhelpful, if not actively harmful, to attempt to base any kind of reasoning on.

In the case of religious thinkers who, starting from that incorrect premise, believe in the divine authorship of the Bible/Koran/scriptures or that Hebrew or Sanskrit encode deep correspondences and govern the nature of the cosmos in every detail, this belief is defensible: being spoken/written by an omniscient being, it is reasonable that the god might have encoded arbitrarily much information in it and careful study unearth it.
In its crudest form, it is sympathetic magic, hiding of 'true names', judging on word-sounds, and schizophrenia-like word-salad free-associative thinking (eg [Nuwaubian Nation](http://squid314.livejournal.com/283860.html "Inverse Law of Scientific Nomenclature")).
Secular thinkers have no such defense for being blinded by words, but the same mystical belief in the deep causal powers of spelling and word use and word choice is pervasive.

#### A Bitcoin+BitTorrent-driven economy for creators (Artcoin)

One criticism of the [Bitcoin](!Wikipedia) system by [cryptographers & commenters](Bitcoin is Worse is Better) is that the fundamental mechanism Bitcoin uses to prevent double-spends is requiring proof-of-work (finding certain very rare random numbers, essentially) for each set of transactions to make it hard for anyone to put together enough computers to be able to find multiple valid sets of transactions and spend the same coin twice. (People are motivated to actually do the proofs-of-work since when they discover a valid set of transactions, the protocol allows them to invent 50 coins for themselves.)

The computing power applied to the problem is nontrivial: it is literally equivalent to a supercomputer, distributed among the various participants. But this is a supercomputer which is devoted solely to calculating some numbers which satisfy a completely arbitrary criteria. Yes, it works - Bitcoin is still around and growing. But can't the situation be improved? Even distributed computing projects like [Folding@home](Charity is not about helping) do *some* good; even the distributed cryptographic projects did some good by proving points about the insecurity of various algorithms.

After all, checking random numbers has the necessary property of being hard to figure out and easy to check, but this sounds like the [P vs NP](!Wikipedia) problem - and that's so interesting because countless real-world economically valuable problems possess the same property. Why can't we take Bitcoin and replace it with a succession of real-world problems suitably encoded? We'll call it "Artcoin". This way we get a secure Bitcoin (because no one can afford to compute multiple solutions) and we also put the computing power to use. Everybody wins.

With centralized systems, we could do other things like implement micropayments for BitTorrent (eg. ["Floodgate: A Micropayment Incentivized P2P Content Delivery Network"](http://disi.unitn.it/security/icccn08.pdf "Nair et al 2008")). Nor are alternate blockchains are not an impossible idea. The [Namecoin](https://en.bitcoin.it/wiki/Namecoin) network is up and running with another blockchain, specialized for registering and paying for domain names. And there's already a quasi-implementation of Bitcoin micropayments in an amusing hack: [Bitcoin Plus](http://www.bitcoinplus.com/miner/embeddable). It is a piece of JavaScript that does the SHA-256 mining like the regular GPU miners. The idea is that one includes a link to it on one's website and then all one's website visitors' browsers will be bitcoin mining while they visit. In effect, they are 'paying' for their visit with their computational power. This is more efficient than [parasitic computing](!Wikipedia) (although visitors could simply disable JavaScript and so it is more avoidable than parasitic computing), but from the global view, it's still highly inefficient: JavaScript is not the best language in which to write tight loops and even if browser JavaScript were up to snuff, CPU mining in general is extremely inefficient compared to GPU mining. Bitcoin Plus works because the costs of electricity and computers is externalized to the visitors. Reportedly, CPU mining is no longer able to even pay for the cost of the electricity involved, so Bitcoin Plus would be an example of [negative externalities](!Wikipedia). A good Artcoin scheme should be Pareto-improving.

One issue that pops up is how do you input the specific real-world problem into the Artcoin network so everyone can start competing to solve it? Perhaps there could be some central authority with a public key that signs each specific problem; everyone downloads it, checks that the signature is indeed valid, and can start trying to solve it. But wait, Bitcoin's sole purpose was to be a *decentralized* electronic currency. (No one needs a new centralized electronic currency: you call it 'Paypal' or 'Stripe' or something.) If there was such a central authority in Artcoin, no one would use it!

And they would be right to not use it. One little noted property about NP problems is that the exponential blowup in difficulty refers to *worst-case* problems: one can construct easily solved instances. This means our Artcoin could be rendered completely worthless and vulnerable if the central signer decide to generate and sign an endless stream of trivial problems, at which point any fraudster could double-spend to his heart's content.

If we had some magical way of estimating the difficulty of an arbitrary NP problem, we could devise a hybrid scheme: 'if the just-released problem is at least 95% difficult, try to solve it; else, just try to solve the old random number problem.' Any central authority attempting to water down the proof-of-work security would just see his signed problems ignored in favor of the old inefficient scheme, and so would have no incentive to release non-real-world problems even if a third party (like a government) attempted to coerce them.

A more P2P scheme would be to have clients simply verify any solution for a set of transactions, and let anyone supply problems so users can pick which problems they work on. Maybe your mining pool has a SF bent so you donate your collective power to solving SETI@home problems, while my mining pool prefers to work on protein folding problems. But this would seem to run into the same problem as before: how do you know a third mining pool isn't "solving" trivial instances it made up for an otherwise perfectly acceptable NP problem?

If we had some way of estimating, we could implement this P2P scheme as well: users can subscribe to their favorite charity publishers of problems (or the publisher could pay the solver a sum to incentivize participation), and if any publisher attempted to weaken the system by publishing trivially solved problems, peers would simply reject the problem in favor of a different problem & solution or a hash.

How could we measure difficulty? Obviously you *could* measure difficulty by trying to solve the problem yourself: if it takes 1000 seconds, you know it's no harder than 1000 seconds. But what good would this do you? You could broadcast a message to all your peers saying "these problems are supposed to take at least 20000 seconds to compute, but this only took 1000 seconds!" but you have no *proof* of this; they could do the check themselves so as to reject trivial solutions & their linked sets of transactions, but if peers rechecked work just because some stranger on the Internet cried foul, they'd spend all their time rechecking work and the system would fail.

Does this magical way of estimating difficulty exist? I don't know. I've [asked](http://stackoverflow.com/questions/10508323/estimating-difficulty-of-instances-of-np-problems), and have been pointed at imperfect predictors: apparently [3-SAT](!Wikipedia) has a curious and well-known spike in difficulty of problems when the total variables divided by number of clauses reaches >=0.5, a "phase transition point". A randomly-generated problem can be inspected and predicted with substantial accuracy how difficult it will be: ["Predicting Satisfiability at the Phase Transition"](http://www.cs.ubc.ca/~kevinlb/papers/2012-PredictingSatisfiability.pdf "Xu et al 2012") claims to reach "classification accuracies of about 70%".

SAT-satisfiability prediction is a good step, but still incomplete: a 70% failure rate is far more than necessary to implement double-spend attacks at random just by hoping that several easy instances will be chosen, and it does not forbid offline pre-computing attacks.

<!--
foucist> sipa: gwern> idea is to convert the hashes BitTorrent does already into Bitcoin style hashes, which are only  assemble-able by the server; if the creator=server, then we just invented a way to turn downloads  into nanopayments with zero mental transaction costs

gwern> sipa: well, let's call this Artcoin. suppose there were a Artcoin-based economy but there were no upper limit but a built-in steady inflation. if downloads solve hashes along the way, we've made a system which gives originators of popular torrents some bitcoins, and the more popular they are, the more bitcoins structured right, this might be the most popular way of downloading - solving the issue of 'BitTorrent/p2p is killing creators' livelihoods!'
foucist> it might be entirely possible to do that with the current Bitcoin system,  modify BitTorrent to use a different combo hash that ties into bitcoins somehow
gwern> sipa: I thought about the Bitcoin increasing difficulty thing, but it seems to me that this would discourage creators. what happens when generating a new Bitcoin becomes a rare event, like once a year or something? the incentive for the creator disappears.
foucist> gwern: well the argument is that bitcoins isn't about mining
gwern> 'even if I release my awesome new movie, I probably won't get this year's Bitcoin. so why bother?'

gwern> sipa: paying creators using inflation isn't that bad an idea. inflation is invisible and doesn't require assent. assent is why microtransaction schemes have failed in their thousands
foucist> i think the base idea is to find a way to tie bitcoins into BitTorrent such that it's an automatic micropayment for creators of that content
sipa> but you're describing two ideas
gwern> (or at least, hundreds. micropayments seems like the sort of thing Internet folks have been trying for ages and failing miserably every time.)
sipa> 1. using bitcoins to pay for downloads
sipa> you mean content creators
sipa> 2. reusing the hashing done by BitTorrent for Bitcoin
gwern> the users mine as a side-effect of downloading for the benefit of the server/tracker, who may be the creator (given the creator's obvious first-mover advantage)
sipa> furthermore, i don't see how you are going to reuse the hashing power... BitTorrent uses it in a deterministic way to verify whether data is identical, while Bitcoin uses it as a pseudorandom function to look for a lot of zeroes
gwern> sipa: #1 is boring. it's the same damn failed idea that has been tried hundreds of times. #2 is more interesting. is there any variant which does both?
gwern> that's where I fail because I don't know the math (is there any way to force mining as part of the download? of course the users/downloaders could volunteer some hashing, but then someone will write a client which does no hashing because 'it was making my computer slow' and the scheme collapses)

gwern> foucist: I don't think that'd work, because suppose you have no bitcoins? you have an incentive to find a plaintext torrent

sipa> and that you pay with Bitcoin hashes to get the data
gwern> foucist: 'oh, but my backup server doesn't have access to my Bitcoin account'
gwern> sipa: hm, that sounds more sensible. what measures could be put in place to prevent colluding clients?
sipa> but that's little more than a using-bitcoins-to-pay-for-data system
foucist> gwern: OK so you think only a purely generative way would work ?  hmm
sipa> and it's very wasteful
sipa> since the profitability of CPU mining is already long gone
gwern> sipa: in the current Bitcoin system, yeah, not some new Bitcoin system
sipa> explain me why people would value your Artcoin's?
gwern> foucist: well, you need to keep the Bitcoin+BitTorrent not much more expensive in time than plain BitTorrent. if the new system took 3 times as long to download, then people have an incentive to use a parallel system. but if it were only, say, a 20% penalty, then lots of people would use it. look at iTunes
foucist> gwern: Bitcoin mining of any kind is still kinda wasteful though, those CPU power could be used for solving real problems etc
sipa> indeed, do that as payment in your paying-for-creators BitTorrent system
gwern> sipa: suppose creators began using it. it would have pretty much the same advantages as regular Bitcoin, it'd give you access to the new stuff, you'd not feel guilty about piracy, that sort of thing. (again, why does anyone download from iTunes?)
sipa> pay with folding@home work units or some
sipa> gwern: but it wouldn't have limited total supply

gwern> sipa: would people suddenly stop being interested in Bitcoin if the guarantee weren't 0% inflation but, say, 0.1% inflation?
sipa> some, certainly
gwern> (US GDP is 14.6 trillion, so 0.1% inflation would be a lot.)

gwern> foucist: wouldn't surprise me if the research has already been done, actually; this is starting to remind me of the 'proof of work' subfield of crypto
Necr0s> I need to understand a bit more about crypto.
foucist> a quick google on 'micropayment hash' reveals a variety of research papers on micropayments with hash-chains
Necr0s> Particularly asymmetric public/private key systems.
foucist> "Micro-payment Protocol Based on Multiple Hash Chains" 'A Study of Micro-payment Based on One-Way Hash Chain'
Necr0s> And their use in signing content.
-->
<!-- http://www.reddit.com/r/Bitcoin/comments/1eorel/compushare_a_way_to_stabalise_the_bitcoin_economy/ -->
<!-- PrimeCoin, a chain using primes as a proof-of-work!
https://bitcointalk.org/index.php?topic=245953.0;all
https://bitcointalk.org/index.php?topic=251850.0
http://ppcoin.org/static/primecoin-paper.pdf
-->
<!-- MCMC? http://www.reddit.com/r/Bitcoin/comments/pve08/btclike_cryptocurrency_with_arbitrary_tradeable/ -->
<!-- https://en.bitcoin.it/wiki/Intrinsic_worth_brainstorming -->
<!-- Lagrangian (probably fails the offline or verifiability properties): https://bitcointalk.org/index.php?topic=108888.0 -->
<!-- http://www.truthcoin.info/blog/pow-and-mining/ -->

#### Good governance & Girl Scouts

See [Girl Scouts and good governance]().

#### Hard problems in utilitarianism

The Nazis believed many sane things, like exercise and the value of nature and [animal welfare](!Wikipedia "Animal welfare in Nazi Germany") and the harmful nature of smoking.

Possible rationalist [exercise](http://lesswrong.com/lw/us/the_ritual/):

1. Read [_The Nazi War on Cancer_](http://www.amazon.com/The-Nazi-Cancer-Robert-Proctor/dp/0691070512/)
2. Assemble modern demographic & mortality data on cancer & obesity.
3. Consider this hypothetical: 'If the Nazis had not attacked Russia and negotiated a peace with Britain, and remained in control of their territories, would the lives saved by the [health benefits](!Wikipedia "Anti-tobacco movement in Nazi Germany") of their policies outweigh the genocides they were committing?'
4. Did you answer yes, or no? Why?
5. As you pondered these questions, was there ever *genuine* doubt in your mind? Why was there or not?

#### Who lives longer, men or women?

Do men or women live longer? Everyone knows [women live a few years longer](!Wikipedia "Life expectancy#Sex differences"); if we look at America and Japan (from the 2011 [CIA World Factbook](!Wikipedia)):

1. 75.92 vs 80.93
2. 78.96 vs 85.72

5-7 years additional bulk longevity is definitely in favor of women. But maybe what we are really interested in is whether women have longer *effective* lives: the amount of time which they have available to pursue those goals, whatever they may be, from raising children to pursuing a career. To take the Japanese numbers, women may live 8.6% longer, but if those same women had to spend 2 hours a day (or 1/12th a life, or 8.3%) doing something utterly useless or distasteful, then maybe one would rather trade off that last 0.3%.

But notice how much we had to assume to bring the female numbers down to male: 2 hours a day! That's a lot. I had not realized how much of a lifetime those extra years represented: it was a larger percentage than I had assumed.

The obvious criticism is that social expectations that women appear as attractive as possible will use up a lot of women time. It's hard to estimate this, but men have to maintain their appearance as well; a random guess would be that men spend half an hour and women spend an hour on average, but that only accounts for a fourth of the extra women time. Let's say that this extra half hour covers make-up, menstruation, waiting in female bathroom lines, and so on. (This random guess may understate the impact; the pill aside, menstruation reportedly can be pretty awful.)

Sleep patterns don't entirely account for the extra time either; one guide says ["duration of sleep appears to be slightly longer in females"](https://web.archive.org/web/20130421050631/http://sleep.health.am/sleep/sleep-and-gender/), and [Zeo, Inc.](!Wikipedia)'s [sleep dataset](http://web.archive.org/web/20120823045359/http://blog.myzeo.com/whats-your-zq/) indicates a difference of women sleeping 19 minutes more on average. If we round to 20 minutes and add to the half hour for cosmetics, we're still not even half the way.

And then there's considerations like men becoming disabled at a higher rate than women (from the dangerous jobs or manual labor, if for no other reason). Unfortunately, the data doesn't seem to support this; while women have longer lifespans, they also seem to have more illnesses than men[^Angus].

[^Angus]: [Angus Deaton](!Wikipedia), ["What does the empirical evidence tell us about the injustice of health inequalities?"](http://www.princeton.edu/~deaton/downloads/What_does_the_empirical_evidence_tell_us_about_the_injustice.pdf) (January 2011):

    > Men die at higher rates than women at all ages after conception. Although women around the world report higher morbidity than men, their mortality rates are usually around half of those of men. The evidence, at least from the US, suggests that women experience similar suffering from similar conditions, but have higher prevalence of conditions with higher morbidity, and lower prevalence of conditions with higher mortality so that, put crudely, women get sick and men get dead, [Case & Paxson 2005](http://www.princeton.edu/rpds/papers/pdfs/case_paxson_morbidity.pdf "Sex Differences in Morbidity and Mortality").

Pregnancy and raising children is a possible way to even things out. The US census [reports a 2000 figure](http://usgovinfo.about.com/library/weekly/aa031601a.htm) that 19% of women 40-44 did not have children. So the overwhelming majority of women will at some point bear the burden of at least 1 pregnancy. So that's 9 months there, and then...?

That's not even 1 year, so a quarter of the time is left over if we assume the pregnancy is a total time-sink but the women involved do not spend any further time on it (but also that the average male expenditure is zero time, which was never true and is increasingly less true as time passes). That leaves a decent advantage for women of ~2 years.

If you wanted to erase the female longevity advantage, you could argue that between many women having multiple children, and many raising kids full-time at the expense of their careers or non-family goals, that represents a good decade of lost productivity, and averaging it out (81% of 10 years) reduces their effective lives by 8.1 years, and then taking into account the sleep and toiletry issues reduces the number by another 2 years, and now women lifetimes are shorter than men lifetime.

So at least as far as this goes, your treatment of childbearing will determine whether the longevity advantage is simply a fair repayment, as it were, for childbearing and rearing, or whether it really is a gift to the distaff side.

#### Chinese Kremlinology

> "I'm not suggesting that any of the news pieces above are false, I'm more worrying about my own ability to be a good consumer of news. When I read about Wisconsin, for example, I have enough context to know why certain groups would portray a story in a certain way, and what parts of the story they won't be saying. When I'm interested in national (US) news, I know where to go to get multiple main-stream angles, and I know where to go to get fringe analysis. Perhaps these tools don't amount to much, but I have them and I rely on them. But I really know very little about how news from China gets to me, and it is filtered through a lot more layers than when I read about things of local interest." --[Antoine Latter](https://plus.google.com/u/0/107517520217732606032/posts/jpMQ9jC5z9z)

It *is* dangerous to judge a very large and complex country with truly formidable barriers to understanding and internal opacity. As best as I can judge the numbers and facts presented for myself, there are things rotten in Denmark. (The question is whether they are rotten enough.)

But at the same time, we can't afford to buy into the China-as-the-next-threat hype. When I was much younger, I read every book my library had on Japan's economics and politics, and many online essays & articles & op-eds besides. They were profoundly educational, but not just in the way that their authors had intended - because they were all from the [_Japan as Number One_](http://www.amazon.com/Japan-Number-One-Lessons-America/dp/1583484108/) ([Ezra Vogel](!Wikipedia)) / [_Rising Sun_](!Wikipedia "Rising Sun (novel)") ([Michael Crichton](!Wikipedia)) period of the [bubble '80s](!Wikipedia "Japanese asset price bubble"), and they were profoundly confident about how Japan would rule the world and quite convincing but *even as I read them*, Japan's bubble had popped brutally and it [continued to stagnate](!Wikipedia "Lost Decade (Japan)"). This dissonance, and my own susceptibility to the authors I had read, was not lost on me. (There was another sobering example from that same period for me - I had read [Frank Herbert](!Wikipedia)'s [_Dune_](!Wikipedia "Dune (novel)") with avidity, thoroughly approving of [Paul's](!Wikipedia "Paul Atreides") actions; then I read _[Dune Messiah](!Wikipedia)_ and some of Herbert's essays and interviews, only to realize that I had been cheering on a mass murderer and had fallen right into Herbert's trap - "I am showing you the superhero syndrome and your own participation in it.")

Years later, I came across [Paul Krugman](!Wikipedia)'s ["The Myth of Asia's Miracle"](http://web.archive.org/web/20090302203414/http://web.mit.edu/krugman/www/myth.html), which told me about a *economic* (as opposed to military or geopolitical) parallel to Japan's ascension that I'd never heard of - Soviet Russia! (And it's worth noting that one of the other '[Asian Tigers](!Wikipedia "Four Asian Tigers")', [South Korea](!Wikipedia), despite its extraordinary growth and own mini-narratives, is still 3k or so below Japan's per-capita income.)

Ever since I have been curious as to China's fate (much greater than or comparable total wealth to the US?), skeptical of the optimistic forecasts, and mindful of my own fallibility. Falling into the narrative once, with Japan, is understandable; fool me twice with Soviet Russia, that's forgivable; fool me three times with China, and I prove myself a fool.

#### William Carlos Williams

Have you ever tried to change the oil in your car? Or stared perplexed at a computer error for hours, only for a geek to resolve it in a few keystrokes? Or tried to do yardwork with the wrong tool? (Bare hands rather than a shovel; a shovel rather than a rake, etc.)

So much depends on the right tool or the right approach. Think of a man lost in a desert. The right direction is such a trivial small bit of knowledge, almost too small a thing to even be called 'data'. But it means the entire world to that man - the entire world.

So much depends on little pieces of metal being 0.451mm wide and not 0.450mm, and on countless other dimensions. (Think of the insides of a jet engine, of thousands of planes and even more tens of thousands of people not falling screaming out of the sky.)

Williams is sharing with us, in true Imagist style, a sudden realization, an epiphany in a previously mundane image.

Here is a farm. It seems robust and eternal and sturdy. Nothing about this neglected wheelbarrow, glazed with rain and noticed only by fowl, draws our attention - until we suddenly realize how fragile everything is, how much everything has to go right 99.999% of the time, how without a wheelbarrow, we cannot do critical tasks and the whole complex farm ecosystem loses homeostasis and falls apart.

(I sometimes have this feeling on the highway. Oh my god! I could die in so many ways right now, with just a tiny movement of my steering wheel or anyone else's steering wheel! How can I possibly still be alive after all these trips?)

#### Fermi calculations

I really like [Fermi problems](!Wikipedia) ([LessWrong](http://lesswrong.com/lw/h5e/fermi_estimates/)) - it's like [dimensional analysis](!Wikipedia) for everything outside of physics^[This is a little misleading; dimensional analysis is much more like [type-checking](!Wikipedia) a program in a language with a good type system like Haskell. Given certain data types as *inputs* and certain allowed transformations on those data types, what data types *must* be the resulting output? But the analogy is still useful.].

Not only are they fun to think about, they can be amazingly accurate, and are extremely cheap to do - because they are so easy, you do them in all sorts of situations you wouldn't do a 'real' estimate for, and are a fun part of a [physics education](http://arxiv.org/abs/1109.1165). The common distaste for them baffles me; even if you never work through Hubbard's [_How to Measure Anything_](http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/1118539273/) ([some strategies](http://80000hours.org/blog/170-estimation-part-i-how-to-do-it "Estimation - Part I: How to do it?")) or [_Street-Fighting Mathematics_](http://www.amazon.com/Street-Fighting-Mathematics-Educated-Guessing-Opportunistic/dp/026251429X) or read [Douglas Hofstadter](!Wikipedia)'s 1982 essay ["On Number Numbness"](/docs/1982-hofstadter.pdf) (collected in _[Metamagical Themas](https://archive.org/details/MetamagicalThemas)_), it's something you can teach yourself by asking, what information is public available, what can I compare this too, how can I put various boundaries around the true answers^[eg. if someone asks you how many piano tuners there are in Chicago, don't look blank, start thinking! 'Well, there must be fewer than 7 billion, because the human race isn't made of piano tuners, and likewise fewer than 300 million (the population of the United States), and heck, Wikipedia says Chicago has only 2.6 million people and piano tuners are rare, so there must be many fewer than *that*...'] You especially want to do Fermi calculations in areas where the data is unavailable; I wind up pondering such areas frequently:

- [is a lip-reading website a good idea?](#lip-reading-website)
- how many women [dye their hair blonde](#somatic-genetic-engineering)
- how many [people does Folding@home kill](Charity is not about helping)
- what's [the entropy of natural language](#efficient-natural-language)
- or [how big a computer](Simulation inferences) is needed to compute the universe
- do [men have shorter *real* lives than women](#who-lives-longer-men-or-women)
- how much do Girl Scouts cookies [cost and earn them](Girl Scouts and good governance#cookie-prices-and-inflation)
- how many people have [used modafinil](Modafinil#side-effects), and what is the cheapest we can expect to find [modafinil for](Modafinil#margins)
- quickly assessing the rough probability of some event as I made one of my [thousands of predictions](Prediction markets)
- how many sellers are there on the [Silk Road](Silk Road)
- checking whether posthumous organ donation [justifies the Chinese justice system](https://news.ycombinator.com/item?id=3389084) (no)
- [how many tourists](http://lesswrong.com/lw/bi7/smbc_comic_poorly_programmed/69fw#body_t1_69gl) to the Egyptian pyramids there have been compared to the workers who build them
- [what we can infer from adultery & false paternity rates](#politicians-are-not-unethical)
- [how many times have video gamers killed Mario](http://www.reddit.com/r/estimation/comments/105okb/request_how_many_times_has_mario_died/)
- is tiger skin [more profitable](http://www.reddit.com/r/WTF/comments/188zui/this_is_beyond_saddening_tigers_starved_to_make/c8cql4x) than tiger bone wine?
- [plating Versailles with gold](http://slatestarcodex.com/2013/03/03/reactionary-philosophy-in-an-enormous-planet-sized-nutshell/#comment-544)
- the [number of people killed by falling pianos](http://lesswrong.com/lw/h5e/fermi_estimates/8pts)
- [the plausibility of](http://www.reddit.com/r/anime/comments/1cqtae/in_honor_of_kuronekos_birthday_my_favorite_scene/c9ja9a5) _[Oreimo](!Wikipedia)'s Kirino Kosaka character
- [how many integers one to a million do you see over a lifetime?](http://www.reddit.com/r/estimation/comments/1f765d/request_in_an_average_lifetime_how_many_of_the/ca85zcl) (R simulation + [Benford's law](!Wikipedia))
- [how many anecdotes of CrossFit causing rhabdomyolysis does it take to justify "CrossFit causes rhabdomyolysis"?](http://lesswrong.com/r/discussion/lw/irm/rationality_competitiveness_and_akrasia/9uoq)
- [how long would it take to read every book in existence?](http://www.reddit.com/r/estimation/comments/27i4wq/request_how_long_would_it_take_to_read_every_book/ci1jfno)
- [what would it cost to replace the US nuclear arsenal?](http://www.reddit.com/r/estimation/comments/2chtft/if_all_the_uss_atomic_bombs_disappeared_assuming/cjfyses)
- [What are the lifetime odds of being pooped on by a bird?](https://www.reddit.com/r/estimation/comments/4f9bxl/what_are_the_lifetime_odds_of_being_pooped_on_by/)

An entire ["estimation" subreddit](http://www.reddit.com/r/estimation/) is devoted to working through questions like these (it can be quite fun), and of course, there are the memorable ["what if?"](http://what-if.xkcd.com/) _xkcd_ columns.

[Timothy Gowers](!Wikipedia) suggests [a number of problems](https://gowers.wordpress.com/2012/06/08/how-should-mathematics-be-taught-to-non-mathematicians/ "How should mathematics be taught to non-mathematicians?") which might help children really learn how to think with & apply the math they learn.

To look further afield, here's a quick and nifty application by investor John Hempton to the [Sino Forestry fraud](!Wikipedia "Sino-Forest Corporation#Fraud allegations and share suspension"): ["Risk management and sounding crazy"](http://brontecapital.blogspot.com/2011/09/risk-management-and-sounding-crazy.html). What I personally found most interesting about this post was not the overall theme that the whistleblowers were discounted before and after they were proven right (we see this in many bubbles, for example, the housing bubble), but how one could use a sort of [Outside View](http://wiki.lesswrong.com/wiki/Outside_view)/Fermi calculation to sanity-check the claims. If Sino Forestry was really causing 17m cubic meters of wood to be processed a year, where was all the processing? This simple question tells us a *lot*. With medicine, there is one simple question one can always ask too - where is the increased longevity? (This is an important question to ask of studies, such as a [recent caloric restriction study](http://aging.wisc.edu/pdfs/2297.pdf).)

Simple questions and reasoning can tell us a lot.

#### Politicians are not unethical

[Dominique Strauss-Khan](!Wikipedia), while freed of the charge of rape, stands convicted in the court of public opinion as an immoral philanderer; after all, even by his account he cheated on his wife with the hotel maid, and he has been accused in France by [a writer](!Wikipedia "Tristane Banon#Banon's allegations against Dominique Strauss-Kahn") of raping her; where there is smoke there is fire, so Khan has probably slept with quite a few women^[Tristane Banon came forward only after the maid, and Khan's calm behavior after the maid incident suggests he considered it routine. Both facts suggest that the 'probability of public revelation', if you will, is fairly low, and so we ought to expect numerous previous unreported such liaisons. (An analogy: a manager catches an employee stealing from the till. The employee claims this was the first time ever and he'll be honest thenceforth. Should the manager believe him?)]. This is as people expect - politicians sleep around and are immoral. Power corrupts. To be a top politician, one must be an risk-taking alpha male reeking of testosterone, to fuel [status-seeking behavior](/docs/nootropics/2011-eisenegger.pdf "The role of testosterone in social interaction").[^faithful-wives] And then it's an easy step to say that the testosterone causes this classically *hubristic* behavior of ultimately self-destructive streaks of abuse:

> "Toward the end of my two-week [testosterone injection] cycle, I can almost feel my spirits dragging. In the event of a just-lost battle, as Matt Ridley points out in his book [_The Red Queen_](http://www.amazon.com/The-Red-Queen-Evolution-Nature/dp/0060556579/), there's a good reason for this to occur. If you lose a contest with prey or a rival, it makes sense not to pick another fight immediately. So your body wisely prompts you to withdraw, filling your brain with depression and self-doubt. But if you have made a successful kill or defeated a treacherous enemy, your hormones goad you into further conquest. And people wonder why professional football players get into postgame sexual escapades and violence. Or why successful businessmen and politicians often push their sexual luck." --[Andrew Sullivan](!Wikipedia), ["The He Hormone"](http://www.nytimes.com/2000/04/02/magazine/the-he-hormone.html?pagewanted=all)

[^faithful-wives]: If this is such common knowledge, one wonders what the *wives* think; during sex scandals, they seem to remain faithful, when other women divorce over far less than such public humiliation. Why would Khan's wife - the wealthy and extremely successful [Anne Sinclair](!Wikipedia) - remain linked with him? I've seen it suggested that such marriages are 'open' relationships, where neither party expects fidelity of the other, and like many aristocratic marriages of convenience, the heart of the agreement is to not be *caught* cheating. In Khan's case, perhaps Sinclair judges him not fatally politically wounded, with still a chance at the French presidency. It is an interesting question how conscious such considerations are; [Keith Henson](!Wikipedia) has [an evolutionary theory](https://www.kuro5hin.org/story/2006/4/17/194059/296) somewhat relevant - that women (in particular) can transfer their affections to powerful males such as captors to safeguard their future reproductive prospects.

    On the other hand, in June 2012, newspapers were reporting that Sinclair had separated from him, which is consistent with the interpretation that she felt it was her duty to not stab her husband in the back immediately but wait for the scandal to die down. On the gripping hand, June 2012 is well after a crushing French Socialist defeat of Sarkozy on all political fronts; President Holland's victory & elevation could be seen as scuttling Strauss-Khan's future prospects for that exact position, and Sinclair's separation merely a cold-blooded cutting her losses on Strauss-Khan. The latter seems less likely than the former, since I seem to recall a number of politicians' wives waiting a discreet period before separating or divorcing.

    And of course, we can't rule out less cynical explanations; for example, perhaps the wives are commendably optimistic about finding forgiveness for their wayward spouses & the chances of patching up their marriages, and it simply takes them that year or two to give up.

Power corrupts, unconsciously, leading to abuse of power and an inevitable fall - the [paradox of power](http://www.wired.com/wiredscience/2011/05/how-power-corrupts/). Such conventional wisdom almost dares examination. Politicians being immoral and sleeping around is a truism - people in general are immoral and sleep around. What's really being said is that politicians do *more* immorality and sleeping-around than another group, presumably upper-class^[National-level legislators usually being well-educated and well-off, when they are not mega-millionaires like John Kerry or millionaires like Barack Obama.] but still non-politician white men^[Minorities and women being rare even now.].

##### Revealed moralities

But is this true? I don't think I've ever seen anyone actually ask this question, much less offer any evidence. It's a simple question: do white male politicians (and national politicians in particular) sleep around more than upper-class white males in general? It's [easy](!Wikipedia "Representativeness heuristic") to come up with examples of politicians who stray paying prostitutes, having a 'wide stance' sending photographs online, possibly to young pages, or impregnating mistresses, but those are anecdotes, not statistics. Consider how *many* 'national-level' politicians there are that could earn coverage with their infidelities: Congress alone, between the [House](!Wikipedia "United States House of Representatives") and the [Senate](!Wikipedia "United States Senate"), has 535 members; then one could add in 9 Justices, the President & Vice-president and [Cabinet](!Wikipedia "Cabinet of the United States") make 17, and then there are the governors of each of the 50 states, for a total of 611 people.

###### A priori rates

If those 611 were merely ordinary, what would we expect? Lifetime estimates of adultery seem to center around 20%[^adultery][^paternity] although Kinsey put it at 50% [for men](!Wikipedia "Adultery#United States of America"). So we might expect *122-305* of the current set of national politicians to be unfaithful eventually! That's 4-10 sex scandals a year on average (assuming a 30-year career), each of which might be covered for weeks on national TV. I do not know about you, but either end of that range seems high, if anything; it's not every other month that a politician goes down in flames. (Who went down as scheduled in September or August 2011? No one?) Why does it feel the opposite way, though? We might chalk it up to the [base rate fallacy](!Wikipedia) - saying 'that's a lot' while forgetting what we are comparing to.

And 611 is very low an estimate. After all, everyone lives *somewhere*. The 8 million inhabitants of New York City will read about and be disgusted by the assistant New York Governor, the Mayor of New York City and his flunkies, the New York State legislature (212 members); and then there are the nearby counties like Nassau or Suffolk which are covered by newspapers in circulation in NYC like _Newsday_. We could plausibly double or triple this figure. (I had not heard of many politicians involved in sex scandals - like Khan, come to think of it - so they do not even need to be famous.)

So we have noticed that there are 'too few' sex scandals in politics; the same reasoning seems to work for ordinary crimes like murder - there are too few! In fact, besides Congressmen rarely committing suicide[^political-suicide] (despite the considerable stresses), it seems that politicians in general are uncannily honest; the only category I can think of where politicians are normally unethical would be finance (bribes, conflicts of interest, insider trading by [Representatives](https://web.archive.org/web/20130128115545/http://insidertrading.procon.org/sourcefiles/abnormal-returns-of-us-house-of-representatives-2011.pdf "'Abnormal Returns From the Common Stock Investments of Members of the U.S. House of Representatives', Ziobrowski et al 2011") & [Senators](http://www.thenationalbusinessassociation.com/content/JFQA-394-Ziobrowski-Proofs.pdf), etc). Why is this?

[^political-suicide]: From ["Leaving Office Feet First: Death In Congress"](http://home.gwu.edu/~forrest/fmdeathincongressps.pdf "Maltzman et al 1996"):

    > According to the most reliable estimate available, eight members of Congress have committed suicide (Eisele 995). Amer (1989) reported only seven, but the 1925 suicide of Senator Joseph McCormick (R-IL), who overdosed on barbiturates, was subsequently made public (Miller 1992). Senator Lester Hunt (D-WY) is the only member to have killed himself in the Russell Office Building. He did so after supporters of Senator Joseph McCarthy (R-WI) threatened to publicize the arrest of Hunt's son for committing homosexual acts in a Washington park unless Hunt withdrew from his 1954 re-election campaign - an incident that provided the inspiration for Allen Drury's (1959) novel [_Advise and Consent_](http://www.amazon.com/Advise-Consent-Allen-Drury/dp/038505419X/).
    >
    > - Eisle, Albert 1995. "Members of Congress No Strangers to Violent Deaths", _The Hill_ (September 6)
    > - Amer, Mildred 1989. "Members of the U.S. Congress Who have Died of Other Than a Natural Death While Still in Office: A Selected List". Washington, DC: Congressional Research Service
    > - Miller, Kristie 1992. [_Ruth Hanna McCormick: A Life in Politics 1880-1944_](http://www.amazon.com/Ruth-Hanna-McCormick-Politics-1880-1944/dp/0826313337/). Albuquerque: University of New Mexico Press

    If there are 535 members of Congress and each has a career of 22 years, and we look at the 88 years of 1925-2013, then that is >=8 suicides spread over 4 blocks or a $\frac{8}{535 \times 4} \times 100 =$ 0.37% suicide rate; but in general, [suicide in the USA](!Wikipedia "Suicide in the United States") is a leading cause of death at around 100,000 deaths a year. Estimating total lifetime risk is harder, but some searching turned up [Nock et al 2008](http://epirev.oxfordjournals.org/content/30/1/133.full "Suicide and Suicidal Behavior") with estimates for US adults of suicide attempts somewhere around 1.9-8.7%, which if another estimate is right that out of every 10 attempts 1 succeeds may imply that the Congressional risk is equivalent (1.9%/10 < 0.37) or lower (8.7/10 > 0.37). For comparison, alcoholics have [2-3% lifetime risk](http://archpsyc.ama-assn.org/cgi/reprint/47/4/383.pdf "'The Lifetime Risk of Suicide in Alcoholism', ")

##### Why?

Self-discipline seems like an obvious key. A reputation is built over decades, and can be destroyed in one instant. But that seems a little too friendly - we're praising our politicians for morality and we're also going to claim it's because they are more disciplined (with all the positive moral connotations)?

Maybe the truth is more sinister: they whore around as much as the rest of us, they're just covering it up better.

And we need a cover-up which actually reduces the number of scandals going public to make this all go away and leave our prejudices alone.

##### Investigating

If all the media was doing was *delaying* reporting on said scandals, we'd still see the same total number of scandals - just shifted around in time. To some extent, we see delays. For example, we seem to now know a lot about [John F. Kennedy's womanizing](http://www.telegraph.co.uk/culture/books/3601644/Adultery-was-his-thing.html), but his contemporaries [ignored](http://articles.cnn.com/1998-05-14/politics/kennedy_1_fbi-memo-memo-quotes-fbi-documents?_s=PM:ALLPOLITICS) even a determined attempt to spread the word; similar stories seem true of other Presidents & presidential candidates ([FDR & Wendell Wilkie](http://www.washingtonmonthly.com/features/2006/0607.benen.html) & [John Edwards](!Wikipedia)^[Edwards is a good example because the news about his mistress [Rielle Hunter](!Wikipedia) was broken by the _[National Inquirer](!Wikipedia)_ - in 2008, 1 year before he admitted the relationship and 3 years before he admitted being the illegitimate child's father as well.]). This suggests a way to distinguish the permanent cover-up from the delayed cover-up theory: hit the history books and see how many politicians in a political cohort turn out to have mistresses and credible rumors of affairs. Take every major politician from, say 1930, and check into their affairs; how many were then known to have affairs? How many were revealed to have affairs decades later? This will give us the delay figure and let us calculate the 'shadow scandals', how many sex scandals there *ought* to be right now but aren't.

(One could probably even automate this. Take a list of politicians from Wikipedia and feed them into Google Books, looking for proximity to keywords like 'sex'/'adultery'/'mistress', etc.)

###### Uses

The shadow rate is interesting since the mass media audience finds sex scandals interesting to a nauseating degree. (Why does the media spend so much time on something like Weiner? Because it sells.) The shadow rate ought to be *negative* if anything: there is so much incentive to report on sex scandals one might expect the media to occasionally make up a scandal, on the same principle as [William Randolph Hearst](!Wikipedia) and the [Spanish-American War](!Wikipedia "Propaganda of the Spanish-American War") - it sells well. Any positive shadow rate shows something very interesting: that the media values the politicians' interests more than its own, to the point where they are *collectively* (it only takes one story to start the frenzy) willing to conceal something their customers avidly demand.

In other words, the shadow rate is a proxy for how corrupt the media is.

[^adultery]: ["Married, With Infidelities"](http://www.nytimes.com/2011/07/03/magazine/infidelity-will-keep-us-together.html):

    > In 2001, The Journal of Family Psychology summarized earlier research, finding that "infidelity occurs in a reliable minority of American marriages." Estimates that "20-25% of all Americans will have sex with someone other than their spouse while they are married" are conservative, the authors wrote. In 2010, NORC, a research center at the University of Chicago, found that, among those who had ever been married, 14% of women and 20% of men admitted to affairs.

    Baumeister 2010, [_Is There Anything Good About Men?_](http://www.amazon.com/There-Anything-Good-About-Men/dp/019537410X/) pg 242 puts it much higher:

    > According to the best available data, in an average year, well over 90% of husbands remain completely faithful to their wives. In that sense, adultery is rare. Then again, if you aggregate across all the years, something approaching half of all husbands will eventually have sex with someone other than their wives...There are many sources on adultery and extramarital sex. The best available data are in Laumann, E. O., Gagnon, J. H., Michael, R. T., & Michaels, S. (1994). [_The social organization of sexuality: Sexual practices in the United States_](http://www.amazon.com/Social-Organization-Sexuality-Sexual-Practices/dp/0226470202/). Chicago, IL: University of Chicago Press. For an older, but thoughtful and readable introduction, see Lawson, A. (1988). [_Adultery: An analysis of love and betrayal_](http://www.amazon.com/Adultery-Analysis-Betrayal-Annette-Lawson/dp/0465000754/). New York: Basic Books.

    Taormino 2008, [_Opening Up_](http://www.amazon.com/Opening-Up-Creating-Sustaining-Relationships/dp/157344295X/):

    > There's another [key] indicator that monogamous marriages and relationships aren't working: cheating is epidemic. The Kinsey Report was the first to offer statistics on the subject from a large study published in 1953; it reported that 26% of wives and 50% of husbands had at least one affair by the time they were 40 years old. Other studies followed, with similar findings. According to the Janus Report of 1993, more than one-third of men and more than one-quarter of women admit to having had at least one extramarital sexual experience. 40% of divorced women and 45% of divorced men reported having had more than one extramarital sexual relationship while they were still married.' In a 2007 poll conducted by MSNBC and iVillage, half of more than 70,000 respondents said they've been unfaithful at some point in their lives, and 22% have cheated on their current partner.

[^paternity]: One interesting perspective is the rate of [false paternity](!Wikipedia "Paternity fraud#Occurrence"): a rough consensus of 4% of all children. ([Cochran](http://westhunt.wordpress.com/2013/01/27/by-blows-paternal-age-and-all-that/) prefers estimates around the 1% range.) Since the per-sex-act pregnancy risk is estimated at 1-5% by various health sites we can ask the question of the [Poisson distribution](!Wikipedia): what is the median number of sex-acts (outside the holy bounds of matrimony) for each of these children? The median is ~28 (this is a lower bound since not all pregnancies come to term). So if ~20% of the USA is children under 14, the US population is ~310m, ~4% of the children are misattributed, and each such child implies 28 illicit sex-acts, then we can give a loose lower bound of annual adultery per year to be $\frac{0.2 \times 310,000,000 \times 0.04 \times 28}{14} = 4,960,000$

#### Defining 'but'

The word 'but' is pretty interesting. It seems to be short hand for a pretty complex logical argument, which isn't *just* [modus tollens](!Wikipedia) but something else, in much the same way that natural language's [if-then](!Wikipedia "Material conditional#Philosophical problems with material conditional") is not just the material conditional.

(Modus tollens, in a quick refresher, is 'A ~> B', 'not B', therefore, 'not A'. Its counterpart is [modus ponens](!Wikipedia), 'A ~> B', 'A', therefore, 'B'.)

Most arguments proceed by repeated modus ponens; 'this' implies 'that' which implies 'the other', and 'this' is in fact the case, so you must agree with me about 'the other'. It's fairly rare to try to dispute an argument immediately by denying 'this' but conceding the rest of the argument; instead, one replies with a 'but'. But what?

I thought, and I think we could formalize 'but' as a probabilistic modus tollens. Usually we know we're dealing in slippery probabilities and inductions; if I make an argument about GDP and tax rates, I only get a reliable conclusion if I am not talking about the cooked books of Greece. My conclusion is always less reliable than my premises because probability intervenes at every step: the probability of both A and B must be less than or equal to the probability of either A or B. So, when we argue by repeated modus ponens, what we are actually saying (although we pretend to be using good old syllogisms and deductive logic) is something more like: 'A implies B; probably A; therefore (less) probably B'.

When someone replies with 'But C!', what they are saying is: 'C implies ~B; both A implies B and C implies ~B cannot be true as it is a contradiction, and C is more likely than A, so we should really conclude that C and ~A, and therefore, ~B'.

They are setting up an unstated parallel chain of arguments. Imagine a physicist discussing FTL neutrinos; 'this observation therefore that belief therefore this conclusion that the neutrinos arrived faster than light'. And someone speaks up, saying 'But there was no burst of neutrinos *before* we saw light from that recent supernova!' What is going on here is the audience is weighing the probabilities of two premises, which then work backwards to the causal chains. One might conclude that it is more likely that the supernova observations were correct than the FTL observations were correct, and thus reason with modus tollens about the FTL - 'FTL-Correct ~> (seeing neutrino burst)^[To be clear, '~Seeing-neutrino-burst' means something like 'the equipment or staff screwed up and the lack of observation is some mistake or bad luck'. In this case, both theories think that the neutrino burst *does* exist.]; ~(seeing neutrino burst); therefore, ~FTL-Correct'. But if it goes the other way, then one would reason, 'Seeing-neutrino-burst ~> ~FTL; FTL; therefore, ~Seeing-neutrino-burst'.

You don't really find such probabilistic inference in English except in 'but'. Try to explain it without 'but'. Here's an example:

1. 'Steve ran by with a bloody sword, but he likes to role-play games so I don't think he's a serial killer' versus
2. 'Steve ran by clutching a sword which is consistent with the theory that he is a serial killer and also consistent with the theory that he is role-playing a game; I have a low prior for him ever being a serial killer and a high prior for him carrying a sword, bloody or otherwise, for reasons like role-playing and when I multiply them out, the role-playing explanation has a higher probability than the serial killer explanation'

I exaggerate a little here; nevertheless, I think this shows 'but' is a little more complex and sophisticated than one would initially suspect.

#### Cryonics cluster

When one looks at cryonics enthusiasts, there's an interesting cluster of beliefs. There's psychological materialism, as one would expect (it's possible to believe your personal identity is your soul and also that cryonics works, but it's a rather unstable and unusual possibility), since the mind cannot be materially preserved if it is not material. Then there's libertarianism with its appeal to free markets and invisible entities like deadweight loss. And then there is ethical utilitarianism, usually act utilitarianism[^ethics]. They're often accused of being nerdy and specifically autistic or Asperger's; with considerable truth. Most have programming experience, or have read a good deal about logic and math and computers. [Romain 2010](http://www.tandfonline.com/doi/pdf/10.1080/01459741003715391 "Extreme Life Extension: Investing in Cryonics for the Long, Long Term") gives the stereotypical image:

> Cryonics is a particularly American social practice, created and taken up by a particular type of American: primarily a small faction of white, male, atheist, Libertarian, middle- and upper-middle-income, computer-engineering "geeks" who believe passionately in the free market and its ability to support technological progress...When I interviewed him, Jerry Lemler, former president of Alcor, claimed that a "typical cryonicist" is highly educated, white, American, male, well-read, employed in a computer or technical field, "not very social," often single, has few or no children, is atheistic or agnostic, and is not wealthy but financially stable. Lemler also told me that cryonicists tend to have very strong Libertarian political views, believing in the rights of the individual and the power of the free market, although Lemler himself is a self-proclaimed "bleeding heart Liberal." Less than 25% of Alcor's members were women, and only a small fraction of these women joined purely out of their own interest; most female Alcor members were the wives, partners, daughters, or mothers of a man who joined first. Lemler also said that cryonicists are highly adventurous, although he added, "You may not see that in their *current* lives. In fact, we have the bookish types, if you will, as I just described. You wouldn't think that they'd be willing to take a chance on this particular adventure."...Like any group, the cryonics community is by no means uniform in demography, thought, or opinion. The majority of cryonicists I met were, indeed, software or mechanical engineers. But I also encountered venture capitalists, traders, homemakers, a shaman, a journalist, a university professor, cryobiologists, an insurance broker, artificial intelligence designers, a musician, men, women, children, people of color, people in perfect health, and people who were terminally ill. Nevertheless, a sort of Weberian "ideal type" (Weber 2001[1930]) of the typical cryonicist has emerged, and this is how cryonicists recognize themselves and one another. ...In an effort to bring the quite passionate technical discussion to a close, one member made a public aside to me, the anthropologist, loud enough for the benefit of everyone in the room. He said, "You know that a typical cryonicist is a male computer programmer, don't you?" Everyone laughed. Another member shouted out, "And a Libertarian!" Everyone laughed harder. Everyone appeared to enjoy the joke, which seemed to reaffirm the group's identity and to promote a kind of solidarity among them.

[^ethics]: In the [2009 LessWrong](http://lesswrong.com/lw/fk/survey_results/) survey, 94/73.4% were consequentialists, and those who didn't believe in morality were only one fewer than the deontologists! (There were 5 virtue ethicists, to cover the last major division of modern secular ethics.) The results were similar in the [2011](http://lesswrong.com/lw/8p4/2011_survey_results/) & [2012](http://lesswrong.com/lw/fp5/2012_survey_results/) surveys.

The results of one long-running online survey (from the sample size, LessWrongers probably made up <0.5% of the sample); ["Understanding Libertarian Morality: The Psychological Roots of an Individualist Ideology"](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1665934 "Iyer et al 2011") (as [summarized by the _WSJ_](http://lesswrong.com/lw/ess/link_inside_the_cold_calculating_mind_of_lesswrong/)):

> Perhaps more intriguingly, when libertarians reacted to moral dilemmas and in other tests, they displayed less emotion, less empathy and less disgust than either conservatives or liberals. They appeared to use "cold" calculation to reach utilitarian conclusions about whether (for instance) to save lives by sacrificing fewer lives. They reached correct, rather than intuitive, answers to math and logic problems, and they enjoyed "effortful and thoughtful cognitive tasks" more than others do. The researchers found that libertarians had the most "masculine" psychological profile, while liberals had the most feminine, and these results held up even when they examined each gender separately, which "may explain why libertarianism appeals to men more than women."

This clustering could be due solely to social networks and whatnot. But suppose they're not. Is there any perspective which explains this, and cryonic's "hostile wife phenomenon" as well?

Let's look at the key quotes about that phenomenon, and a few quotes giving the reaction

> The authors of this article know of a number of high profile cryonicists who need to hide their cryonics activities from their wives and ex-high profile cryonicists who had to choose between cryonics and their relationship. We also know of men who would like to make cryonics arrangements but have not been able to do so because of resistance from their wives or girlfriends. In such cases, the female partner can be described as nothing less than hostile toward cryonics. As a result, these men face certain death as a consequence of their partner's hostility. While it is not unusual for any two people to have differing points of view regarding cryonics, men *are* more interested in making cryonics arrangements. A recent membership update from the Alcor Life Extension Foundation reports that 667 males and 198 females have made cryonics arrangements. Although no formal data are available, it is common knowledge that a substantial number of these female cryonicists signed up after being persuaded by their husbands or boyfriends. For whatever reason, males are more interested in cryonics than females. These issues raise an obvious question: are women more hostile to cryonics than men?
>
> ...Over the 40 years of his active involvement, one of us (Darwin) has kept a log of the instances where, in his personal experience, hostile spouses or girlfriends have prevented, reduced or reversed the involvement of their male partner in cryonics. This list (see appendix) is restricted to situations where Darwin had direct knowledge of the conflict and was an Officer, Director or employee of the cryonics organization under whose auspices the incident took place. This log spans the years 1978 to 1986, an 8 year period...The 91 people listed in this table include 3 whose deaths are directly attributable to hostility or active intervention on the part of women. This does not include the many instances since 1987 where wives, mothers, sisters, or female business partners have materially interfered with a patient's cryopreservation(3) or actually caused the patient not to be cryopreserved or removed from cryopreservation(4). Nor does it reflect the doubtless many more cases where we had no idea...
>
> ...The most immediate and straightforward reasons posited for the hostility of women to cryonics are financial. When the partner with cryonics arrangements dies, life insurance and inheritance funds will go to the cryonics organization instead of to the partner or their children. Some nasty battles have been fought over the inheritance of cryonics patients, including attempts of family members to delay informing the cryonics organization that the member had died, if an attempt was made at all(5). On average, women live longer than men and can have a financial interest in their husbands' forgoing cryonics arrangements. Many women also cite the "social injustice" of cryonics and profess to feel guilt and shame that their families' money is being spent on a trivial, useless, and above all, selfish action when so many people who could be saved are dying of poverty and hunger now...Another, perhaps more credible, but unarguably more selfish, interpretation of this position is what one of us (Darwin) has termed "post reanimation jealousy." When women with strong religious convictions who give "separation in the afterlife" as the reason they object to their husbands' cryopreservation are closely questioned, it emerges that this is not, in fact, their primary concern. The concern that emerges from such discussion is that if cryonics is successful for the husband, he will not only resume living, he may well do so for a vast period of time during which he can reasonably be expected to form romantic attachments to other women, engage in purely sexual relationships or have sexual encounters with other women, or even marry another woman (or women), father children with them and start a new family. This prospect evokes obvious insecurity, jealousy and a nearly universal expression on the part of the wives that such a situation is unfair, wrong and unnatural. Interestingly, a few women who are neither religious nor believers in a metaphysical afterlife have voiced the same concerns. The message here may be "If I've got to die then you've got to die too!" As La Rochefoucauld famously said, with a different meaning in mind, "Jealousy is always born with love, but does not always die with it."...While cryonics is mostly a male pursuit, there are women involved and active, and many of them are single. Wives (or girlfriends) justifiably worry that another woman who shares their husbands' enthusiasm for cryonics, shares his newly acquired world view and offers the prospect of a truly durable relationship - one that may last for centuries or millennia - may win their husbands' affections. This is by no means a theoretical fear because this has happened a number of times over the years in cryonics. Perhaps the first and most [publicly acknowledged](http://www.lifepact.com/history.htm) instance of this was the divorce of Fred Chamberlain from his wife (and separation from his two children) and the break-up of the long-term relationship between Linda McClintock (_nee_ Linda Chamberlain) and her long-time significant other as a result of Fred and Linda working together on a committee to organize the Third National Conference On Cryonics (sponsored the Cryonics Society of California).^[["Is That What Love is? The Hostile Wife Phenomenon in Cryonics"](http://www.depressedmetabolism.com/pdfs/hostile.pdf), by Michael G. Darwin, Chana de Wolf, and Aschwin de Wolf; [HTML version](http://www.depressedmetabolism.com/is-that-what-love-is-the-hostile-wife-phenomenon-in-cryonics/)]

Going back to Romain 2010, reproduction is also a theorized concern:

> For many cryonicists, having children is considered an unnecessary diversion of resources that can and should be devoted to the self, especially if one is to achieve immortality. Phil, one of the few cryonicists I know with children, once said to me, "They're good kids. But if their moms hadn't wanted them, they wouldn't exist." He did not see much value in passing on genes or creating new generations and preferred to work toward a world in which people no longer need to procreate since the extension of human lifespans would maintain the human species. Indeed, I have heard some in the community theorize that having children is an evolutionary byproduct that could very well become vestigial as humans come closer and closer to becoming immortal. I have also heard several lay theories within the cryonics community about genetic or brain structure differences between men and women that cause men to favor life-extension philosophies and women to favor procreation and the conservative maintenance of cultural traditions...In a very different example, Allison wanted to have children but decided that she will wait until post-reanimation because she was single and in her mid-30s and thus approaching age-related infertility (medicine of the future would also reverse loss of fertility, she assumed). When I suggested that she might freeze her eggs so that she could possibly have genetically related children later in life, she responded that she has too much work to accomplish in the immediate future and would rather wait until she "came back" to experience parenthood.

Eliezer Yudkowsky, remarking on the number of women in one cryonics gathering, inadvertently demonstrates that the gender disparity is still large:

> This conference was just young people who took the action of signing up for cryonics, and who were willing to spend a couple of paid days in Florida meeting older cryonicists. The gathering was 34% female, around half of whom were single, and a few kids. This may sound normal enough, unless you've been to a lot of contrarian-cluster conferences, in which case you just spit coffee all over your computer screen and shouted "WHAT?" I did sometimes hear "my husband persuaded me to sign up", but no more frequently than "I persuaded my husband to sign up". Around 25% of the people present were from the computer world, 25% from science, and 15% were doing something in music or entertainment - with possible overlap, since I'm working from a show of hands. I was *expecting* there to be some nutcases in that room, people who'd signed up for cryonics for just the same reason they subscribed to homeopathy or astrology, i.e., that it sounded cool. *None* of the younger cryonicists showed any sign of it. There were a couple of older cryonicists who'd gone strange, but none of the young ones that I saw. Only three hands went up that did *not* identify as atheist/agnostic, and I think those also might have all been old cryonicists.^[["Normal Cryonics"](http://lesswrong.com/lw/1mc/normal_cryonics/), [Eliezer Yudkowsky](!Wikipedia)]

Some female perspectives:

> Well, as a woman, I do have the exact same gut reaction [to cryonics]. I'd never want to be involved with a guy who wanted this. It just seems horribly inappropriate and wrong, and no it's nothing to do at all with throwing away the money, I mean I would rather not throw away money but I could be with a guy who spent money foolishly without these strong feelings. I don't know that I can exactly explain why I find this so distasteful, but it's a very instinctive recoil. And I'm not religious and do not believe in any afterlife. It's sort of like being with a cannibal, even a respectful cannibal who would not think of harming anyone in order to eat them would not be a mate I would ever want.^[['Anne'](http://www.overcomingbias.com/2010/07/modern-male-sati.html#comment-450483), commenting on Overcoming Bias]

> "You have to understand," says Peggy, who at 54 is given to exasperation about her husband's more exotic ideas. "I am a hospice social worker. I work with people who are dying all the time. I see people dying All. The. Time. And what's so good about me that I'm going to live forever?"
>
> ...Peggy finds the quest an act of cosmic selfishness. And within a particular American subculture, the pair are practically a clich. Among cryonicists, Peggy's reaction might be referred to as an instance of the "hostile-wife phenomenon," as discussed in a 2008 paper by Aschwin de Wolf, Chana de Wolf and Mike Federowicz."From its inception in 1964," they write, "cryonics has been known to frequently produce intense hostility from spouses who are not cryonicists." The opposition of romantic partners, Aschwin told me last year, is something that "everyone" involved in cryonics knows about but that he and Chana, his wife, find difficult to understand. To someone who believes that low-temperature preservation offers a legitimate chance at extending life, obstructionism can seem as willfully cruel as withholding medical treatment. Even if you don't want to join your husband in storage, ask believers, what is to be lost by respecting a man's wishes with regard to the treatment of his own remains? Would-be cryonicists forced to give it all up, the de Wolfs and Federowicz write, "face certain death."
>
> ...Cryonet, a mailing list on "cryonics-related issues," takes as one of its issues the opposition of wives. (The ratio of men to women among living cyronicists is roughly three to one.) "She thinks the whole idea is sick, twisted and generally spooky," wrote one man newly acquainted with the hostile-wife phenomenon. "She is more intelligent than me, insatiably curious and lovingly devoted to me and our 2-year-old daughter. So why is this happening?"...A small amount of time spent trying to avoid certain death would seem to be well within the capacity of a healthy marriage to absorb. The checkered marital history of cryonics suggests instead that a violation beyond nonconformity is at stake, that something intrinsic to the loner's quest for a second life agitates against harmony in the first...But here he doesn't expect to succeed, and as with most societal attitudes that contradict his intuitions, he's got a theory as to why. "Cryonics," Robin says, "has the problem of looking like you're buying a one-way ticket to a foreign land." To spend a family fortune in the quest to defeat cancer is not taken, in the American context, to be an act of selfishness. But to plan to be rocketed into the future - a future your family either has no interest in seeing, or believes we'll never see anyway - is to begin to plot a life in which your current relationships have little meaning. Those who seek immortality are plotting an act of leaving, an act, as Robin puts it, "of betrayal and abandonment."^[["Until Cryonics Do Us Part"](http://www.nytimes.com/2010/07/11/magazine/11cryonics-t.html), _NYT_, Kerry Howley]

> As the spouse of someone who is planning on undergoing cryogenic preservation, I found this article to be relevant to my interests! My first reactions when the topic of cryonics came up (early in our relationship) were shock, a bit of revulsion, and a lot of confusion. Like Peggy (I believe), I also felt a bit of disdain. The idea seemed icky, childish, outlandish, and self-aggrandizing. But I was deeply in love, and very interested in finding common ground with my then-boyfriend (now spouse). We talked, and talked, and argued, and talked some more, and then I went off and thought very hard about the whole thing...Ultimately, my struggle to come to terms with his decision has been more or less successful. Although I am not (and don't presently plan to be) enrolled in a cryonics program myself, although I still find the idea somewhat unsettling, I support his decision without question. If he dies before I do, I will do everything in my power to see that his wishes are complied with, as I expect him to see that mine are. Anything less than this, and I honestly don't think I could consider myself his partner.^[[C](http://community.nytimes.com/comments/www.nytimes.com/2010/07/11/magazine/11cryonics-t.html?permid=44#comment44)]

> To add a data point, I found myself, to put it strongly, literally losing the will to live recently: I'm 20 and female and I'm kind of at the emotional maturity stage. I think my brain stopped saying "live! Stay alive!" and started saying "Make babies! Protect babies!", because I started finding the idea of cryopreserving myself as less attractive and more repulsive (with no change in opinion for preserving my OH), and an increase in how often I thought about doing the right thing for my future kids. To the extent that I now get orders of magnitude more panicked about anything happening to my reproductive system than dying after future children reach adulthood.^[[Sarokrae](http://lesswrong.com/r/discussion/lw/e0z/mentioning_cryonics_to_a_dying_person/78er)]

[Quentin's](http://www.overcomingbias.com/2010/07/modern-male-sati.html#comment-450481) explanation is even more extreme:

> What follows below is the patchwork I have stitched together of the true female objections to a mate undergoing cryonic suspension. I believe many women have a constant low-level hatred of men at a conscious or subconscious level and their narcissistic quest for entitlement and [meaningfulness] begrudges him any pursuit that isn't going to lead directly to producing, providing, protecting, and problem solving for her. It would evolutionarily be in her best interest to pull as many emotional and physical levers to bend as much of his energies toward her and their offspring as she can get away with and less away from himself. That would translate as a feeling of revulsion toward cryonics that is visceral but which she dares not state directly to avoid alerting her mate to her true nature.
>
> She doesn't want him to live for decades, centuries, or millennia more in a possibly healthier and more youthful state where he might meet and fall in love with new mates. She doesn't want her memory in his mind to fade into insignificance as the fraction of time she spent with him since she has died to be a smaller and smaller fraction of his total existence; reduced to the equivalent in his memory of an interesting conversation with a stranger on the sidewalk one summer afternoon. She doesn't want him to live for something more important than HER. So why not just insist she join him in cryonic suspension? Many of these same wives and girlfriends hate their life even when they are succeeding. Everyone is familiar with the endless complaints, tears, and heartache that make up the vast majority of the female experience stemming from frustration of her hypergamous instinct to be the princess she had always hoped to be and from resentment of his male nature, hopes, dreams, and aspirations. She thinks: "He wasn't sexually satisfying! He isn't romantic enough! He never took me anywhere! He didn't pay attention to me! Our kids aren't successes! We live in a dump! His hobbies are a waste of time and money! My mother always told me I can do better, and his mother will never stop criticizing me! I am fat, ugly, unsuccessful, old, tired, and weary of my responsibilities, idiosyncrasies, insecurities, fears, and pain. My life sucked but at least it could MEAN something to those most important to me." But if they are around for too long it shrinks in importance over time.She wants you to die forever because she hates what you are. She wants to die too, because she hates what she is. She wants us all to die because she hates what the world is and has meant to her.

In the same vein:

>> But why not go with him then [into cryonics]?
>
> Show me the examples of the men who asked, or even insisted that their wives go with them, and said "If you don't go with me, I won't go". The fact that men generally don't do this, is likely a big contributor to the female reaction. Imagine your husband or boyfriend telling you, "I just scheduled a 1 year vacation in Pattaya, and since I know you hate Thai food, I didn't buy you tickets. I'll remember you fondly." That's very different from the man who says, "I've always dreamed of living in Antarctica, but I won't do it without you, so I'm prepared to spend the next 5 years convincing you that it's a great idea".^[[JS Allen](https://meteuphoric.wordpress.com/2010/08/08/on-the-hostility-of-wives/#comment-1393), commenting on Katya Grace's post on hostile wives, ["Why do 'respectable' women want dead husbands?"](https://meteuphoric.wordpress.com/2010/08/08/on-the-hostility-of-wives/)]

> Indeed, I buy the "one way ticket away from here" explanation. If I bought a one-way ticket to France, and was intent on going whether my wife wanted to come with me or not, then there would be reason for her to be miffed. If she didn't want to go, the "correct" answer is "I won't go without you". But that is not the answer the cryonicist gives to his "hostile" wife. It's like the opposite of "I would die for you" - he actually got a chance to take that test, and failed.^[[Thom Blake](https://meteuphoric.wordpress.com/2010/08/08/on-the-hostility-of-wives/#comment-1427)]

Robin Hanson tries to explain it in terms of evolutionary incentives:

> Mating in mammals has a basic asymmetry - females must invest more in each child than males. This can lead to an equilibrium where males focus on impressing and having sex with as many females as possible, while females do most of the child-rearing and choose impressive males.
>
> ...And because they are asymmetric, their betrayal is also asymmetric. Women betray bonds more by temporarily having fertile sex with other men, while men betray bonds more by directing resources more permanently to other women. So when farmer husbands and wives watch for signs of betrayal, they watch for different things. Husbands watch wives more for signs of a temporary inclination toward short-term mating with other men, while wives watch husbands more for signs of an inclination to shift toward a long-term resource-giving bond with other women. This asymmetric watching for signs of betrayal produces asymmetric pressures on appearances. While a man can be more straight-forward and honest with himself and others about his inclinations toward short-term sex, he should be more careful with the signs he shows about his inclinations toward long term attachments with women. Similarly, while a woman can be more straight-forward and honest with herself and others about her inclinations toward long-term attachments with men, she should be more careful with the signs she shows about her inclinations toward short term sex with men.
>
> ...Standard crude stereotypes of gender differences roughly fit these predictions! That is, when the subject is one's immediate lust and sexual attraction to others, by reputation men are more straight-forward and transparent, while women are more complex and opaque, even to themselves. But when the subject is one's inclination toward and feelings about long-term attachments, by reputation women are more self-aware and men are more complex and opaque, even to themselves...if cryonics is framed as abandonment, women should be more sensitive to that signal.^[["Why Men Are Bad At 'Feelings'"](http://www.overcomingbias.com/2011/07/homo-hypocritus-mates.html), [Robin Hanson](!Wikipedia)]

The "selfishness" of cryonics does seem to be an issue for women and many men; one might wonder, would other heroic medical procedures be more socially acceptable if they involved "other-directedness"? I suggest the answer is yes: [cord blood banking](http://lesswrong.com/lw/8qz/cryonics_is_far_cordblood_is_near/ "Cryonics is Far, Cord-blood is Near") costs thousands with a lower (<0.1%) success rate (usage of the cord blood) than many cryonicists expect of cryonics (the Fermi estimates tend to be [<5%](plastination)); [sperm banking](Ethical sperm donation) costs a similar amount, while egg/oocycte banking may cost something like half what cryonics does! In the media coverage I have read of those 3 practices, I have the impression that people see them as legitimate medical procedures albeit ones where the cost-benefit equation may not work out. (Cryonicists, on the other hand, are just nuts.) Perhaps this is because sperm and egg banking - while fundamentally selfish, since if you cannot use your egg or sperm later, why don't you want to adopt? - involves the creation of another person as hallowed by society.

##### Reductionism is the common thread?

The previously listed 'systems of thought', as it were, all seem to share a common trait: they are made of millions or trillions of deterministic interacting pieces. Any higher-level entity is not an ontological atom, and those higher-level illusions can be manipulated in principle nigh-arbitrarily given sufficient information.

That the higher-level entities really are nothing but the atomic units interacting is the fundamental [pons asinorum](!Wikipedia) of these ideologies, and the one that nonbelievers have not crossed.

We can apply this to each system.

- Many doubters of cryonics doubt that a bunch of atoms vitrified in place is *really* 'the self'.
- Many users of computers anthropomorphize it and can't accept that it is really just a bunch of bits (this is related to the thesis that [the camel has two humps](#the-camel-has-two-humps), the test being, basically, whether a sample program will be executed as-is by the (dumb) computer)
- Many doubters of materialist philosophy of mind are not willing to say that an extremely large complex enough system can constitute a consciousness
- Many doubters of utilitarianism doubt that there really is a best choice or good computable approximations to the ideal choice, and either claim utilitarianism fails basic ethical dilemmas by [forcing the utilitarian to make the stupid choice](http://lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/) or instead vaunt as the end-all be-all of ethics what can be easily be formulated as simply heuristics and approximation, like [virtue ethics](!Wikipedia)^[I always wondered - suppose one cultivates a character of generosity, bravery, etc. How does *that* character decide? Virtue ethics seems like buck-passing to me.]
- Many doubters of libertarianism doubt that prices can coordinate multifarious activities, that the market really will find a level, etc. Out of the chaos of the atoms interacting is supposed to come all good things...? This seems arbitrary, unfair, and unreasonable.
- The same could be said of evolution. Like the profit motive, how can mere survival generate "from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved"^[Charles Darwin, _[On the Origin of Species](!Wikipedia)_, (1st ed.)]?
- Finally, atheism. A faith of its own in the power of reductionist approaches across *all* fields. What is a God, but the ultimate complex high-level irreducible ontological entity?

In all, there is incredulity at sheer numbers. An ordinary person can accept a few layers since that is what they are used to - a car is made of a few dozen systems with a few discrete thousand parts, a dinner is made of 3 or 4 dishes with no more than a dozen ingredients, etc. The ordinary mind quails at systems with *millions* of components (number of generations evolution can act on), much less billions (length of programs, number of processor cycles in a second) or trillions (number of cells in human body, number of bits on consumer hard drives).

If one doesn't deal first-hand with this, if one has never worked with them at any level, how does one *know* that semiconductor physics is the sublayer for circuits, the sublayer for logic gates; logic gates the sublayer for memory and digital operations, which then support the processor with its fancy instruction like `add` or `mov`, which enables machine code, which we prefer to write as assembler (to be compiled and the linked into machine code), which can be targeted by programming languages, at which point we have only begun to bring in the operating system, libraries, and small programs, which let us begin to think about how to write something like a browser, and a decade later, we have Firefox which will let Grandma go to AOL Mail.

(To make a mapping, the utilitarian definition is like defining a logic gate; the ultimate decisions in a particular situation are like an instance of Firefox, depending on trillions of intermediate steps/computations/logic gates. Non-programmers can't see how to work backwards from Firefox to individual logic gates, and their blindness is so profound that they can't even see that there *is* a mapping. Compare all the predictions that 'computers will never X'; people can't see how trillions of steps or pieces of data could result in computers doing X, so - 'argument from incredulity' - they then believe there is no such way.)

A programmer will have a hard time being knowledgeable about programming and debugging, and also not appreciative of reductionism in his bones. If you tell him that a given system is actually composed of millions of interacting dumb bits - he'll believe you. Because that's all his programs are. If you tell a layman that his mortgage rate is being set by millions of interacting dumb bits (or his mind...) - he'll probably think you're talking bullshit.

Religious belief seems to [correlate and causate](http://www.apa.org/pubs/journals/releases/xge-ofp-shenhav.pdf) with quick intuitive thinking (and deontological judgments [as well](http://lesswrong.com/lw/74f/are_deontological_moral_judgments_rationalizations/)), and what is more counterintuitive than reductionism?

I don't know if this paradigm is correct, but it does explain a lot of things. For example, it correctly predicts that evolutionism will be almost universally accepted among the specified groups, even though logically, there's no reason cryonicists have to be evolutionists or libertarians, and vice-versa, no reason libertarians would have any meaningful correlation with utilitarianism.

I would be deeply shocked & fascinated if there were data showing that they were uncorrelated or even inversely correlated; I could understand libertarianism correlating inversely with atheism, at least in the peculiar circumstances of the United States, but I would expect all of the others to be positively correlated. The only other potential counterexample I can think of would be engineers and terrorism, and that is a relatively small and rare correlation.

#### Domain-squatting externalities

In developing my [custom search engine](http://www.google.com/cse/home?cx=009114923999563836576:1eorkzz2gp4) for finding [sources](!Wikipedia "WP:RS") for Wikipedia articles, one of its chief benefits turned out to nothing other than filtering out mirrors of Wikipedia! Since one is usually working on an *existing* article, that means there may be hundreds or thousands of copies of the article floating around the Internet, all of which match very well the search term one is using, but which contribute nothing. This is one of the hidden costs of having a FLOSS license: the additional copying imposes an overhead^[This is also true of new content in general; they are not a pure win, but impose additional costs on catalogers and collectors and libraries and whatnot. This is true even when they do not take a common name or word as their title, as lamentably many new works do. New works in general are hard to justify; see [Culture is not about Esthetics]().]. This cost is not borne by the copier, who may be making quite a bit of money on their Wikipedia mirror, even penalized by Google as they have since become. In other words, cluttering up searches is a *[negative externality](!Wikipedia)*. (One could say the same thing of the many mirrors or variant versions of social news sites like Hacker News. Who are they imposing costs upon unilaterally?)

Domain-squatters are another nuisance; so often I have gone to an old URL and found nothing but a parking domain, with maybe the URL plugged into a Google search underneath a sea of random ads. But, the libertarian objects, clearly these domain-squatters are providing a service since otherwise there would be no advertising revenue and the domain-squatters could not afford to annually renew the domain, much less turn a profit.

But here is another clear case of externalities.

On parking domains, only 1 person out of thousands is going to click on an ad (at best), find something useful to them, and make the ads a paying proposition. but those other thousands are going to be slowed down - the page has to be loaded, they have to look at it, analyze it, and realize that it's not what they wanted and try something else like a differently spelled domain or a regular search. A simple domain-not-found error would have been faster by a second at least, and less mental effort. The wasted time, the cost to those thousands, is *not* borne by the domain-squatter, the ad-clicker, or the advertiser. They are externalizing the costs of their existing.

#### Worldbuilding: The Lights in the Sky are Sacs

On page 217 of evolutionary biologist [Geoffrey Miller](!Wikipedia)'s 2011 book [_Spent_](http://www.amazon.com/Spent-Sex-Evolution-Consumer-Behavior/dp/0143117238/), in the middle of some [fairly interesting](http://lesswrong.com/lw/82g/on_the_openness_personality_trait_rationality/) material on [Openness to Experience](!Wikipedia), one reads:

> '...Our six verbal creativity tasks included questions like: "Imagine that all clouds had really long strings hanging from them - strings hundreds of feet long. What would be the implications of that fact for nature and society?"...

To make the obvious point: strings hundreds of feet long strong enough to support themselves and any real weight are better termed 'ropes'. And ropes are heavy. There's no obvious way to change physics to permit just ropes to not be heavy, in the same way you can't [remove fire & keep cellular respiration](http://lesswrong.com/lw/hq/universal_fire/). (If we insist on the 'string' language ad the implication that the strings are weak and thin, we can take some sort of [arachnid tack](!Wikipedia "Ballooning (spider)"), which would be either creepy or awesome.) So let's engage in a little [worldbuilding](!Wikipedia) exercise and imagine alternatives.

A cloud with a rope dangling is an awful lot like a balloon or lighter-than-air vehicles in general. How do they work? Usually by using hot air, or with a intrinsically lighter gas like helium or hydrogen. Both need good seals, though, which is something a biological organism can do. But where is an organism going to get enough heat to be a living hot air balloon? So maybe it uses helium instead, but then, where does it get helium? We get helium by applying hundreds of billion of dollars in R&D to digging deep narrow holes in the ground, which is not a viable strategy for a global population of clouds. So hydrogen? That'd work actually; hydrogen is very easy to obtain, just crack water! Even better, the organisms creating this hydrogen to obtain flight could reuse the hydrogen for energy - just burn with oxygen! The Laws of Thermodynamics say that burning wouldn't *generate* any new energy, so this isn't what they feed on. But the answer presents itself - if you're in the *sky* or better yet, above the cloud layer, there' something very valuable up there - sunlight. Trees grow so big and engage in chemical warfare just to get access to the sun, but our hydrogen sacs soar over the groundlings. There might be a similar competition, but the sacs have their own problems: as altitude increases, ambient pressure decreases (which is good) but temperatures plunge (bad) and other forms of radiation increase (ultraviolet?). As well, if our sacs are photosynthetic, they need inputs: water & carbon dioxide for photosynthesis, and the usual organic bulk materials & rarer elements for themselves. Which actually explains where our ropes are coming from: they are the sacs' "roots".

How could such a lifeform evolve? I have no idea. There are animals which glide (eg. flying squirrel), others which are dispersed by wind (spiders), and so on, but none that actually crack water into hydrogen & oxygen or exploit hydrogen for gliding or buoyancy. And there are serious issues with the hydrogen sacs: lightning would seem to be a problem... Still, we could reuse our 'competition for solar radiation' idea; maybe a tree, striving to be taller but running into serious engineering issues to do with [power laws](!Wikipedia), tweaked its photosynthesis to divert some of the split hydrogen to storage vacuoles which would make it lighter and able to grow a little taller. Rise and repeat for millions of years to obtain something which is free-floating and has shed much of its old tree-form for a new spherical shape.

Imagine that a plant or animal did so evolve, and evolved before humanity did. Millions of floating creatures around the world, each one with lifting capacity of a few pounds; or since they could probably grow very large without the same engineering limitations as trees, perhaps hundreds to thousands of pounds. When humanity gets a clue, they will seize on the sacs without hesitation! Horses changed history, and the sacs are *better* than horses. The sacs are mobile over land and sea, hang indefinitely, allow aerial assaults, and would be common. It's hard to imagine a Great Wall of China effective against a sac-mounted nomad force! There's [barrage balloons](!Wikipedia), but those are impossibly expensive on any large scale.

More troubling, early states had major difficulties maintaining control. When you read about ancient Egypt or China or Rome, again and again one encounters barbarians or nomads invading or conquering entirely the state, and how they were, man for man, superior to the soldiers of the government. Relatively modest technical innovations meant that when the Mongols got their act together and refined their strategy, they conquered most of the world. Formal empires and states are not inevitable outcomes, as much as they dominate our thinking in modern times - they didn't exist for most of human history, didn't control most territory or people for much of the period they could be said to exist, and it's unclear how much longer they will survive even in this age of their triumph & universalization. History is shot through with contingency and luck. That China did not have an Industrial Revolution and oddball England did is a matter to give us pause.

What happens when we give nomadic humans, in the un-organized part of history, a creature unparalleled in mobility? At the very least, I think we can expect any static agriculture-based empire (the Indus, Yang-tze, Nile) to be strangled in its cradle. Without states, history would be completely different with few recognizable entities except perhaps ethnicities. The English state seemed closely involved in the Industrial Revolution (funding the Age of Exploration, patents, etc.) and also the concurrent scientific revolution (it is the *Royal* society, and even Newton worked much of his life for the Crown). No state, no Revolution? As cool as it would be to ride a sac around the world, I wouldn't trade them for science and technology.

But optimistically, could we expect something else to arise - so that the sac variant of human history not be one damn thing after another, happy savages until a pandemic or asteroid finally blots out the human world? I think so. If a sac can lift one person, then can't we tie together sacs and lift multiple people? Recycling ropes from dead sacs, we could bind together hundreds of sacs and suspend buildings from them. (I say suspend because to put them 'on top' of the sac-structure would cut off the light that the sacs need and might also be unstable as well.) A [traveling village](fiction/Missing Cities#i) would naturally be a trading village - living in the air is dangerous, so I suspect there will always be villages planted firmly on the ground (even if they keep a herd of sacs of their own). This increased mobility and trade might spark a global economy of its own.

I failed to mention earlier that the sacs, besides being a potent tool of mobility exceeding horses, could also constitute a weapon of their own: a highly refined and handy package of hydrogen. Hydrogen burns very well. If nothing else, it makes arson and torching a target very handy. Could sacs be *weaponized*? Could a nomad take a sac, poke a spigot into it, light a match and turn the sac into a rocket with a fiery payload on impact? If they can be, then things look very dim indeed for states. But on the flip side, hydrogen burns hot and [oxyhydrogen](!Wikipedia) was one of the first mixtures for welding. Our nomads will be able to easily melt and weld tough metals like iron. Handy.

I leave the thought exercise at this point, having overseen the labefaction of the existing world order and pointed at a potential iron-using airborne anarchy. Which of the two is a better world, I leave to the unknowable unfolding of the future.

#### On meta-ethical optimization

> "The killer whale's heart weighs one hundred kilos
> but in other respects it is light.
> There is nothing more animal-like
> than a clear conscience
> on the third planet of the Sun." --"In Praise of Self-Deprecation ", [Wislawa Szymborska](http://www.journeywithjesus.net/PoemsAndPrayers/Wislawa_Szymborska.shtml)

> "Jesus said unto him, If thou wilt be perfect, go and sell that thou hast, and give to the poor, and thou shalt have treasure in heaven: and come and follow me." --Matthew 19:21

When I or another utilitarian point out (eg. in [Charity is not about helping]()) that it costs only a few thousand dollars to reliably save a human life, and then note that one choosing to spend money on something else is choosing to not save that life, one of the common reactions is that this is true of every expenditure and that this implies we ought to donate most or all of our wealth.

This is quite true. If you have \$10,000 and you donate it all, there will be say 5 more humans alive than in the counterfactual scenario where you spend \$10,000 on a daily cup of coffee at Starbucks. This is a fact about how the world works. To deny it requires quibbling about probabilities and expected value (despite one accepting them in every other part of one's life) or engaging in desperate postulations about infinitely precise counter-balancing mechanisms ("maybe if I donate, that means someone somewhere will donate that much less! So it conveniently doesn't matter whether or not I do, I don't make a difference!"). Fundamentally, if to give a little helps, then for non-billionaires, giving a lot helps more, and given even more helps even more. What a dull point to make.

But the *reaction* to this dull point is interesting. Apparently for many people, this shows that utilitarianism is not correct! I saw this particularly in the reception to [Peter Singer](!Wikipedia)'s book _[The Life You Can Save](!Wikipedia)_ - that Singer to some extent lives up to his proposed standards seems to make the ideas even more intolerable for these people.

It seems that people intuitively think that the true ethical theory will not be *too* demanding. This is rather odd.

A few criteria are common in meta-ethics, that the One True Ethics should satisfy. For example, universalizability: the One True Ethics should apply to Pluto just as much as it does Earth, or work a few galaxies over just like we would apply it in the Milky Way. Similarly for time: it'd be an odd and unsatisfying ethics which said casual murder was forbidden before 2050 AD but OK afterwards. (Like physics, the rules should stay the same, even if different input means different output.) It ought to cover all actions and inactions, if only to classify it as morally neutral. (It would be odd if one were pondering the morality of something and asked, only to be told in a very Buddhist way, that the action was: not moral, not immoral, not neither moral nor immoral, not both moral and immoral...) And finally, the ethical theory has to do *work*: it has to make relatively specific suggestions, and ideally those suggestions would be specific enough that it permits little and forbids much. (For example, could one base a satisfactory ethical theory on the Ten Commandments and nothing else? If all one had to do was be moral was to not violate a commandment? That would be not that hard, but I suspect, as we watch our neighbors fornicate with their goats and sheep, we will suspect that it is immoral even though nowhere in the Ten Commandments did God forbid bestiality - or many other things, for that matter, like child molestation.) The theory may not specify a *unique* action, but that's OK. (You see two strangers drowning and can save only one; your ethical theory says you can randomly pick, because saving either stranger is equally good. That seems fine to me, even though your ethics did not give you just one moral option, but two.)

Given that every person faces, at every moment, a mindboggling number of possible actions and inactions, even an ethics which permitted thousands of moral actions in a given circumstance is ruling out countless more. And since there are a lot of moments in a lifetime, that's a lot of actions too. Considering this, it would not be a surprise if people frequently chose immoral or amoral actions: no one bats a thousand and even Homer nods, as the sayings go. So there is a lot of room for improvement. If this were true of ethics, that would only mean ethics is like every other field of human endeavour in having an ideal that is beyond attainment - no doctor never makes a mistake, no chess player never overlooks an easy checkmate, no artist never messes up a drawing, and so on. There is no end to moral improvement:

> Disquiet in philosophy may be said to arise from looking at philosophy wrongly, seeing it wrong, namely as if it were divided into (infinite) longitudinal strips instead of into (finite) cross strips. This inversion in our conception produces the *greatest* difficulty. So we try, as it were, to grasp the unlimited strips and complain that it cannot be done piecemeal. To be sure it cannot, if by a piece one means an infinite longitudinal strip. But it may well be done, if one means a cross-strip. --But in that case we never get to the end of our work! --Of course not, for it has no end.^[Ludwig Wittgenstein's _[Zettel](!Wikipedia)_, 447]

Once we abandon the neurotic quest for certainty and perfection, then these ideas become acceptable:

> The moral code of our society is so demanding that no one can think, feel and act in a completely moral way. [...] Some people are so highly socialized that the attempt to think, feel and act morally imposes a severe burden on them. In order to avoid feelings of guilt, they continually have to deceive themselves about their own motives and find moral explanations for feelings and actions that in reality have a non-moral origin.^[[Theodore Kaczynski](!Wikipedia), _The Unabomber Manifesto: Industrial Society And Its Future_, ch 4, #25]

Yet, people seem to expect moral perfection to be easy! When utilitarianism tells them that they are far from being morally *perfect* (like they are not perfect writers or car drivers), they say that utilitarianism is stupid and sets unobtainable goals. Well, yes. Wouldn't it be awfully odd if goodness were as attainable as playing a perfect game of tic-tac-toe? If all one had to do to be a good person on par with heroes like Jonas Salk or Norman Borlaug was to simply not do anything awful and be nice to the people around you? ("It takes a certain lack of imagination to have an entirely clean conscience.") Why would one expect morality to be easy? Is morality really easier to master than making wine or cheese? Most human endeavors are hard, and ethics covers all our endeavors; and in those endeavours, people somehow seem comfortable being aware of their fallibility and the large gap between perfection and what they actually achieve - engineers do not say that the bridge which kills only a *few* people is perfect and a better bridge would be "supererogatory", mathematicians do not say that perfect proofs have only a *few* non sequiturs in them and fixing the gaps would be supererogatory, programmers do not regard a program with only a *few* bugs in it as the same as a perfect program...

To object to utilitarianism because it points to a very high ideal is reminiscent, to me, of rejecting heliocentrism because it makes the universe much bigger and the earth much smaller. The small-minded want an equally small-minded ethics.


#### Remote monitoring

Desire: some way to monitor freelancer's activity (if they are billing by time rather than results).

Why? This enables better compliance and turns freelancers into [less of a lemon market](!Wikipedia "The Market for Lemons") - allowing for higher salaries due to lower risk. Reportedly, such monitoring also helps one's own akrasia - one could use it both while 'working' and 'not working', just with someone else (akin to [coffee shops](http://www.theatlantic.com/business/archive/2011/04/working-best-at-coffee-shops/237372/) perhaps). The idea comes from Cousin It and Richard Hollerith's <http://lesswrong.com/lw/2qv/browser_buddies_remote_monitoring_experiment/> (even if it wouldn't go as far as letting one's [life be managed](http://lesswrong.com/lw/dp/fighting_akrasia_incentivising_action/4meu)!).

Potential solutions:

1. remote desktops: screenshots or video. Requirements:

     - cross-platform (Linux, Windows & Mac)
     - secure (eg. using SSH for transport, especially since we already use SSH for full-text access)
     - easily toggleable on and off

     Of the remote desktop protocols, only the VNC protocol is acceptable: has many open source & proprietary cross-platform implementations for both client and server on, and can be tunneled over SSH. ([Nick Tarleton](http://lesswrong.com/lw/2qv/antiakrasia_remote_monitoring_experiment/2pda) says Macs are already compatible with a VNC client user.) TightVNC seems like it would work well. (One difficulty: the natural tool to use once a VNC server is running on the remote desktop is [vncsnapshot](http://vncsnapshot.sourceforge.net/) which does what you think it does, but the Debian summary warns it does *not* work over SSH. [vnccapture](http://search.cpan.org/~lbrocard/Net-VNC-0.36/bin/vnccapture) may or may not work.)
2. browser URL logging (since much work takes place in browsers). Requirements:

    - cross-browser (Firefox, Chrome, Safari; IE users can die in a fire)
    - at a minimum, passworded

    [RescueTime](http://www.rescuetime.com) has a paid [group tracking](http://www.rescuetime.com/tour_biz) set of features that seems designed for this sort of task. There are many [other Internet possibilities](!Wikipedia "Comparison of time tracking software"). (I used to use the Firefox extension PageAddict which worked well for this sort of thing but is unmaintained; the most popular maintained extension, [Leechblock](https://addons.mozilla.org/en-US/firefox/addon/leechblock/), doesn't export statistics. [about:me](https://addons.mozilla.org/en-US/firefox/addon/aboutme/) would probably work, but wouldn't be automated.)
3. Other

    For example, my [sousveillance](http://lesswrong.com/lw/2qv/antiakrasia_remote_monitoring_experiment/2s3n) script; it would be trivial to set up a folder and then add a call to the script like `scp xwd-?????.png yyylj@euclid.u.washington.edu:/rc12/d16/yyli/screenshots/gwern/`. This should be easily implemented for Macs, but for Windows? I am sure it is doable to write some sort of batch script which integrates with Task Scheduler, but I left Windows before I wrote my first script, so I don't know how hard it would be.



#### Laplace's rule of succession, the Hope function, and Waiting for AI

["Chapter 18: the A_p_ Distribution and the Rule of Succession"](http://www-biba.inrialpes.fr/Jaynes/cc18i.pdf), from E.T. Jaynes's [_Probability Theory: The Logic of Science_](http://www.amazon.com/Probability-Theory-E-T-Jaynes/dp/0521592712/) starting pg 9

> Poor old Laplace has been ridiculed for over a Century because he illustrated use of this rule by calculating the probability that the sun will rise tomorrow, given that it has risen every day for the past 5,000 years.y One gets a rather large factor (odds of $5000 \times 365.2426 + 1 = 1826214 : 1$) in favor of the sun rising again tomorrow. With no exceptions at all as far as we are aware, modern writers on probability have considered this a pure absurdity. Even Keynes (1921) and Jeffreys (1939) find fault with the rule of succession. We have to confess our inability to see
>
> Here are some famous examples of the kind of objections to the rule of succession which you find in the literature:
>
> 1. Suppose the solidification of hydrogen to have been once accomplished. According to the rule of succession, the probability that it will solidify again if the experiment is repeated is 2/3. This does not in the least represent the state of belief of any scientist.
> 2. A boy 10 years old today. According to the rule of succession, he has the probability 11/12 of living one more year. His grandfather is 70; and so according to this rule he has the probability 71/72 of living one more year. The rule violates qualitative common sense!
> 3. Consider the case N = n = 0. It then says that any conjecture without verification has the probability 1/2. Thus there is probability 1/2 that there are exactly 137 elephants on Mars. Also there is probability 1/2 that there are 138 elephants on Mars. Therefore, it is certain that there are at least 137 elephants on Mars. But the rule says also that there is probability 1/2 that there are no elephants on Mars. The rule is logically self-contradictory!
>
> ...The trouble with examples (1) and (2) is obvious in view of our earlier remarks; in each case, highly relevant prior information, known to all of us, was simply ignored, producing a flagrant misuse of the rule of succession. But let's look a little more closely at example (3). Wasn't the rule applied correctly here? We certainly can't claim that we had prior information about elephants on Mars which was ignored.
>
> ...In the case N = 0, we could solve the problem also by direct application of the principle of indifference, and this will of course give the same answer P (A|X ) = 1/2, that we got from the rule of succession. But just by noting this, we see what is wrong. Merely by admitting the possibility of one of three different propositions being true, instead of only one of two, we have already specified prior information different from that used in deriving the rule of succession. If the robot is told to consider 137 different ways in which A could be false, and only one way in which it could be true, and is given no other information, then its prior probability for A is 1/138, not 1/2. So, we see that the example of elephants on Mars was, again, a gross misapplication of the rule of succession.
>
>
> We give the derivation in full detail, to present a mathematical technique of Laplace that is useful in many other problems. There are $K$ different hypotheses, ${A_1, A_2,...,A_K}$, a belief that the 'causal mechanism' is constant, and no other prior information. We perform a random experiment $N$ times, and observe $A_1$ true $n_1$ times, $A_2$ true $n_2$ times, etc. Of course, $\sum_{i} n_i = N$. On the basis of this evidence, what is the probability that in the next $M = \sum_{i} m_i$ repetitions, $A_i$ will be true exactly $m_i$ times?
>
> ...In the case where we want just the probability that $A_1$ will be true on the next trial, we need this
> formula with $M = m_1 = 1$, all other mi = 0. The result is the generalized rule of succession:
>
> > (18-39):    $p(A_1|n_1,N,K) = \frac{n_1 + 1}{N + K}$
>
> You see that in the case $N = n_1 = 0$, this reduces to the answer provided by the principle of indifference, which it therefore contains as a special case.
>
> ...Now, use of the rule of succession in cases where $N$ is very small is rather foolish, of course. Not really wrong; just foolish. Because if we have no prior evidence about $A$, and we make such a small number of observations that we get practically no evidence; well, that's just not a very promising basis on which to do plausible reasoning. We can't expect to get anything useful out of it. We do, of course, get definite numerical values for the probabilities, but these values are very 'soft', i.e., very unstable, because the $A_p$ distribution is still very broad for small $N$. Our common sense tells us that the evidence $N_n$ for small $N$ provides no reliable basis for further predictions, and we'll see that this conclusion also follows as a consequence of the theory we're developing here.
>
> The real reason for introducing the rule of succession lies in the cases where we do get a [large] amount of information from the experiment; i.e., when $N$ is a large number. In this case, fortunately, we can pretty much forget about these fine points concerning prior evidence. The particular initial assignment $(A_p | X)$ will no longer have much influence on the results, for the same reason as in the particle-counter problem of Chapter 6. This remains true for the generalized case leading to (18-38). You see from (18-39) that as soon as the number of observations $N$ is large compared to the number of hypotheses $K$, then the probability assigned to any particular hypothesis depends for all practical purposes, only on what we have observed, and not on how many prior hypotheses there are. If you contemplate this for ten seconds, your common sense will tell you that the criterion $N \gg K$ is exactly the right one for this to be so.

cf. [hope function](/docs/statistics/1994-falk), [bus waiting](/docs/statistics/2007-teigen.pdf)

#### Birthday game theory

From [Peter Watts](!Wikipedia "Peter Watts (author)")'s _[Blindsight](!Wikipedia "Blindsight (science fiction novel)"), ["Theseus"](http://www.rifters.com/real/Blindsight.htm#Theseus):

> And then, a bit defensive in spite of myself, I added, "I've found it [[game theory](!Wikipedia)] useful, though. In areas you might not expect it to be."
>
> "Yeah? Name one."
>
> "Birthdays," I said, and immediately wished I hadn't..."Well, according to game theory, you should never tell anyone when your birthday is."
>
> "I don't follow."
>
> "It's a lose-lose proposition. There's no winning strategy."
>
> "What do you mean, strategy? It's a *birthday*."
>
> Chelsea had said exactly the same thing when I'd tried to explain it to her. *Look*, I'd said, *say you tell everyone when it is and nothing happens. It's kind of a slap in the face.*
>
> *Or suppose they throw you a party*, Chelsea had replied.
>
> *Then you don't know whether they're doing it sincerely, or if your earlier interaction just guilted them into observing an occasion they'd rather have ignored. But if you* don't *tell anyone, and nobody commemorates the event, there's no reason to feel badly because after all, nobody* knew. *And if someone* does *buy you a drink then you know it's sincere because nobody would go to all the trouble of finding out when your birthday is-and then celebrating it-if they didn't honestly like you.*
>
> Of course, the Gang was more up to speed on such things. I didn't have to explain it verbally: I could just grab a piece of ConSensus and plot out the payoff matrix, _Tell/Don't Tell_ along the columns, _Celebrated/Not Celebrated_ along the rows, the unassailable black-and-white logic of cost and benefit in the squares themselves. The math was irrefutable: the one winning strategy was concealment. Only fools revealed their birthdays.

TODO The implied payoffs are a little odd. And concealment is highly inefficient from an information point of view: at most you gain information about one or two people.

#### On dropping _Family Guy_

The other day I saw a mention of _[Family Guy](!Wikipedia)_, and I remembered: I used to watch it all the time on Fox & Adult Swim, and liked it a fair bit. I still have several seasons' worth in my big DVD binder, so I could watch some anytime. But I haven't watched it in ~4 years. Why did I sour on it?

It's still available on TV, so that's not it (although the wretched _[American Dad](!Wikipedia)_ and _[The Cleveland Show](!Wikipedia)_ series seem to be on a lot; [Seth MacFarlane](!Wikipedia) does not know his limits). Nor is it that I now dislike animation; I watch as much anime as ever, and I enjoy _[The Simpsons](!Wikipedia)_ whenever I get the chance. And _The Simpsons_ highlights another reason which is not the reason: many _Family Guy_ episodes are awful, but that's equally true of _The Simpsons_, especially in the later seasons. We watch for the good episodes, and forgive the bad.

No, I think the reason is more that I became tired with _Family Guy_. Something in it tired me out. After some thought, I realized that the humor was the reason, and specifically the *kinds* of humor _FG_ used.

Yes, there's more than one kind of humor in _FG_ despite its reputation. One venerable classification of culture is into '[high culture](!Wikipedia)' and 'low culture'. The former requires education and knowledge, while the latter caters to the 'lowest' common denominator (or to put it nicely, is 'universal' or 'accessible').

The kind of humor _FG_ is known for is clearly 'low'. Jokes about bodily functions, sex, norm-breaking in general - these are without question low. _The Simpsons_ has, of course, low humor of its own: pretty much anytime Homer says "D'oh!" is an instance of low humor.

But then again, _The Simpsons_ also has 'high' humor - characters, guest appearances, allusions, background images, things one might not even realize are meant to be funny until one has already gotten the joke. In the classic episode "[Thirty Minutes Over Tokyo](!Wikipedia)" where they go to Japan, on the flight there Marge tells Homer to not pout about going to Japan rather than Jamaica because "You liked [Rashomon](!Wikipedia "Rashomon (film)")" to which Homer replies "That's not how *I* remember it." The first several times I saw this, I had no idea what the allusion meant until _[Rashomon](!Wikipedia)_ happened to be shown in school and I learned the plot revolved around differing retellings of a crime, at which point Homer's reply became funny. I suspect 99% of viewers have never seen _Rashomon_, and almost as many have no idea what the plot was or what the joke was, but for the last 1%, it's funny.

Does _FG_ have this "high" humor? Yes! Although here is the difficulty: although I know that there is high humor, *most of it I don't understand*. Whenever Stewie or Brian break into song or dance, I understand that probably some classic movie or Broadway musical is being alluded to & homaged, but I have no idea what. I don't even when I think I should: in one _FG_ episode, Brian finally becomes a writer for _[The New Yorker](!Wikipedia)_, a publication I have read sporadically for many years - nevertheless, most of the jokes go clean over my head.

Isn't that weird? I think of myself as a fairly knowledgeable guy, and I catch much of the high humor on _The Simpsons_ (I read a few episode guides from [The Simpsons Archive](http://www.snpp.com/episodeguide.html) identifying all the jokes and allusions, and had seen a good fraction of them). Which raises a question, at least for me: if I am missing all or most of the high jokes on _FG_, who exactly *are* they aimed at? Especially when the Adult Swim demographic skews younger (and more ignorant) than me?

But regardless of why the high jokes are incredibly high and elitist in a sense, I am still missing them. I like a little low humor, but I like best a mix of high and low (heavy on the high). If I am missing out on most of the high, then I am left with the dregs: the low humor, which after sustained exposure wore thin. And exhausted me.

And so I stopped watching.

#### _Pom Poko_'s glorification of group suicide

The first time I watched [Studio Ghibli](!Wikipedia)'s 1994 _[Pom Poko](!Wikipedia)_ has so far been the last: I found it a transparently thoroughgoing narrative about mass suicide in WWII, and as harrowing as _[Grave of the Fireflies](!Wikipedia)_ - but worse in a way because there is no condemnation of the mass suicides. Instead, we somewhat admire and sympathasize with them.

Alexandra Roedder wrote:

> I always saw _Pom Poko_ as being what it claimed to be: a story of the development of the [Tama](!Wikipedia "Tama, Tokyo") region of Tokyo, told with a strange kind of humor to offset (maybe?) or emphasize the sad facts of the development's impact on the environment. The mass sacrifice strikes me as being a *part* of the portrayal of the era, relying on the cultural memory (for lack of a better term) of [kamikaze](!Wikipedia) from WWII, not a commentary on it.

But WWII *also* brought the end of the traditional sort of Japanese development and increased Western-style development - like the city we see by the end, and things like the nightclub (or casino?) where they met the foxes.

The parallels are there, if you want to see them.

The [Tanuki](!Wikipedia) (the traditional Japanese) see their traditional way of life threatened by modern foreign-style development with apartments and powered construction vehicle (Western conquest & economic development), and fight back against the initially minor developments (embargo) and then escalate into full warfare (Pearl Harbor and the Pacific War), which ultimately leads to failure against the humans' superior tools (American materiel advantage), individual kamikaze missions, and mass suicidal attacks of multiple Tanuki in separate places, groups, and methods (Japanese use of suicide submarines, kamikaze planes, banzai charges), and finally, as despair and defeat set in, sheerly futile mass civilian suicide like scores of Tanuki setting off as part of the Buddhist cult-boat to the afterworld (the mass Okinawa suicides with the imperialist justifications - and remember that Buddhism was [heavily implicated](!Wikipedia "Zen at War") in this ideology too, it wasn't just Shintoism, having come to terms with the imperial government during the [Meiji restoration](!Wikipedia)).

That you see them as good-humored shows that the suicides are not condemned but if anything approved of them as noble and demonstrating their purity of heart. (I understand _[Gone with the Wind](!Wikipedia)_ is not without its own good humor to offset the sad story of the decline of the Old South.)

We all know of the conservative trends and [Japanese nationalism](!Wikipedia) which lingers in Japanese politics and manifests in such forms as: denying that any bad things like [war crimes](!Wikipedia "Japanese war crimes") happened during their pre-WWII expansion or during WWII itself with the general denial of culpability exemplified by the [comfort women](!Wikipedia); the [revisionism in textbooks](!Wikipedia "Japanese history textbook controversies") about such incidents (no doubt whipped up by those who hate Japan) like the [Rape of Nanking](!Wikipedia); the [war criminal shrine](!Wikipedia "Controversies surrounding Yasukuni Shrine"); and the martyr complex over the nuclear bombings as a perpetual club against the West.

The trend is not absent from anime. _Grave of the Fireflies_ is all about *Japanese* suffering, for example, with not the slightest sense that other countries were suffering even more.

I like Ghibli movies well enough, but my own particular focus is Gainax films and Hideaki Anno in particular. One of the striking aspects of the WWII material that Anno loves so dearly is that in his discussions, at hardly any point does he exhibit any sense of guilt or culpability or sense that the conquests and Pacific War might have been a bad thing for any other reason than Japan losing it and suffering the consequences. When the topic comes up, they say other things - from an _Atlantic_ interview:

> Anno understands the Japanese national attraction to characters like Rei as the product of a stunted imaginative landscape born of Japan's defeat in the Second World War. "Japan lost the war to the Americans", he explains, seeming interested in his own words for the first time during our interview. "Since that time, the education we received is not one that creates adults. Even for us, people in their 40s, and for the generation older than me, in their 50s and 60s, there's no reasonable model of what an adult should be like." The theory that Japan's defeat stripped the country of its independence and led to the creation of a nation of permanent children, weaklings forced to live under the protection of the American Big Daddy, is widely shared by artists and intellectuals in Japan. It is also a staple of popular cartoons, many of which feature a well-meaning government that turns out to be a facade concealing sinister and more powerful forces.

Further examples of this rhetoric and regret over losing the Pacific War can be found in [Takashi Murakami](!Wikipedia)'s long essay ["Earth in my Window"](/docs/eva/2005-murakami) or in [Sawaragi](/docs/eva/2005-sawaragi) (transcribed from [_Little Boy_](http://www.amazon.com/Little-Boy-Japans-Exploding-Subculture/dp/0300102852/), 2005)

One can't help but wonder - if the "Pacific War" led to peace since then, is that really so bad? I suspect I already know how the Chinese and Koreans regard this tragedy. (And why is it the "*Pacific* War", anyway? Japan was engaged in Asian land wars or occupation or subversion for decades before Pearl Harbor.)

I'll give another example for Anno. [Numbers-kun](http://forum.evageeks.org/viewtopic.php?p=434860#434860) translates part of an Anno interview with one of his favorite film-makers, [Kihachi Okamoto](!Wikipedia):

> Then some talk about Okamoto's Nikudan. Anno watched it twice and Okamoto said it's more than enough...Anno said he still remembered a lot of the scenes and how they are edited and linked. But the ones he watched most are _Japan's Longest Day_ [1968] and _[Okinawa Battle](!Wikipedia)_. He even played it as BGV [background video] when he was doing storyboarding at one time, and then slowly his attention was drawn to the video and ended up spending 3 hours watching it.

I have not been able to watch _The Battle of Okinawa_ yet, but [Animeigo's liner notes](http://www.animeigo.com/liner/out-print/battle-okinawa) do a good job indicating why it might be a tad controversial...

Okinawa came up in my _Evangelion_ research, incidentally, because Okinawa comes up in _[Gunbuster](!Wikipedia)_ as one of the (very subtle and easy for non-Japanese to miss) indications throughout that Japan has been restored to its rightful dominant place in the world[^Gunbuster], in _Evangelion_ there was a cut episode with a trip to Okinawa, and for _End of Evangelion_, OST commentary indicates that the victorious JSSDF shock forces (who cut down surrendering NERV personnel without mercy and burn them alive with flamethrowers) were intended to demonstrate man's viciousness and inhumanity. During the nonfictional battle of Okinawa, of course, the victorious troops using flamethrowers were American. Very few (non-Japanese) people ever notice the Okinawa references in _EoE_.

[^Gunbuster]: For an extended analysis of _Gunbuster_, see:

    - ["Imperialism, Translation, _Gunbuster_ (Introduction)"](http://animekritik.wordpress.com/2011/11/22/imperialism-translation-gunbuster-introduction/)
    - ["Imperialism, Translation, _Gunbuster_ (Episode One)"](http://animekritik.wordpress.com/2011/11/25/imperialism-translation-gunbuster-episode-one/)
    - ["Imperialism, Translation, _Gunbuster_ (Episode Two - NSFW)"](http://animekritik.wordpress.com/2011/11/27/imperialism-translation-gunbuster-episode-two-nsfw/)
    - ["Imperialism, Translation, _Gunbuster_ (Episode Three)"](http://animekritik.wordpress.com/2011/11/29/imperialism-translation-gunbuster-episode-three/)
    - ["Imperialism, Translation, _Gunbuster_ (Episode Four)"](http://animekritik.wordpress.com/2011/12/01/imperialism-translation-gunbuster-episode-four/)
    - ["Imperialism, Translation, _Gunbuster_ (Episode Five)"](http://animekritik.wordpress.com/2011/12/03/imperialism-translation-gunbuster-episode-five/)
    - ["Imperialism, Translation, _Gunbuster_ (Episode Six)"](http://animekritik.wordpress.com/2011/12/05/imperialism-translation-gunbuster-episode-six/)

> Takahata's films always seem to have that kind of "laugh because we can't do anything else" humor.

But [Isao Takahata](!Wikipedia) being the director is one of the signs: recall that one of his other films was... _Grave of the Fireflies_.

Tedne suggests:

> Someone claimed that it was a parable of the decline of the radical Left in Japan. I think it is a good parable of the decline and fall of indigenous communities. The Tanuki who kill themselves are simply trying to be true to themselves; same thing with their warfare. Under extreme threat people sometimes take extreme actions. There is no simple and compelling reason to either condemn or commend that.

It can be both; the radical Left - and Right, let's not forget [Yukio Mishima](!Wikipedia)^[speculated to have visually influenced _Evangelion_'s Gendo Ikari because of the gloves] - had certain classic positions. What was the Left most opposed to? The security treaties with America and the bases Okinawa. They were young, energetic, and wished to 'revolutionize the world' in service of an ideal, one might say - just like their noble kamikaze forebears. And likewise failed, for similar reasons.

Further, I disagree that the presented actions are normal. The pervasive suicide in _Pom Poko_ is not a universal. People *rarely* commit suicide: groups fighting to the last man or committing suicide are so rare that they command considerable attention when they happen deliberately. One can command considerable attention just by threatening to kill oneself, and self-immolation - both in the Middle East and Asia - are compelling protests. In practice, people surrender, adapt, and live on. (Unsurprisingly!)

The concept of [suicide-bombers and kamikazes](!Wikipedia "Suicide attack") live on because they are so unusual; lone assassins and fanatics may occasionally hazard certain death, but entire organized bodies of men? One has to look far for counterparts. (The Greeks at Thermopylae? But most of them survived. The concept of [forlorn hopes](!Wikipedia) at sieges? But they expected to win wealth & glory if they broke through, and certainly didn't carry [petards](!Wikipedia) with them even if that might've been an effective idea.) Far more common in history is soldiers deserting or mutinying at what they consider a [suicide mission](!Wikipedia). US observers, even knowing of the much-discussed suicidal strains in [bushido](!Wikipedia) like [seppuku](!Wikipedia), were still shocked by the course of the Pacific War; so it was that [Admiral Nimitz](!Wikipedia) could write to the [Naval War College](!Wikipedia):

> The war with Japan has been [en-acted] in the game room here by so many people and in so many different ways that nothing that happened during the war was a surprise - absolutely nothing except the Kamikaze tactics towards the end of the war; we had not visualized those.^[Quoted in [_The bomb and the computer: wargaming from ancient Chinese mapboard to atomic computer_](http://www.amazon.com/Bomb-Computer-Wargaming-Ancient-Mapboard/dp/B0006CZ8OA/), Wilson 1969]

Allegories can be difficult to understand the more remote and foreign they become: who can read Dante Alighieri's _Inferno_ and understand all the political or historical material without a scholarly apparatus? I think something similar is happening with _Pom Poko_. If we were to come up with a contemporary *Islamic* allegory, how would people react to it?

It's not that hard to come up with an isomorphic version which would make an average Westerner a little uneasy: a tribe of cheerful Arabian Djinni under the good King of Djinn discover that oil drilling is extending into the Empty Quarter which they have lived in for so long; they declare _jihad_ and attempt to fight back, using their magical powers, but while 1 or 2 rigs catch on fire after some male djinni magically blow themselves up and some other djinni can commandeer a truck to smash into the gate of an oil refinery, their efforts are generally futile. Dozens of djinni decide in their despair to permanently depart the world on a giant magic carpet bound for Paradise (where they hope for houris), while the rest wish together and perform one last spell in the urban streets of Riyadh: evoking the Golden Age of Baghdad and the _One Thousand And One Nights_ with the bronze giant and roc and princes of Serendip and enchanted women and divers other fantastical characters & objects. Exhausted, they abandon their smoky forms to masquerade as ordinary turban, bisht, or hijab wearing immigrant workers & expats in the Saudi government & oil industries, only periodically showing their true colors.

<!--
The story was also adopted into live-action films (the first of which won Best Screenplay at the Czech Film Festival in 1977) and two animated versions. For a whole generation, Japanese animators had avoided discussion of the war, instead allegorising it in space adventures or alien invasions. Barefoot Gen the anime rode a wave of change inspired by an exhibition in Tokyo about the life and famous diary of Anne Frank. An Anne Frank anime followed in 1979, establishing a new sub-genre within anime: war films about children, in which Japan's baby-boomers cast themselves as a blameless generation, forced to endure the consequences of their parents' martial past.

After the success of Barefoot Gen in 1983, other animated works appeared, many of them similarly autobiographical and child-centred. These ranged from Isao Takahata's masterpiece about the fire-bombing of Kobe, Grave of the Fireflies (1988), to less polished TV movies such as Toshio Hirata's Rail of the Star (1993), depicting a Japanese colonial family's desperate rush to reach American-occupied South Korea. Almost every major city in Japan seemed to gain a personalised film about the horrors of WW2, but many of Barefoot Gen's imitators used youthful protagonists to present the Japanese as innocent victims. This played well at home, but also into the midst of the 'textbook controversy', a long-running debate over the selective information imparted about WW2 to Japanese schoolchildren.
http://schoolgirlmilkycrisis.com/blog/2012/12/keiji-nakazawa-1939-2012/
-->

<!-- Ippei Kotora on the NAUSICAA ML

 It can theoretically, if the intention of the creator is as such. But that's not an effective way to propagate messages and thus rarely used; clearly that was not Takahata's intention in his films. There was an interview in 1988 by Animage asking what he wanted to express in Grave of the Firefly. Takahata talked about how children (Seita and Setsuko) try to create a pseudo family and such, but very little about the war. As I said before, the story just happened to take place in that time frame, nothing more, at least in the director's mind.

 ...Takahata is a member of Kyujyo no kai, a rather famous group of left-wing activists concerning the Japanese Constitution, especially the Article 9. He has expressed his political views through this group and others including Akahata, political newspaper for Japanese Communist Party. I doubt the articles have been translated. You can find some of the original articles here:

http://kenpo-9.net/document/041124_kouenroku.html
http://www.jicl.jp/hitokoto/backnumber/20050110.html
http://www.1101.com/ghibli/2004-08-06.html
-->

#### Alternate Futures: The Second English Restoration

The pricing of third-party candidates in political [prediction markets](Prediction markets) is a difficult exercise in pricing low-probability outcomes which may well include genuine [black swans](!Wikipedia). A case in point is the repeated pricing of libertarian/Republican [Ron Paul](!Wikipedia) for American president in Intrade, the Iowa Electronic Markets, & Bets of Bitcoin at a floor of ~1%; this pricing persists even long into the particular presidential campaign, well past the Democratic & Republican conventions, and up to Election Day. Part of this represents the inefficiencies of those markets, who make it difficult to profitably short contracts below 10% (leading to a "long-shot bias"), and due to Ron Paul fans who cannot face reality. But an unknown part of it is due to the observation that it is *possible* for a third-party candidate or a major-party dark horse to win and so the predictions should not be *exactly* 0%.

The American plurality election system (as opposed to some sort of proportional or probabilistic system) almost [forces a polarized system](!Wikipedia "Duverger's law") of [2 parties](!Wikipedia "Two-party system"), because any third party serves to 'split' the vote of the closer party (and be split) and hence there's strong incentive to somehow merge or for voters to force the merge by backing the stronger horse. So it's not surprising that we see no third-party candidates elected to offices higher than Representative or Senator after the Democrat/Republican system solidified in the late 1800s/early 1900s, and Teddy Roosevelt demonstrated Duverger's Law in practice with his 1912 [Progressive Party](!Wikipedia "Progressive Party (United States, 1912)") (and Ralph Nader in 2000). On the other hand, plurality voting only forces there to be 2 parties, not that they be 2 specific parties or that each party remain consistent - the Progressive Party's Teddy Roosevelt beat the Republican's William Taft 27% to 23%, and in the late 2000s we saw something close to a hostile takeover or schism in the Republican party by the [Tea Party](!Wikipedia "Tea Party movement") (note the name), which while the Tea Party didn't *entirely* succeed, it still had a dramatic impact on the composition and planning of the main Republican party. This is 2 'near-misses' in just 1 century with ~25 presidential elections.

Would you be willing to bet \$1,000 to my \$10 that from 2016-2116, every single President will be a Democrat or a Republican I wouldn't! If we used Laplace's [rule of succession](!Wikipedia), we'd estimate $\frac{0+1}{25+2} = 3.7%, and actually, I would be uneasy at any [prediction under 5%](http://predictionbook.com/predictions/8404)!

How would this 5% actually work out? There could be a split in one of them and the new party steal all the old think-tanks, voter lists, incumbents, and the whole laundry list of resources which power the giant parties to their assured victories; that's one route. Or... there could be a [*convention fight*](!Wikipedia "Brokered convention"). Conventions have an odd vestigial function in presidential elections: technically, the entire apparatus of caucuses and primaries doesn't 100% determine who the delegates vote for at the convention! It's understood - of course! who could possibly think otherwise - that the delegates, even when not legally bound to vote for the person who won the most votes, will do so. 'Understood', which is another word for 'they could do otherwise'. But delegates used to frequently changes who they'd vote for, throughout the 1800s for both parties. Why can't this happen again? No real reason. There's a gap between the formal powers of the convention and how everyone expects the convention to go, but such gaps are ripe for rare events to exploit. (A program might have a security vulnerability which requires [14 different bugs](http://blog.chromium.org/2012/06/tale-of-two-pwnies-part-2.html) to exploit, which could never happen in practice just from random click or writing, until along comes one motivated hacker.) A similar thing is true of the [Electoral College](!Wikipedia "Electoral College (United States)"); it was not *intended* by the Founding Fathers to be a mechanical rubberstamp of voting totals, since if they had intended a direct election they would have simply wrote the Constitution that way, but to allow the electors to make their own choices. Here too we all expect them to be rubberstamps... but the formal powers are still there.

The United States is far from alone in having some curious gaps between _de facto_ and _de jure_ powers. Every constitutional monarchy exemplifies this - and open up their own low-probability events. Constitutions sometimes have loopholes like 'emergency powers' which are prudent precautions and of course would never be abused, until they are. (Who in 1870, seeing the emerging German economic & military giant under the leadership of the Kaiser and the realpolitik genius Otto von Bismarck, could have guessed that within 80 years the Kaiser would be a bad memory and a failed artist would have risen on mass approval to seize, [quite legally](!Wikipedia "Enabling Act of 1933") and with surprisingly little opposition, all power to the utter ruin of the country?)

England is an interesting example: the monarchy is a funny little thing for the tourists and tabloids, but suppose a driven strategic genius like [Frederick the Great](!Wikipedia) were crown prince and the Queen died tomorrow; do you really think that there would still be a <1% chance that in 50 years when he dies, England won't be something like Singapore writ large The Royal Family is completely feckless and embarrassing (perhaps *because* they have no purpose but useless, or to be polite, 'ceremonial' duties), but they possess a power-base that ordinary politicians would kill for: annual income in the dozens of millions, world-wide fame, the unthinking adoration of a still-significant chunk of the British masses, well-attended [bully](!Wikipedia "Speech from the throne#Commonwealth realms") [pulpits](!Wikipedia "Royal Christmas Message"), and in general enough tradition & age & properties sufficient to beat down and render groveling the staunchest democrat.

An Englishman would tell you that any attempt by a monarch to meddle in affairs would - of course! who could possibly think otherwise - be slapped down by the *real* government and any _de jure_ laws employed would be quickly repealed by Parliament. After all, their "[uncodified constitution](!Wikipedia)" is believed to say as much. (But who exactly carries out the orders of a constitution which doesn't even have a physical embodiment) But on the other hand, [reserve powers](!Wikipedia "Reserve power#Commonwealth realms") still exist in the Commonwealth and are exercised from time to time.

Maybe we can rule out a simple coup scenario. But a more subtle strategy carried out over decades? An [Outside View](http://wiki.lesswrong.com/wiki/Outside_view) doesn't help too much in assessing such strategies. We certainly can point to existing monarchies with tremendous power and wealth who rule through a democratic framework: pre-WWII Japan saw considerable influence by the Emperor through the nominally democratic government, the [monarchy of Thailand](!Wikipedia) is widely believed - outside the reach of Thai censorship - to exert considerable control over Thai politics, and some countries like Saudi Arabia don't have even that democratic framework. [~45 monarchies](!Wikipedia "Monarchy#Current monarchies") exist, of varying degrees of symbolism; just one country with powerful royalty would give us ~4% rate of predicting a powerful royalty in a country given the data that the country is also a monarchy, but we already know England is a weak symbolic monarchy. We are more interested in the change the English monarchy will cease to be symbolic in the next century. Is it more, equally, or less probably than a third-party winning? (We can think of the monarchy as an inactive third-party in the English political system.) In the absence of known attempts, it's really hard to calculate - we can calculate that if there's 1 success in 100 'attempts', that gives us a point-estimate of $\frac{1+1}{100+2} = 1.96%$ but if we ask instead the 95% [binomial proportion confidence interval](!Wikipedia) of 1 success in 100 trials, we get 0-3%! Any big bets on it being 1% seem like a bad idea when it could easily be 0.1% or 3% instead... (This is not a surprise if you think about it a little: how could you be precise to *as much or more* than a single percentage when you only have 100 pieces of data? To narrow it down to a specific percentage will take more than that!)

Statistics aside, we can ask a different question: are there multiple independent disjunctive paths to power (increasing the odds of it happening), or just one unlikely conjunctive path consisting of multiple necessary steps? What might a path to power look like? And specifically, one exploiting the formal gaps in power? Monarchies have been rising and falling throughout history, so it stands to reason that some managed to claw their way back from irrelevancy (the [Meiji Restoration](!Wikipedia) providing a well-documented example with far-reaching consequences).

Formally, the English monarchy doesn't seem to directly command either the police or military, and is under the Parliament which apparently can legally do pretty much anything it wants. So Parliament will figure in plans: Parliament must be co-opted, made to delegate powers, or simply neutralized. Since the constitution is unwritten, sufficient popularity would enable the monarch to do anything or at least shift the [Overton window](!Wikipedia) to its desired policies.

An example of a strategy for neutralizing Parliament: a young crown prince is gifted with a copy of [Edward Luttwak](!Wikipedia)'s _[Coup d'Etat: A Practical Handbook](!Wikipedia)_; he enters the military (as is usual for the royal family) and begins building a power-base or "[deep state](!Wikipedia)" using his good looks, hard work, heritage, and also his inherited wealth (helping out impoverished retired officers, sponsoring parties, etc.). He leaves the military to go into politics, gradually easing his way in (pushing the Overton window to make this acceptable); during a major crisis - perhaps a second Great Depression? - which highlights the fecklessness of the civilian government, his cabal of young turks stages a lightning bloodless coup to restore the legitimate monarch to _de facto_ control over the civilian government, and who immediately calls for the Parliamentary elections which the existing Parliament had been delaying since it was fearful of voter anger, fears which immediately prove justified as the new king's favored candidates sweep in. The king now controls the military, is legitimized by a popular aegis, and has a compliant Parliament to enact his new deal. The public can be counted on to remain passive and accept the changes in the Overton window, just as the American public could be counted on post-9/11 to acquiesce to anything.

(Yes, we are postulating a remarkable crown price here: it is rare for someone to be handsome *and* intelligent *and* driven by a nigh-sociopathic lust for power *and* extroverted or charming; however, a century is multiple generations and our story only requires 1 such person. His low probability is just evidence that the current English royal family is self-sabotaging its prospects - by indulging in the [demographic transition](!Wikipedia) and having so few kids! When you need a win from the genetic lottery, you cannot afford to buy few tickets. If nothing else, the spare heirs can make themselves useful by gathering power-bases in various business industries or government agencies; they'll almost have to, given the limited royal funds. The other steps in this scenario, while all less than likely, do not seem extremely unlikely.)

We can think of an even more interesting strategy! Consider the very long perspective: way back in 1066 when [William the Conqueror](!Wikipedia) [conquered England](!Wikipedia "Norman conquest of England"), he technically owned the whole place as spoils of war. Where did it all go? Well, most of it went to his supporters as their reward, sooner or later. And we can't appeal to the formal/informal gap and have the monarchy repossess it because the sales usually included clauses about the sales being permanent or perpetual, which are hard to escape. But actually, billions is left! Why isn't the Queen a billionaire, really? Because it's all *controlled by Parliament* in a strange agreement dating back to 1760 in which the monarchy gets a sort of pension called the [Civil List](!Wikipedia) for paying the bills of the [Royal Households of the United Kingdom](!Wikipedia) which runs to ~$10 million annually, and in exchange Parliament controls the entirety of the [Crown Estate](!Wikipedia) - worth a cool ~\$11 billion and yielding ~\$*300* million annually. It's clear that Parliament has the better end of this deal, and also clear that our hypothetical prince won't be running much of a campaign based on the gleanings from his politically-vulnerable income of \$10 million.

The formal/informal gap may help here. This agreement turns out to have been modified since 1760 at the start of each new reign, because the new monarch has to agree to the arrangement! It's understood that he or she will immediately agree - of course! who could possibly think otherwise - but here is a chink. Control over a fortune of \$11b goes a very long way towards building a genuine power-base.

The question of the Crown Estate and the deal's stability has been discussed from time to time; the longest discussion I've seen is a 1901 essay by G. Percival Best on ["The Civil List and the Hereditary Revenues of the Crown"](http://www.andywightman.com/docs/civil_list_crown_1901.pdf). He mentions many interesting details, for example provisions in the relevant laws which trigger only if the monarchy decides to *not* surrender the Crown Estate income: eg.

> 1. The Hereditary Excise Duties: These were granted to the Crown in 1660 by the Acts 12 Car II c 24 in lieu of the feudal rights then abolished. Various re-arrangements were made from time to time, whereby some of the duties ceased to be payable. The remaining duties, being duties on ale, beer, and cider brewed in Great Britain, are in abeyance but will revive in the event of the Crown at any future time not making the usual surrender...
>
> 3. Compensation for Wine Licence Revenue: The revenue from wine licences ceased to form part of the Hereditary Revenues in 1757, when by the Act 30 Geo II c 13 the annual sum of 7,002 14s 3d was granted to the Crown in lieu thereof. This will be payable to the Crown in the event of any resumption of the Hereditary Revenues.

Best confirms my suspicions that between the deal and the provisions in law for the deal lapsing, the only real barrier to a new monarch is that great bugaboo, "custom" or the "unwritten constitution" or "public opinion"[^Shaw]:

[^Shaw]: Which I regard as quite nugatory, much like the Englishman Shaw in _[Back to Methuselah](!Wikipedia)_:

    > The lieutenants of God are not always persons: some of them are legal and parliamentary fictions. One of them is Public Opinion. The pre-Darwinian statesmen and publicists were not restrained directly by God; but they restrained themselves by setting up an image of a Public Opinion which would not tolerate any attempt to tamper with British liberties. Their favorite way of putting it was that any Government which proposed such and such an infringement of such and such a British liberty would be hurled from office in a week. This was not true: there was no such public opinion, no limit to what the British people would put up with in the abstract, and no hardship short of immediate and sudden starvation that it would not and did not put up with in the concrete. But this very helplessness of the people had forced their rulers to pretend that they were not helpless, and that the certainty of a sturdy and unconquerable popular resistance forbade any trifling with Magna Carta or the Petition of Rights or the authority of parliament. Now the reality behind this fiction was the divine sense that liberty is a need vital to human growth. Accordingly, though it was difficult enough to effect a political reform, yet, once parliament had passed it, its wildest opponent had no hope that the Government would cancel it, or shelve it, or be bought off from executing it. From Walpole to Campbell-Bannerman there was no Prime Minister to whom such renagueing or trafficking would ever have occurred, though there were plenty who employed corruption unsparingly to procure the votes of members of parliament for their policy.

> That His present Majesty had a legal right to resume possession of these Hereditary Revenues is clear from the provisions of the [Civil List Act, 1837](!Wikipedia), but whether he could constitutionally have done so is open to question. It has been said that "the arrangements by which the Crown at the beginning of each reign surrenders its life interest in the Crown lands and other Hereditary Revenues, though apparently made afresh on each demise of the Crown, is really an integral part of the Constitution and could not be abandoned."^2^ This view was shared by [Spencer Walpole](!Wikipedia), who, writing with reference to the surrender of the casual revenues by [William IV](!Wikipedia "William IV of the United Kingdom"), stated that "a surrender of this kind once made was virtually irrevocable. It would have been as impossible for any future Sovereign to have resumed a revenue which his predecessors had surrendered as it would have been impracticable for him to have restored the [Star Chamber](!Wikipedia), or to have made the appointment of the Judges dependent on his pleasure."^3^ The late Professor Freeman's words on the point are equally emphatic. After discussing the rights of the Crown and of the public over the Crown lands he continued, "A custom as strong as law now requires that at the beginning of each fresh reign the Sovereign shall, not by an act of bounty but by an act of justice, restore to the nation the land which the nation lost so long ago."^4^
>
> ...If, therefore, the King exercised his legal right and resumed possession be would only be entitled to retain a sum sufficient for the support of his household and family in a state befitting the Royal dignity. The remaining produce would have to be devoted to the public service. As in the last resort it would be for Parliament to say what sum the King should retain, the advantage of a resumption instead of a surrender is problematical.

Note that this alone could still be very useful for our would-be Frederick the Great - since this seems to imply that in a resumption, the monarchy will gain complete control of how it spends its allowance, and more importantly, how any properties in the Crown Estate are disposed of or contracted about.

With these legalities in mind, we can imagine a new scenario: The old monarch dies, and the crown prince succeeds. He declines to surrender, whereupon if Parliament strikes back by insisting the state budget must now be maintained by his Crown Estate (which is of course these days grossly inadequate), he beseeches Parliament to authorize the usual taxes to close the gap in his funding... This puts them in a fascinating dilemma: if they refuse, he carries on the most limited core functions and abandons everything else, causing people and especially those dependent on state subsidies to hate Parliament and sweep monarchists in during the next election which he of course has called; while if they agree, he now has full power of the purse and can begin building up his power base with wise administration to withstand the future attacks of Parliament.

Legislatures are rarely known for their courage and for being willing to hazard enormous upheaval,  but there doesn't *have* to be too insane upheaval - that's the threat to Parliament: "sure, you can take my bet if you think I'm bluffing and then pass appropriate laws later, but do you want to?"

We can analogize this brinksmanship to American [government shutdowns](!Wikipedia): who will the voters blame for being unreasonable and inflicting the pain & suffering? The [1995 shutdown](!Wikipedia "United States federal government shutdown of 1995 and 1996") was widely interpreted as a victory for the Democrats and a defeat for Republican architect [Newt Gingrich](!Wikipedia) (who of course has argued that this interpretation is wrong and it was actually a victory[^victory] despite Clinton's [boosted approval rating](!Wikipedia "United States federal government shutdown of 1995 and 1996#Result")).

[^victory]: [Wikipedia](!Wikipedia "Newt Gingrich#Government shutdown"):

    > Discussing the impact of the government shutdown on the Republican Party, Gingrich later commented that, "Everybody in Washington thinks that was a big mistake. They're exactly wrong. There had been no reelected Republican majority since 1928. Part of the reason we got reelected ... is our base thought we were serious. And they thought we were serious because when it came to a show-down, we didn't flinch."[71] In a 2011 op-ed in _The Washington Post_, Gingrich said that the government shutdown led to the balanced-budget deal in 1997 and the first four consecutive balanced budgets since the 1920s, as well as the first re-election of a Republican majority since 1928.[72]
How closely we can analogize American government shutdowns to an English resumption of the Crown Estate is an interesting question. If Congress fails to pass a budget, the Treasury default procedure is to stop cutting checks to everyone who must then make do with their existing budgets or income (eg. the postal service can continue running on its fees), which inflicts tremendous pain as so much of the economy and population is linked to the government. But would Parliament really insist on the monarch resuming paying for government? If it does, would the bureaucracy really try and fail to pay for things with the Crown Estate revenue? What happens next? The simplest outcome may just be that it becomes a _fait accompli_. As large a sum as it is, \$200m must be put into context: it is not going to single-handedly break the English budget or substantially exacerbate its structural problems.

Another interesting example of the Overton window and the frailty of 'custom', besides the obvious point that the Founding Fathers would not recognize the current giant federal government or understand how their carefully-written Constitution could have permitted such a thing (whatever good reasons underlie the growth), is the explosion of the [filibuster](!Wikipedia "Filibuster in the United States Senate") from a legislative judo move which was understood to be a key tool of the minority (and whose removal by the majority would be the "[nuclear option](!Wikipedia)") whose invocation was personally taxing (internalizing the costs, eg. _[Mr Smith Goes to Washington](!Wikipedia)_) and used only in rare circumstances (like the English reserve powers! How about that). So much for the camaraderie of the Senate and centuries-old custom.

#### Possible Amazon Mechanical Turk surveys/experiments

- [Sunk cost](): see whether manipulation of learning affects willingness to endorse sunk costs
- backfire effect idea: manipulation of argument selection affects backfire effect?
- followup SDr's lipreading survey, unexpected and contrary to my theory
- can one manipulate the subadditivity effect in both directions [for cryonics](http://lesswrong.com/lw/fz9/more_cryonics_probability_estimates/85gm)? In one version, enumerate all the ways things can go right and in another all the ways it can go wrong.
- test [my theory](https://plus.google.com/106597887376283858570/posts/XFEApmFEvMP)

[Ken Liu](!Wikipedia)'s ["The Paper Menagerie"](http://io9.com/5958919/read-ken-lius-amazing-story-that-swept-the-hugo-nebula-and-world-fantasy-awards) is the most critically acclaimed Fantasy short story in history, to judge by its simultaneously winning the short story category for the 2011 [World Fantasy Award](!Wikipedia) & [Hugo Award](!Wikipedia) & [Nebula Award](!Wikipedia)s (narrowly missing the [Locus Award](!Wikipedia)), a sweep which had never happened since the youngest award was started in 1975 - 35 years before.

<!-- https://en.wikipedia.org/wiki/Araki_Yasusada
http://www.newyorker.com/books/page-turner/when-white-poets-pretend-to-be-asian -->

Presumably this means that the story is, if not the best fantasy story ever, at least an extremely good story and by far the best of 2011. So I read it eagerly with high expectations, which were immediately dash. The story is not that good. The prose is OK: not nearly as wooden as, say, Isaac Asimov, but not as spare & finely-honed as Ted Chiang's, deliriously excessive as R.A. Lafferty, extraordinarily smooth and literary as Gene Wolfe, mannered as John Crowley, dream-like as Neil Gaiman... The plot itself is sentimental. In fact, as I read it, words kept rising to consciousness that should never be associated with a winner of any of those awards much less all three simultaneously, words like "trite" and "maudlin". With a skeptical eye, the story crumbles even more into a pitiful sort of self-indulgent narcissism, in which a character angsts over small issues which seem large only because they live a life so blessed that they have never known real hardship; with even a little bit of perspective, their complaints become almost incomprehensible, and what was meant to be moving becomes absurd. Part of my objection is a lurking sense that Orientalism and/or "diversity" promotion lies behind the triple crown.

One of the difficulties in attributing people's evaluations of something to essentially tribal or ideological motives is that typically it is hard to rerun or vary the scenario to control for the key aspect; for example, if we wondered how much the Barack Obama's presidential election owed to racial politics (rather than other factors that have been mentioned, such as John McCain's uninspiring campaign or choice of Sarah Palin, Obama's slick staff, the well-timed meltdown of the American economy etc), we are left to parse tea leaves and speculate because there is no way we can re-run the election using a Barack Obama who chose to identify as white rather than black, or an Obama who was simply white, and we cannot even run polls on a hypothetical alternative Obama with the same biography as a junior senator from Illinois with no signature accomplishments because the parallels would be obvious to too many Americans one might poll.

If we were to hypothetically vary Liu's story, we would want to replace the main character with an equivalent character whose non-Anglophone nationality was involved in WWII, resulted in many refugees and women from that country returning as wives to America, who might know a beautiful paper-working art suited for depicting tigers, and who was mocked on ethnic or racial grounds. Remarkably, this turns out to be easily doable on all points: Liu's story could easily turned into a story about a half-German boy in America mocked for being a filthy Nazi whose mother came from the German post-WWII wasteland and who spoke mostly German while making [Scherenschnitte](http://en.wikipedia.org/wiki/Scherenschnitte) for her son who later spurned the paper cutout animals and even later realizes the cutouts formed German words (in [Fraktur](!Wikipedia), which is a pretty hard-to-read family of fonts) with the same ending. The question is, if we take Liu's story, rename the author "Ken Schmitt" or "Ken Hess" or "Ken Brandt" or "Ken Schmidt" perhaps (making sure to pick a surname as clearly German as Liu is Asian, and ideally a single-syllable as well to control for issues related to memorability or length), make the minimal edits necessary to convert it to the above version - do you think this hypothetical "The Scherenschnitte Menagerie" would've won even 1 award, much less 3?

It seems highly unlikely to me, but unlike with Obama, we can produce the variant version without trouble, and in any survey, we can count on very few SF/F reader-respondents having actually read "The Paper Menagerie" (short stories are generally published in specialty magazines, whose circulations have declined precipitously over the past decades, and rarely ever achieve the popularity of the top SF/F novels). If we surveyed a sample of SF/F readers and saw a preference for the original Liu version (especially if the preference were moderated by some measure of liberalism), then any non-ideological explanation must explain how the original version using Asia is so enormously esthetically superior to an isomorphic version using European-specific details.

With all this in mind, it seems like it should be easy to design a survey. Take the two versions of the story with the two different author names, ask the respondent to rate their randomly-chosen story 1-5 (Likert scale), ask how much SF/F they consume, their general politics, a question asking whether they had heard of the short story before (this could be tricky), and some additional demographic information like age, ethnicity, and country. For extra points, one randomize whether a short biography of the author appears after each story, to see if there is "who? whom?" reasoning at work where knowing that Liu is from China increases the positiveness of ratings but knowing that "Schmitt" is from Germany does not affect or reduces ratings.

#### Surprising Turing-complete languages

Split out to [Turing-complete](/Turing-complete).

#### Cicadas

> 5 Words Or Less Summary: "Got some. Cicadas are crunchy."

In April 2013, I was excited to read [local paper's article](http://www.gazette.net/article/20130503/NEWS/130509471/1103/Missing-Waldorf-teen-14-returns-home "Cicadas soon to emerge, Southern Maryland experts say: Bugs to begin short sojourns above ground, only to mate and die") on a cicada emergence this year; the print version included a detailed Maryland map apparently sourced from [`cicadas.info`](http://cicadas.info/) with point-estimates of emergences - it was hard to see my particular hamlet, but I was clearly near more than a few. I had had no idea that there were any cicadas in the area or that this was the year. A 17-year brood, so I resolved to make the most of this opportunity - and eat some cicadas!

(Yes, they're perfectly safe to eat as long as you aren't stupid and forget to cook it or try to eat a rotting dead one. People eat insects all the time, and weirder things like [insect barf](!Wikipedia "Honey").)

I eagerly tracked a website for [Virginian daily ground temperature readings](http://cicadatracker.sutron.com/cicada/tw/) throughout April 2013, and was frustrated by the incredibly slow rise and occasional reverses that set back progress by weeks. Finally - the line was crossed! I woke up early to search for cicadas (I had read they tended to be most active early in the morning), only to find none at all. Turns out that cicada groups are *very* localized, and indeed, none emerged in my area. The closest I came was in late June, when I thought spotted a single severed cicada wing on the road, but I was not sure.

I could just go elsewhere, since it's not as if there was any shortage of cicadas in places that had them. But I had to wait until early June due to interference like my sister visiting and trying to piggyback a harvesting expedition on my jury duty (which was fantastically ill-timed in overlapping with both catching cicadas and driving my sister from & to BWI). Waiting was very frustrating because I would read articles in places like the [NT](https://www.nytimes.com/2013/07/01/nyregion/during-cicadas-swan-song-many-wonder-if-they-missed-the-show.html) of fully-active emergences which were finishing, and know that just up the road were cicadas if only I could reach them. Finally, I managed to get to a local park by Leonardtown where `Magicicada.org`'s live ["2013 _Magicicada_ Brood II Records"](http://magicicada.org/databases/magicicada/map.html) collaborative Google Map indicated that cicadas had been spotted.

We got there to find that most of the cicadas were dead and shells. The overall sound was remarkable: like being on the shoulder of a freeway in the middle of the day.

Finding cicadas a little challenging, but the red eyes helped a lot - very striking against a green backdrop. Capturing was both easy and difficult. I had problems with my own personal squeamishness in not being willing to pinch a cicada with enough force to avoid dropping it or it flying away. some stupid enough to just shift branch and wait for me to try again. funny response on being seized: they switched to a steady buzz which sounded quite unhappy until I dropped them into my ziplock plastic bag. Collected 20-30 or ~75g. Sitting on my windowsill, they churned around in their bag making a slight buzzing noise and crawling over each other:

![My cicadas, fresh from the park's trees and bushes](/images/2013-cicadas/bag.jpg)

I carried the bag around to the animals; the dog didn't seem interested, and the cat just stared even when I gave it a cicada to play with. My sister was napping and gratified me with a shriek. No one seemed remotely interested in having them for dinner, and the most I could extract was a promise that they might try cicada chip cookies if I made them. Well, bugger that for a lark - I wanted a right proper meal off them after busting my hump to secure them. I had been hoping for enough cicadas that I could make multiple recipes, but I had to settle for making just one big batch.

On pg8 of [_Cicada-licious: Cooking and Enjoying Periodical Cicadas_](http://www.tullabs.com/cicadaworld/cicadarecipes.pdf) (Jenna Jadin & the University of Maryland Cicadamaniacs, 2004), I hit a likely-sounding recipes:

> *The Simple Cicada*: Don't want to bother cooking up something fancy just to enjoy the delicious taste of the cicada?? Well here is a quick and easy main dish recipe that should take only minutes to prepare:
>
> *Ingredients*:
>
> - 2 cups blanched cicadas
> - Butter to saut
> - Two cloves crushed garlic
> - 2 tbsp finely chopped fresh basil, or to taste
> - Your favorite pasta
>
> *Directions*:
>
> 1. Melt butter in saut pan over medium heat.
> 2. Add garlic and saut for 30 seconds.
> 3. Add basil and cicadas and continue cooking, turning down the heat if necessary, for 5 minutes or until the cicadas begin to look crispy and the basil is wilted.
> 4. Toss with pasta and olive oil. Sprinkle with Parmesan cheese if desired.
>
> *Yield*: 4 servings

I had all those ingredients except for Parmesan. The cicadas being sauted in butter:

![A saucepan of butter, spare bacon grease, and 75g of cicadas.](/images/2013-cicadas/saucepan.jpg)

~10 minutes later (I had to reheat the pasta, which I made first), I had my final product:

![A pasta and cicada and tomato sauce dinner](/images/2013-cicadas/dinner.jpg)

How was it? Well, it would've been better with more sauce. The cicadas themselves? They had an odd consistency - they were crispy and hollow, like a [cheese puff](!Wikipedia), and tasted sort of like toasted peanuts, but mostly just like sauted butter. The main problem: the wings and legs were also crisped, so every so often as they went down the hatch, there would be a sort of scratching sensation. Not terribly pleasant. I regretted thinking they would break off or burn away, and ignoring the cookbook's advice to remove them:

> Adult males have very hollow abdomens and will not be much of a mouthful, but the females are filled with fat. Just be sure to remove all the hard parts, such as wings and legs before you use the adults. These parts will not harm you, but they are also not very tasty.

I have no one to blame but myself there. (Given how late I went hunting and many of my cicadas were crispy/hollow, I suspect I caught mostly males who had failed to mate.) I also somewhat miscalculated portions, and wound up stuffed to the gills with cicada & pasta.

Overall, an interesting experience. The next cicada emergence I am near, I'll try the chocolate chip cicada cookie recipes.

#### Epigrams

#### _Cherchez Le Chien_

In the anime _[Azumanga Daioh](!Wikipedia)_, a key bit of characterization for schoolgirl Chiyo Mihama comes when her friends visit her house and are awestruck that it is enormous, has ample yardage & greenery.
The final proof of her family's wealth is when out of the mansion comes bounding an enormous friendly white [Great Pyrenees](!Wikipedia) dog named Mr Tadakichi.
Mr Tadakichi emphasizes the space available to the Mihama family (someone living in a 6-tatami apartment does not have room for a large dog nor permission from their landlord) and their ability to care for a foreign breed of dog (it eats a lot and must be regularly walked).
Further, the dog breed is French, and France has strong connotations of wealth & elegance (see also [Paris syndrome](!Wikipedia)).

Then I began to notice that in anime, cats were far more common than dogs, and I noticed that in anime/manga set in contemporary Japan, dogs seemed to be associated almost exclusively with either rural settings or with people implied to be middle to upper-class.

It seems tome that the use of Mr Tadakichi was not accidental: no other character in _AD_ has a dog, and this makes sense when one considers the space & permission issues - Chiyo is the only rich character, and so of course she's the one who has a family dog, who has a summer house by the ocean, who is going to study overseas rather than endure the hell of college entrance exams, etc.
The other parts are more obvious than dog-ownership, though, and I might have noticed a peculiarity of _AD_: perhaps it's only that one manga/anime where dogs are signifiers of wealth.
What about all the other anime?

TODO: methodology?
Claim is dog-owning characters will be more likely to be wealthy than other characters.
This is not within-series (imagine a series like _Maria-sama_ where every girl is either rich or middle-class; if they all had dogs, that would clearly support the hypothesis even though there would be no correlation with their variants) but across series.
Collect a random sample of normal characters, then collect a full list of dog-owning characters, compare the log regression where owning a dog = success?
Can I use the same sources as in [hafu]() - TvTropes, AniDB, WP, MAL, Baka-Updates Manga, Google/Scholar?

#### Tradeoffs and costly signaling in appearances

At some point I confused two [tsundere](!Wikipedia) anime characters: they both had brown [long hair](!Wikipedia) and flat chests and I messed up a comment.
This pairing of long hair and flat chests seems to be common for tsundere character designs (eg 4 of 5 'notable examples on [Know Your Meme](http://knowyourmeme.com/memes/tsundere) with the exception being from an unpopular and fairly obscure anime; indeed, as [TvTropes](http://tvtropes.org/pmwiki/pmwiki.php/Tsundere/Anime)'s list indicates, almost all of the Rie Kugimiya-voiced characters have this pair of traits).

When I realized my mistake, I noticed that their counterpart female characters both had short brown/black hair and ample bosoms.
This inverse relationship struck me as a little odd because the counterparts only need one distinguishing feature, as many characters get by with, so why did they have the opposite length of hair *and* opposite cleavage settings?
And the more I thought about it, the more it seemed like this pattern held true in real-life too: I might see women with long hair, or with cleavage, but rarely with both.

Of the 2x2 table of short/long hair and large/small breasts, the counterpart can't have the original long/small combination because that would be confusing; and if the counterpart had short/small, that would unavoidably cast them as either 'child-like' or more masculine which is often inappropriate, so the short/small combination would be avoided; but that still leaves long/large as an option, which changes only one aspect.

But then I remembered: what is the stereotypical haircut of a new mother in both Japan and America - isn't it to cut the hair very short, often less than shoulder-length?
And isn't long hair in many cultures associated with young women, and *unwed* young women in particular, and considered positive?
("for a woman, if her hair is abundant, it is a glory to her"; "hair is the richest ornament of women"; "they say that the hair is everything, you know"; "the bald woman boasts of her sister's hair
"; "if you meet a red-haired woman, you'll meet a crowd"; "when the month of May arrives, women's hair grows and penises become strong"; "one hair of a woman draws more than a bell-rope" / "one hair of a maiden's head pulls harder than ten yoke of oxen" / "one hair from the head of a woman pulls more than a ship's hauser" / "beauty draws with a single hair";  - but remember men, as attractive as blonde hair may be, "falseness often lurks beneath fair hair" / "often a troll-woman is under fair skin, and virtue under dark hair" and remember women, "short hair is soon brushed"!)
And as pointed out by evo-psych theorizers, long hair may be sexually attractive as a reliable signal of health; if the hair is blonde, then (in pre-hair-dye eras) it'd be a reliable signal of youth too.

If long hair really is an attractive asset for a woman (and I think a lot of men would agree that barbigerous factors matter, if not as much as breasts or buttocks), then one might wonder why marriage is accompanied by hair-cutting.
After all, why deliberately make yourself less attractive?
Surely it's nice to be beautiful and admired even if you've already found a husband.
Plus, everyone grows hair so it seems like a relatively egalitarian aspect of attractiveness - it's not set in stone like so much of one's appearance.

The answer may be that long hair is not just a reliable signal, but a *costly* one: the longer hair is, the harder it is to take care of it.
One has to use up more shampoo & conditioner cleaning the full mass, rendering showers a complicated affair; the weight of long hair is a literal burden; brushing the hair may take a long time; one has to keep an eye out to avoid getting the hair in one's eyes, caught in anything around you, avoid knocking things over with one's tresses, keep it clean, or avoid stepping on it in the most extreme cases.

I don't know how much time & effort is involved in maintaining, say, hip-length rather than shoulder-length hair, but it must be considerable to explain why hip-length hair is so unusual even before marriage.

This doesn't explain the apparent inverse relationship where one has either breasts or hair.
Hair may be costly, and so women shed it at the first opportunity, but why isn't long hair universal before marriage?
I think this may be explained by the optionality of hair: one cannot choose the size of one's breast without resorting to desperate measures like surgery, one cannot change one's eye colors without unpleasant measures like colored contact lenses, one cannot change the shape of one's face, losing weight & being fit is a lifelong battle - but hair is optional.
So suppose one lucks out and has a curvaceous cleavage men drool over; perhaps that is sufficient and you don't want to go to the extra costs of long hair, and so you never grow flowing locks and this works for you.
But suppose one instead has a chest like a cutting board, what can you do about that?
Not much... but you *could* compensate by instead growing long hair, so wouldn't you?
It's better than being both short-haired *and* flat-chested.
(There will be exceptions of course; a supermodel might be both busty & hairy because their job makes it worth their while, some women may simply like long hair a lot and want to have long hair regardless, and one may be cursed with bad hair & growing is never worthwhile.)

In general, if a factor of attractiveness is optional and costly, we'd expect people blessed with more non-optional factors to avoid the optional costly ones, and that the optional costly factors will vary with perceived prospects or need for attractiveness (eg we'd expect sharp decreases in hair length after marriage, and gradual decreases with age).

To test this:

- compile a set of tsundere female characters, a random selection of non-tsundere characters, and classify each by hair length & breast size, and see if there is an inverse relationship in both groups or whether it's a tsundere artifact
- real-world datasets?

    - Perhaps photos from dating sites where women might be expected to be explicitly optimizing for physical attractiveness? (But what dating sites record both hair length and breast size?)
    - Do porn preferences map onto attractiveness preferences enough? Then we might see the inverse relationship there. (One might worry that all porn performers would have huge surgically-augmented breasts, but [an analysis of the IAFDB](http://jonmillward.com/blog/studies/deep-inside-a-study-of-10000-porn-stars/ "Deep Inside: A Study of 10,000 Porn Stars and Their Careers") says the modal breast size is 34B and natural hair colors are common albeit blonde is still 6x overrepresented.)
- more exotically, increases or decreases in cost should cause corresponding decreases & increases on the margin

<!--
http://tvtropes.org/pmwiki/pmwiki.php/Main/ImageBooru
http://booru.org/top

Danbooru:

1. 1girl short_hair small_breasts: 6300
2. 1girl long_hair small_breasts: 10400
3. 1girl short_hair large_breasts: 35200
4. 1girl long_hair large_breasts: 90100

Safebooru:

1. 1girl short_hair small_breasts: 44800
2. 1girl long_hair small_breasts: 78400
3. 1girl short_hair large_breasts: 281600
4. 1girl long_hair large_breasts: 792000

Gelbooru:

1. 1girl short_hair small_breasts: 448056
2. 1girl long_hair small_breasts: 712656
3. 1girl short_hair large_breasts: 2245572
4. 1girl long_hair large_breasts: 5741820

Big Booru:

1. 1girl short_hair small_breasts: 290000
2. 1girl long_hair small_breasts: 487500
3. 1girl short_hair large_breasts: 1450000
4. 1girl long_hair large_breasts: 3825000

Sankaku:

1. 1girl short_hair small_breasts: 10254
2. 1girl long_hair small_breasts: 16715
3. 1girl short_hair large_breasts: NA
4. 1girl long_hair large_breasts: NA

Rule 34:

1. 1girl short_hair small_breasts: 142002
2. 1girl long_hair small_breasts: 220206
3. 1girl short_hair large_breasts: 854070
4. 1girl long_hair large_breasts: 2123856

~~~{.R}
hair <- read.csv(stdin(), header=TRUE)
Source,Hair,Bust,Count
D,0,0,6300
D,1,0,10400
D,0,1,35200
D,1,1,90100
S,0,0,44800
S,1,0,78400
S,0,1,281600
S,1,1,792000
G,0,0,448056
G,1,0,712656
G,0,1,2245572
G,1,1,5741820
B,0,0,290000
B,1,0,487500
B,0,1,1450000
B,1,1,3825000
SC,0,0,10254
SC,1,0,16715
R,0,0,142002
R,1,0,220206
R,0,1,854070
R,1,1,2123856


R> summary(lm(Count ~ Bust*Hair + Source, weights=hair$Count, data=hair))
Call:
lm(formula = Count ~ Bust * Hair + Source, data = hair, weights = hair$Count)

Weighted Residuals:
       Min         1Q     Median         3Q        Max
-798138384 -288240567  193252689  533023699  941450087

Coefficients:
               Estimate  Std. Error  t value  Pr(>|t|)
(Intercept)   -30694.32   755866.99 -0.04061 0.9682254
Bust         1384690.13   785083.24  1.76375 0.1012455
Hair          211974.71   912130.60  0.23240 0.8198497
SourceD     -2770890.08  1868457.08 -1.48298 0.1619127
SourceG      1424192.80   364668.74  3.90544 0.0018075
SourceR     -1263029.02   474504.53 -2.66178 0.0195680
SourceS     -2329375.92   696359.93 -3.34507 0.0052704
SourceSC      -86426.14  4267593.31 -0.02025 0.9841501
Bust:Hair    2358765.29   984894.94  2.39494 0.0323902

Residual standard error: 695968600 on 13 degrees of freedom
Multiple R-squared:  0.9203475, Adjusted R-squared:  0.8713305
F-statistic: 18.77611 on 8 and 13 DF,  p-value: 5.871975e-06

R> summary(glm(Count ~ Bust*Hair + Source, data=hair, family=quasipoisson))

Call:
glm(formula = Count ~ Bust * Hair + Source, family = quasipoisson,
    data = hair)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-67.802390  -20.428275   -0.038452   11.096047   40.913905

Coefficients:
               Estimate  Std. Error   t value   Pr(>|t|)
(Intercept) 12.55492580  0.04015097 312.69300 < 2.22e-16
Bust         1.65372828  0.04166133  39.69456 5.9170e-15
Hair         0.48294373  0.04840779   9.97657 1.8489e-07
SourceD     -3.75239963  0.09916244 -37.84094 1.0963e-14
SourceG      0.41307523  0.01935321  21.34401 1.6693e-11
SourceR     -0.59446048  0.02517676 -23.61147 4.6336e-12
SourceS     -1.62082008  0.03695088 -43.86417 1.6296e-15
SourceSC    -3.31597685  0.22650330 -14.63986 1.8625e-09
Bust:Hair    0.46622704  0.05227022   8.91955 6.6649e-07

(Dispersion parameter for quasipoisson family taken to be 1364.304961)

    Null deviance: 35294730.334  on 21  degrees of freedom
Residual deviance:    18149.327  on 13  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 3
~~~

Why isn't the popularity of big-long images just the sum of big+long? What is this interaction?
-->

#### Physical Beauty

Is [physical beauty](!Wikipedia "Physical attractiveness"), masculine or feminine, a negative-sum, zero-sum (positional) or positive good?
And has beauty increased or decreased over time?

In looking at historical paintings & statues, I've always been struck by how, even in erotic artwork or work meant to depict the epitome of human beauty or artwork intended to flatter a patron (or serve as an advertisement for a possible betrothal), they *just aren't that beautiful*.
(Yes, them being 'Rubenesque' may be part of it but the modern age of obesity should have long ago negated that.)
The disparity gets worse when you look at American photographs from the 1800s onward, such as in biographies; a woman might be described as stunningly beautiful but look quite average in the provided photograph.
Or when reading about classic Hollywood starlets such as [Jean Harlow](!Wikipedia), after making allowance for the fashions like hideous eyebrows and frying their hair, I can only find them odd looking.
Or when highschool/college class photos are provided from the early 1900s, I can compare them to my own high school class photos, and the sets are almost disjoint in attractiveness - perhaps the top quarter of the old photos overlaps with the bottom quarter of the new photos.
But on the other hand, American material from the 1970s or 1980s, does not strike me as any worse than in the 1990s or 2000s (perhaps even better), with most of the increase being perhaps in the 1920-1960 time range.
(There may have been increases before then, but while related things like adult life expectancy & height can be documented to have increased considerably before the 1920s, there are no high-quality photographs from before then to judge beauty by.)
So if I can see such a clear trend in increasing beauty over time, does that mean that beauty is increasing?

If it has, then there are many possible reasons.
The 20th century in particular saw major progress in nutrition (eg iodization eliminating goiters, which surely are not beautiful), vaccinations eliminating harmful and disfiguring diseases like smallpox, an almost total shift from outdoors work to indoors work (bringing with it protection from the sun and the elements), delayed entry into the workforce, far less manual labor^[Which one might expect to hurt, but manual labor is not as effective as regular exercise as it is highly repetitive, can be harmful, does not spread the work over the body evenly and cannot be calibrated to one's fitness level, and must often be done at rates, times, places, and conditions minimally of one's choosing. So increasing gender equity, permitting - even expecting - women to participate more in sports and use public gyms etc, could well offset this reduction. Certainly an Afghanistani woman confined to her house by [purdah](!Wikipedia) is not better off for it.], cheaper clothing and cosmetics (not to mention a radical expansion in the kinds of cosmetics available such as the creation from almost nothing of the plastic surgery industry), lower lifetime birth rates etc.
Many of these changes happened during the 1920-1960 time window, in which iodization went nationwide, key vaccines like polio were rolled out or used to eradicate diseases in the USA, [urbanization rates](!Wikipedia "Urbanization in the United States") almost doubled, per capita GDP doubled, etc.
All of these could be expected to improve physical beauty, and we can see first-hand proof of how 'aging' life in poor countries can be when we look at photographs of women: for example, there is a famous photograph "Migrant Mother" from the Great Depression of a [despairing worn-out woman with her children](!Wikipedia "Florence Owens Thompson"), who one might guess was in her 40s or 50s - she was 32.
A striking example is provided by the before/after of the famous [Afghan Girl](!Wikipedia): from the original photograph, one might guess at her 20s (she was 12), and when she was refound 17 years later at age 30, one might guess she was in her 60s from how haggard and worn her face is.
[Isabella Bird](!Wikipedia), traveling in impoverished central Japan in 1878, was struck in the mountains by the sight of the people: "The married women look as if they have never known youth, and their skin is apt to be like tanned leather. At Kayashima I asked the house-master's wife, who looked about 50, how old she was (a polite question in Japan), and she replied 22 - one of many similar surprises." (_Unbeaten Tracks in Old Japan_, pg94, Letter XII) comparing them unfavorably to the women of the [Ainu](!Wikipedia), who "look cheerful, and even merry when they smile, and are not like the Japanese, prematurely old, partly perhaps because their houses are well ventilated, and the use of charcoal is unknown."
One can also see this phenomenon in other countries like Russia with jokes about how 'devushkas' turn into 'babushkas' overnight on their 30th birthday.
This accelerated aging turns out to be politically relevant, as many wealthy countries grant special immigration privileges to people under 18 years, but older people in poor countries can claim to be much younger than they are and proving otherwise is difficult.
Jean Harlow herself furnishes an interesting example, as after long-running health problems such as weight gain/fatigue/paleness, she died aged 26 of kidney disease (now mostly treatable) which was probably the sequelae of a childhood infection by [scarlet fever](!Wikipedia) (now curable & occurrence largely suppressed by antibiotics).

Some objections come to mind:

- with an increasingly large population, the most extreme models and actresses will be much more beautiful than early on, similar to sports. The USA was a smaller population in 1900 than in 2016, and Hollywood & advertising have likewise expanded enormously, in addition to recruiting globally. Early Hollywood starlets were big fish in small national pools. Or perhaps modern advertisements and media are increasingly manipulated with Photoshop

    But then why does it also hold true when we compare photographs of ordinary people, and why would the artwork, whose artists were little constrained by reality, have been exceeded as well? And can we really say that the elimination of things like smallpox scarring makes no difference?
- beauty is purely relative

There are at least 2 possibilities for how beauty works:

1. beauty is (mostly) relative/ordinal and is perceived as relative: a beautiful person is merely someone above the average on some arbitrary cultural measurements which are caused by no important objective attributes like health or strength; in another group of people, the same person would be rated by the same raters as ugly rather than beautiful. Particularly good examples of the relativism include the centuries of tooth-blackening and eyebrow-plucking among the Japanese aristocracy, Chinese foot-binding, tanning vs white skin, gavage in Mauritania etc.

    Changes in beauty, therefore, indicate no gains to the possessors of beauty, cause no additional pleasure/displeasure in those around them (as they will perceive the same average level of beauty regardless), will vary wildly from culture to culture, and beauty itself is a harmful construct in that the biases in favor of beauty can disproportionately harm subgroups and in general causes wasteful arms races in time & money spent on tactics like cosmetics, clothing, or surgery, which leaves the group worse off.
2. beauty is (mostly) objective/cardinal and is perceived as objective: a beautiful person is above average on objective attributes like [facial](!Wikipedia "Facial symmetry") [symmetry](!Wikipedia "Fluctuating asymmetry"), long hair, smooth undiseased skin, height, energy & health, personality, intelligence etc. Hence, entire groups of people can increase or decrease in their average beauty, and ratings of individuals will not shift based on reference group.

    Changes in beauty, therefore, may be due to objective improvements or it may be due to cosmetics etc. However, since perceptions are not relative, people will enjoy more what they see, so the arms races may be worthwhile in the same way that any decoration or artwork is worthwhile - because it looks nicer. On the other hand, to the extent that beauty serves as an indicator for objective things, this may be harmful: for example, if beauty & reproductive fitness are to reduce genetic [mutation load](!Wikipedia), use of cosmetics is harmful as it hides the harm being done by bad genes & prevents them from being purged.

If #1 is right, then there should be high levels of disagreement about whether a photograph of an individual is ugly or beautiful between raters (who will have been raised in different social groups and have different standards), higher still across ethnic groups, and almost total global disagreement across cultures; and beauty should correlate minimally with traits because social treatment has little effect on stable traits like height or health or intelligence or personality.

["Maxims or Myths of Beauty? A Meta-Analytic and Theoretical Review", Langlois et al 2000](http://www.jonathanstray.com/papers/Langlois.pdf), meta-analyzes a variety of studies, and on the first point, finds that ratings of beauty are remarkably consistent and actually increase with distance: within-culture, _r_=.9/.85; cross-ethnic, _r_=.88; cross-culture, _r_=.94.
(Given the limits of such inventories, this might imply that agreement on beauty cross-culturally approaches identity.)
Langlois et al 2000 also finds that more attractive adults are more employed, date & have sex more and are more socially skilled & extraverted, are in better mental & physical health, and are slightly more intelligent.
Unsurprisingly, beliefs that the beautiful are treated better by other people also turn out to be true. (Given that sex did not strongly moderate the results, this suggests that either men pay too little attention to their appearances or women too much.)
Combined with the other evidence for things like fluctuating symmetry, #1 can be rejected.
(Theory #2 is also more consistent with my personal observations.)

The past is a foreign country, so it seems like a safe assumption that the beauty ratings of someone in, say, 1920 would correlate _r_=.94 with ours.
Then ratings will still be very similar - eg someone rated at the 84th percentile (+1SD) by us would on average be rated 82nd percentile (+0.94SD) by them.
So we would expect that the modern mean of beauty would be higher as long as it's at least 0.06SDs higher, which is not much at all.

That would assume the difference is random, though, and not systematic: in the worst case, if that remaining 0.06 reflects a consistent cultural preference & fashion of the moment, then someone in 1920 will rate higher all people from 1920, and someone from 2016 will rate higher all people from 2016.
How large would this rating bonus have to be to produce an overall correlation of _r_=.94?
The total variance is $0.94^2 + b^2 = 1$, so a binary variable totally explaining the remaining variance must have the effect _b_=0.342. <!-- Simulation check: x <- rnorm(100000); era <- rbinom(100000, 1, p=0.5); cor(ifelse(era, x+0.342, x), ifelse(era, x, x+0.342)) -->
So in the worst case, we would have to demonstrate an increase by our standards of +0.342SDs before we could be sure that people from 1920 would agree there had been an increase.
The implication of this increase is that our 50th percentile would have to match their 63rd percentile; or to put it another way, in random pairs, ~59.5% of modern people would have to be judged the more beautiful.
I think this is a bar that could definitely be met, so even in the worst case, beauty has increased over time.

#### No-poo self-experiment

Modern Western-style [shampoo](!Wikipedia) is a fairly recent hygiene innovation, which for some people raise the question of how useful it could really be and whether it actually works; claims that shampoo is useless or harmful to hair appearance has given rise to the [no poo](!Wikipedia) meme, which is what it sounds like.

I find it an interesting assertion (it's not like I've ever run into randomized controlled trials demonstrating shampoo is superior to no-poo), and my fine curly brown hair often becomes oily and unattractive if I do not shower regularly, so it would be great if I could save time, cut out shampoo/soap, and look better more consistently.
No poo advocates also cite some low-quality studies in support of their claims.

But I didn't see how I could test the claims in a self-experiment: you quickly adapt to your own body odor or appearance, you cannot be blinded since you know if you're not using shampoo/soap and you can't use my usual placebo-pill trick for blinding, and the consequences of being wrong about whether you have offensive body odor or nasty hair can be severe (judgment based on appearance is pervasive and applicable to all sexes & ages, see [Langlois et al 2000](http://www.jonathanstray.com/papers/Langlois.pdf "Maxims or Myths of Beauty? A Meta-Analytic and Theoretical Review")).
You could try to get around the adaptation problem by asking a third party to sniff you regularly and rank you on a dankness scale, but there's no one I'd inflict such an ordeal upon.
So my initial interest subsided until I happened to read Julia Scott's NYT Magazine article, ["My No-Soap, No-Shampoo, Bacteria-Rich Hygiene Experiment"](http://www.nytimes.com/2014/05/25/magazine/my-no-soap-no-shampoo-bacteria-rich-hygiene-experiment.html), about a startup arguing that some commensual bacteria can substitute for soap & shampoo and how she enrolled in their trial to see how it works.
She reports the usual sequence for no-poo anecdotes: an initial period of a week or three where her cleanliness and appearance go to hell, and then a slow recovery to baseline.

Reading about how her hair "turned a full shade darker for being coated in oil that my scalp wouldn't stop producing", I suddenly realized: there *was* a simple way to test the hair half of the no-poo meme, in a way which was blind, did not involve a third party, and avoided the adaptation problem of a rating each day.
Take photos of your hair every morning of the experiment in the same place & posture & indoors lighting (I choose 3 pictures in 'automatic' mode and 3 in 'closeup' mode), storing them on the same digital camera; cameras record metadata such as the day a photo was taken, preserving the information about what randomized (50-50) experimental condition (poo or no-poo) the photo was taken under; then at the end of the experiment, without ever looking at any of the photos by date, have a program randomly select photos, ask you for a rating, storing the date/filename/rating; then do the statistical analysis on that triplet.
In this way, an objective dataseries (of hair photos) is created without any chance for (visual) adaptation and the rater is kept blinded (as to whether each photo is from poo or no-poo days) when extracting the rating.
Because only the rating is being done blinded, it's a partial blinding and it's possible that the subject could neglect their hair differentially under one condition but not the other - but this partial blinding addresses the biases I highly likely expect to be skewing most no-poo anecdotes, and renders the self-experiment more than a self-deluding waste of time.

The ratings will be a Likert scale of 1-5.
The best analysis for this would be, I think, a multilevel ordinal logistic model with ratings nested in photos and photos nested in days; the main variable is poo vs no-poo, of course, but my hair is visibly affected by other factors and those should be included as covariates: whether I showered the previous day; whether I took a long walk the previous day; the local heat high the previous day; the local humidity (high humidity makes my hair curlier); how long I slept the night before (bedhead); and for an interaction term, whether the day is the first week of the month when - for no-poo - hair should look worst, and it might be work trying #### of days since start of month to see if there's a linear improvement over time.
The weather data can be sourced from Wunderground like for my [Weather]() analysis.
A final tweak might be use multiple ratings of each photo to estimate how much measurement error there are for ratings and fit a [errors-in-variables model](!Wikipedia) (although these don't seem to be well-supported in regular R libraries, which might motivate a move to a Bayesian language like JAGS or Stan).

Because no one seems to adapt in under a week, the blocks must be very long.
I chose pairs of months as usefully long blocks which are also convenient.
I am not sure in advance how long I should run the experiment: I have no previous experiments I can compare to for effect sizes nor any guide from research literature; probably I will cut it short if the no-poo turns out disastrous, otherwise I'll run it for perhaps half a year since it's not much work to take photos in the morning.
(Hopefully I will have an answer before Christmas forces me to reach a conclusion.)

1. June: poo; July; no-poo
2. August: poo; September 1-20: no-poo <!-- started August 4th: forgot first day, too busy to shower next two, showered on the 3rd after photos; absentmindedly soaped up on the 20th -->
3. September 21 - October 21: poo; October 22 - November 22: no-poo
4. November 23 - 4 January 2015: poo; 5 - 30 January: no-poo

    The exigencies of the holidays interfered with the planned switch in late December. On 6 January, my camera died (RIP 2004-2015) and I switched to using my Samsung Galaxy smartphone.
5. 31 January - 30 March: poo; 31 March - 3 April: no-poo

    Easter interfered with the planned transition; didn't want to risk being greasy.
6. 4 April - 8 April: poo; 9 April - 5 May: no-poo
7. 5 May: poo; 30 May - 30 June: no-poo
8. 1 - 16 July: no-poo; 17 July - 7 September: poo
9. 8 September - 30 September: no-poo; 1 October 2015 - 27 March 2016: poo
10. 12 April 2016 - 20 May 2016: no-poo; 21 May - 24 June: poo
11. 25 June 2016 - 17 July 2016: no-poo; 18 July - 18 August?: poo

The main concerns with this design seem to be whether a month is enough time to show any no-poo adaptation, and whether it would be possible to estimate time for each photo despite controlling the conditions as much as possible.

After 2 weeks of no-poo, it seems like my hair has indeed darkened, but it also looks fine for the usual time period after a shower; the main difference seems to be that when it looks bad (due to yardwork or not showering), it looks very bad.

<!-- https://www.google.com/search?num=100&q=%22no-poo%22%20OR%20%22no%20poo%22%20OR%20shampoo%20site%3Alesswrong.com -->

#### TV & the Matrix

One of the most common geek criticisms of _The Matrix_ is that the supposed value of the humans to the machine overlords is as an *energy source*; but by any comparison to alternatives like burning coal, solar power, fusion plants etc, human flesh is a terrible way of generating electricity and feeding dead humans to other humans makes no sense. [An example](http://hpmor.com/chapter/64):

> `NEO`: "I've kept quiet for as long as I could, but I feel a certain need to speak up at this point. The human body is the most inefficient source of energy you could possibly imagine. The efficiency of a power plant at converting thermal energy into electricity decreases as you run the turbines at lower temperatures. If you had any sort of food humans could eat, it would be more efficient to burn it in a furnace than feed it to humans. And now you're telling me that their food is *the bodies of the dead, fed to the living?* Haven't you ever heard of the laws of thermodynamics?"

There's a quick way to rescue the Matrix-verse from this objection: that was simply a dumbing-down for the general movie audience.
To take an existing SF trope (eg from [Dan Simmons](!Wikipedia)'s [Hyperion Cantos](!Wikipedia)), the real purpose of humans is to reuse their brains as a very energy-efficient (estimates of the FLOPS of a human brain against the known ~watt energy consumption indicate orders of magnitude more efficiency than the best current hardware) highly-parallel supercomputer, which would justify the burden of running a Matrix.
From the Matrix short story ["Goliath"](http://matrix.wikia.com/wiki/Goliath):

> "...we were really just hanging there, plugged and wired, central processing units or just cheap memory chips for some computer the size of the world, being fed a consensual hallucination to keep us happy, to allow us to communicate and dream using the tiny fraction of our brains that they weren't using to crunch numbers and store information. "

But this raises additional questions:

1. the AIs won the war with the humans in this version too, so why exactly do they need any human computing horsepower?

    Perhaps the AIs collectively are superior to humans in only a few domains, but these domains had military advantage and *that* is why they won.

    Or more narrowly, perhaps the AIs are collectively superior in general, but there's still a few domains they have not reverse-engineered or improved on human performance and those are what the human brains are good for.

    More intriguingly, it's well-known in machine learning & statistics that something like [Condorcet's jury theorem](!Wikipedia) holds for prediction tasks: a collection or [ensemble](!Wikipedia "Ensemble learning") of poor error-prone algorithms can be combined into a much better predictor as long as their errors are not identical, and a new different algorithm can improve the ensemble performance even if it's worse than every other algorithm already in the ensemble.
    So the humans could, individually or collectively, be useful even if humans are always inferior to other AIs!
2. how do you make use of intact humans brains? With existing machine learning/AI approaches to neural networks, each neural network is trained from scratch for a specific task, it's not part of a whole personality or mind on its own. What do you do with an entire brain with a personality and memories and busy with its own simulated life? If the AIs want the humans for image-recognition tasks (very handy for robots), how do they extract this image recognition data in a useful manner from people that are spending 24h in a computer simulation?

    The most obvious way is to hire researchers normally: run a shadowy "hedge fund" or "defense agency" and assign real problems from outside the Matrix; many techniques are generalizable, and the circumstances guarantee that no-one will be able to figure out what the R&D is for.
    This will show up as countries spending ridiculously large amounts of money on their financial sectors despite the minimal economic gain associated with them, and ridiculously large amounts of money on "national defense" (one can perhaps use up as much as a twentieth of GDP without risking verisimilitude: one must keep defense budgets plausible when trying to extract useful work from large industrialized countries with weak neighbors, sea borders, and no realistic threats to their national security, but if all else fails, wars can always be ginned up and subjects' fear centers stimulated en masse).
    That approach has limits though, as world-class mathematicians etc are intrinsically rare; so how do you get anything useful out of the remaining 99% of the human populace, since you can hardly hire them to labor outside of the Matrix?

    Insert the tasks into the simulated environment in a naturalistic way, of course. You have an image which might be a bat? Insert it and see if people think "a bat!" You need to recognize street numbers? hijack someone walking down a "street", replace the real house number with the unrecognized image, and see what they think. Ditto for facial recognition.

    This works because it may be easier to detect a human brain thinking "bat" than it is to recognize a bat; the human may say "bat" (very easy), subvocalize the word "bat" (fairly easy), or think "bat" (not so easy, but near or at the 2014 fMRI state of the art). You could make it even easier by feeding your human brains a test set or library of known-images, figuring out the common brain signature which corresponds to "bat", then one can easily deduce the brain signature on subsequent unknown images, thereby classifying the unknown images - very similar to existing machine vision practices.

    Of course, to do that on *all* topics of interest and not just bats, you would have to feed human brains a great deal of imagery which could make no sense as part of their ordinary daily life.
    Ideally, they would be raptly focused on a rapidly changing sequence of images, and as much as you can feed them, so the equivalent of a full-time job, perhaps 5+ hours a day or [24-33 hours a week](http://www.nydailynews.com/life-style/average-american-watches-5-hours-tv-day-article-1.1711954 "Average American watches 5 hours of TV per day, report shows: Time spent watching live TV increases steadily as we get older, according to a new report from Nielsen. African-Americans, followed by whites, watch more TV than other ethnic groups.").
    You'd want to start programming human brains as early as possible in life, perhaps starting around 2 years of age, so as to minimize how much food & energy they use before they can start computationally-useful tasks.
    And given how strange and alien as this all sounds to any normal healthy human lifestyle, you would need to make the test-set uploading as addictive as possible to ensure all this - it'd be no good if a lot of humans opted out & wasted your investment.

In other words, *television is how the Matrix operators exploit us*.

#### Mathhammer

*In the grim future of Mathhammer 4e4, there is only proof!*

"Men, we face an acute situation. Within arcminutes, we will reach the enemy tangent.
I expect each and every one of you to give the maximum.

Marines, do not listen to the filthy Polars!
Remember: the Emperor of Mankind watches over you at the Zero!
Without his constant efforts at the Origin, all mankind would be lost, and unable to navigate the Warp (and Woof) of the _x_ and _y_ axes.
You fight not just for him, but for all that is good and real!
Our foes are degenerate, pathological, and rootless; these topologists don't know their mouth from their anus!
BURN THE QUATERNION HERETIC! CLEANSE THE HAMILTONIAN UNCLEAN!"

And in the distance, sets of green-skinned freaks could be heard shouting:

"Diagonals for the Orthogonal God! Affines for the Affine God!
More lemma!
WAAAAAAAAAAAAGGHHH!!!!!!"

Many good men would be factored into pieces that day.

#### North Paw

A direction sensor belt (a ring of vibrators around one's waist; the one closest to North buzzes gently). See the [_Wired_](http://www.wired.com/wired/archive/15.04/esp.html) article on it, and a [2009 article](http://hplusmagazine.com/articles/enhanced/my-new-sense-organ) describing Sensebridge's North Paw product  The [feelSpace](http://feelspace.cogsci.uni-osnabrueck.de/) homepage is here. There is a thread on [Hackers News](https://news.ycombinator.com/item?id=609983) about building one's own. Here's a [version of the belt](http://www.exothermia.net/monkeys_and_robots/2009/02/04/on-the-haptic-compass/) made using [Arduino](!Wikipedia). There's a quasi-commercial version available for $119-214 from [Sensebridge](http://sensebridge.net/projects/northpaw) and through [Think Geek](https://www.thinkgeek.com/product/f358/) ([video](http://vimeo.com/groups/45234/videos/11912761)), intended for wearing on one's ankle (the original ankle-based project is ["Noisebridge"](https://www.noisebridge.net/wiki/Compass_Vibro_Anklet)). There's an [Arduino-based belt](http://www.gradman.com/hapticcompass), then there's a [_hat_](https://www.noisebridge.net/wiki/Compass_Vibro_Anklet)! I think most approaches are just a little baroque; it might make more sense to have each vibrator be independent - with a vibrator, a compass, and a battery. After all, each one should be able to know independently of the others whether it is facing North or not.

     Parts:
     - <http://www.imagesco.com/catalog/DigitalCompass/DigitalCompass.html>
     - <http://www.imagesco.com/articles/1490/01.html>
     - <http://www.imagesco.com/kits/digital-navigation-boards.html>

     Tutorials:
     - <http://www.sparkfun.com/commerce/tutorials.php>


I had been meaning to buy or build one ever since I read the _Wired_ article back in 2007 or so, but had never quite gotten around it.
The topic came up briefly on Hacker News and I suddenly remembered my intention and I worried that Sensebridge no longer sold them 7 years later; fortunately, they still did, so I took the hint and decided to get around to it.

##### Purchase

I ordered a pre-assembled North Paw on 14 August 2014.

It arrived 20 August; [calibration](http://sensebridge.net/projects/northpaw/instructions/step-3/) was straightforward.
Smaller than it looked in the few photos online, and the packaging is accordingly brief:

![North Paw in packaging](/images/northpaw/packaging.jpg)

Stretched out, it reminds me of a watch, with its big black box and smaller blue battery attached by wires:

![A North Paw laid out flat, with control box and battery pack visible](/images/northpaw/full.jpg)

Curled up, it look more reasonable to wear:

![](/images/northpaw/circled.jpg)

The gray cable you can see in this closeup is how the chip/compass communicates with and controls the motors hidden in the band itself:

![](/images/northpaw/casecloseup.jpg)

The fabric band can be unzipped to see and rearrange the little motors if their positioning is bad:

![North Paw unzipped and 8 vibrators visible](/images/northpaw/internalmotors.jpg)

And then putting it on to take a look:

![Wearing a North Paw on one's ankle, from the side and from above](/images/northpaw/ankle.jpg)

My initial impression is that the vibrations are stronger than expected, but they turn off after a minute or so with no movement.
Interesting sensation feeling the motors successively turn on/off as one spins; it's also a dramatic demonstration of the 'sensory homunculus' - I can feel the individual motors very distinctly when I hold it with my fingers, but when I put it on, the skin around the ankle reports only the vaguest "there's some buzzing over here, maybe" sensations.
After about 6 hours of use (1 hour walking around the neighborhood with it on), I don't feel transformed.

Instead, what I feel is sort of a 'wall' to the north of me, in the same way that when you're in a very large open room such as a gymnasium or the Smithsonian Air & Space museum outside of Washington D.C., you don't feel uprooted & disoriented like you might in a place like Iowa or out in the ocean where it's flat and landmarkless as far as the eye can see; instead, you sort of orient yourself 'against' or 'towards' the nearest wall (however far away it might be) and you get closer or further to the wall as you move around.
With the North Paw on, I feel vaguely like there's a wall far away to the north of me that I rotate or shift with respect to (which I suppose is more or less the case with the magnetic north pole).
An odd feeling.

The battery life seems to be at least 8 hours, and one recharges it with a [USB Mini B](!Wikipedia "USB#Mini and Micro connectors") cable.
(I was worried I didn't have one and would have to order a cable, but it turned out the hard drive enclosures for my laptop-size backup hard drives is such a cable.)
There does not seem to be a battery life indicator, so I will simply charge it overnight.

On the third day, I noticed that the vibrations seemed to be weaker and harder to notice, although when I felt it with my fingers the motors seemed to be vibrating as strongly as ever, so perhaps the adaptation really is happening and my mind is gradually filtering out the vibrations.
During my walk, the battery pack came loose: it turns out to be attached to the fabric circlet by an adhesive mount, so it can come loose.
This is a little worrisome (what if it comes off during a violent or sudden movement? will it break the North Paw as it rips out?) but pressing it back in place firmly seemed to work, and for good measure, I used a black [binder clip](!Wikipedia) overnight.

The fourth day, I happened to take a drive.
The vibration from driving seems to mostly drown out the North Paw, but I did notice that roads seem to be aligned north/south or east/west to a degree I'd never appreciated before.
The binder clip didn't work and the battery came off again.
This time I simply took a blue rubber band and wrapped it around the battery/anklet, which works nicely.

By day 7, the vibration is definitely starting to be filtered out and is no longer annoying.
It's a little comforting, even.
(For a few moments one night while going to sleep, I thought I could feel some vibrating on my left ankle. Phantom paw syndrome?)

After 4 weeks or so, I began to get a little disenchanted with it; I was feeling nothing particularly new.

Another few weeks after that, it no longer seemed to be working right as motors would not go off in sequence as I did a slow spin, so I put it aside for 2 months.
After dealing with the holidays, I was playing with it some more (was the battery dead? were individual motors not working, perhaps because the wiring had come loose? was it not turning on right?) since I had spent a fair bit of money on it, when I noticed that doing a vertical rotation seemed to trigger all the motors - so perhaps somehow the *calibration* had gone wrong.
I re-calibrated, and that fixed the problem. So I began using it again.

##### Conclusion

After some further on/off periods, I decided to stop use and sold it in October 2016 to someone else to try.
The buyer noted that most of the motors didn't seem to be working, and the wires/motors looked badly corroded (perhaps I sweated too much?); he bought 10-wire cable with IDC clips, shrink tubing, & pager motors, and was able to get the North Paw working again - it was only a wire/motor problem and the circuit box/battery were still working.

Ultimately, the North Paw was a failure for me. I felt little sense of mental rewiring or intuitive sense of direction, and nothing worth the money. Oh well.

<!-- North Skirt:
http://steampunkworkshop.com/north-skirt
http://makezine.com/video/north-finding-led-skirt-craft-video/ -->

#### Lighting

I am well aware of the effects of lighting on my mind from reading up on the effects of light (and blue light in particular) on circadian rhythms & [melatonin](Melatonin) secretion, and have done a [sleep self-experiment on red-tinting my laptop screen](Zeo#redshift-flux).
(There seems to be a voluminous literature on bright lights being beneficial for alertness in the workplace, but I haven't read much of it.)
Despite this, my room is lit primarily by a lamp with 4 [CFL](!Wikipedia "Compact fluorescent lamp") light bulbs which I inherited, and not designed in any sense - I've focused on modifying myself more than my environment.

The 4 bulbs are puny CFLs: 13 watts (52 total), with a light temperature of 2700k (yellowish).
Particularly during winter, when darkness falls around 4PM sharp, I find the illumination inadequate.
A [LW discussion](http://lesswrong.com/lw/gdl/my_simple_hack_for_increased_alertness_and/) reminded me that I didn't *have* to put up with perpetual gloom - I could buy much larger CFLs and replace the smaller ones.

So after some Amazon browsing, and getting frustrated at how CFL listings equivocate on how many watts they draw vs how many watts-equivalent-incandescent-bulbs they are, I settled on ["LimoStudio 2 x Photo Studio Photography 105 Watt 6500K Day Light Fluorescent Full Spectrum Bulb"](http://www.amazon.com/dp/B005FRCUHY/) for \$22.15 & ordered 6 December 2014.
They arrived on 9 December 2014 & I immediately installed them.
Their temperature is much bluer and 2 105 watt bulbs would roughly quadruple light output (assuming equal efficiency) and since brightness is perceived logarithmically, make the room something like half again as bright.
(They're almost comically larger than the small 13 watt bulbs.)

I had to move the lamp since the naked bulbs in corner of my eye were giving me a headache, but the lighting works well in that corner.
It's nice to have things brighter and it does indeed feel like it reduces sleep pressure in the evening; downsides: shows the walls dirtier, shadows much sharper, feels like it may be much harder to fall asleep even with melatonin if I leave the lights on past 11 PM.
A small benefit is that I still have some incandescent light bulbs installed; with 2 CFLs bumped out of the lamp, I can take them and replace 2 incandescents, which should save some electricity.

During the darkest winter days, just 2 of them now feels inadequate, so I ordered another pair on Amazon on 18 2014.

One handy way to quantify the effect is via my laptop webcam.
Since 2012 or so, I have run a script which periodically takes a snapshot through the laptop webcam and saves the photo.
I haven't gotten much use out of them, but changes in ambient lighting over time would seem to be a perfect use-case.
I've written a script using Imagemagick which analyzes each webcam photo and calculates the average brightness (as a grayscale intensity) and the average [LAB](!Wikipedia "Lab color space") color triplet.
(I originally used RGB but the 3 colors turned out to correlate so highly that the data was redundant and I was told LAB better matches human perception; in any case, the LAB values turn out to be less inter-correlated and so should be more useful.)
The light intensity might affect sleep patterns (particularly sleep timing) and daily productivity.

#### Newton's System of the World and Comets

In the 1724 ["Account of a conversation between Newton and Conduitt"](http://www.newtonproject.sussex.ac.uk/view/texts/normalized/THEM00173), the aged (83) [Isaac Newton](!Wikipedia) reveals some of his speculations about the true system of the world to his inlaw [John Conduitt](!Wikipedia): Newton's [_Principia_](!Wikipedia "Philosophi Naturalis Principia Mathematica") had laid out the laws of celestial motion, but it didn't explain how Solar System was created, how old the Solar System was, how God kept it stable rather than chaotic, how long the Sun would burn until its fire went out, what is the fate of humanity, or the role of comets.

He explains "what he had often hinted to me before" to Conduitt: celestial bodies grow by accretion due to gravity, and as they grow bigger, pass from moons to planets to even larger (!) comets. Comets, in looping past the Sun, slowly become 'cooked'.
The Sun would go out due to its constant conflagration, but fortunately, it is constantly renewed and powered by the fresh fuel provided it by comets passing nearby.
'Intelligent beings' (angels?) oversee this whole process of regular fueling of the sun, but unfortunately the [Great Comet of 1680](!Wikipedia) passed *so* close to the Sun that it seems likely that it will soon fall directly into the sun, rather than feeding it a small measure of fuel.
With an enormous quantity of fuel abruptly dumped into the Sun instead of dispensed over eons, it will flare up like a bonfire and quite likely roast the Earth (like a [red supergiant](!Wikipedia) might, incidentally), naturally killing everything on it.
This possibly happens regularly, since humanity seems to have been created only recently, as evidenced by how recently such major innovations like printing or needles had been made (contradicting any supposition humanity had existed for more than a few thousand years).
After this, possibly God would renew creation by repopulating instead the moons of Saturn or Jupiter which have escaped the inferno relatively intact.
The quotes:

> ...there was a sort of revolution in the heavenly bodies that the vapours & light emitted by the sun which had their sediment as water & other matter had gathered themselves by degrees into a body & attracted more matter from the planets & at last made a secondary planet (viz. one of those that go round another planet) & then by gathering to them & attracting more matter became a primary planet, & then by increasing still became a comet which after certain revolutions by coming nearer & nearer the sun had all its volatile parts condensed & became a matter fit to recruit & replenish the sun...as a fagot would this fire if put into it...
>
> ...that would probably be the effect of the comet in 1680 sooner or later...perhaps have 5 or 6 revolutions more first, but whenever it did it would so much increase the heat of the sun that this earth would be burnt & no animals in this earth could live
>
> ...He seemed to doubt whether there were not intelligent beings superior to us who superintended these revolutions of the heavenly bodies by the direction of the supreme being -
>
> He seemed to be very clearly of opinion that the inhabitants of this earth were of a short date & alleged as one reason for that opinion that all arts as letters long ships printing - needle &c were discovered within the memory of History which could not have happened if the world had been eternal
>
> ...how this earth could have been repeopled if ever it had undergone the same fate it was threatened with hereafter by the Comet of 1680, he answered that required the power of a creator

[pg365-371](/docs/1694-gregory.pdf "Memoranda by David Gregory, 5-7 May 1694: Annotations Physical, Mathematical and Theological from Newton. 5, 6, 7 May 1694") of _The Correspondence of Isaac Newton_ (Turnbull 1961), Volume III:

> [Newton says] that a continual miracle is needed to prevent the Sun and the fixed stars from rushing together through gravity: that the great eccentricity in Comets in directions both different from and contrary to the planets indicates a divine hand: and implies that the Comets are destined for a use other than that of the planets. The Satellites of Jupiter and Saturn can take the places of the Earth, Venus, Mars if they are destroyed, and be held in reserve for a new Creation.

This is a remarkable cosmology and has a lot of sense to it (how *does* the Sun burn more than a long time without a magical process like 'fusion' or else regular resupply? pace Lord Kelvin's [well-reasoned & acute but wrong comments on the Sun's age, comets as inadequate fuel sources, & refuting Evolution](http://zapatopi.net/kelvin/papers/on_the_age_of_the_suns_heat.html "'On the Age of the Sun's Heat', 1862")), but is still very alien.
Angels in charge of comets! Things really were different then.

It would make for an excellent retro SF or [steampunk](!Wikipedia) novel, if nothing else.
(Naturally, the heroes would be recruited by the angels to help deal with the crisis of the return of Newton's comet...
Perhaps [Ted Chiang](!Wikipedia) would like to write a followup to ["Exhalation"](http://www.lightspeedmagazine.com/fiction/exhalation/) or ["Seventy Two Letters"](https://web.archive.org/web/20010802144026/http://www.tor.com/72ltrs.html)?)

It's also interesting for why it's wrong: Newton needs comets to be at least planet-sized, because comets are too rare to be plausibly fuel sources for the Sun if they're small (they wouldn't dump enough fuel in to keep combustion going for another few years/decades/centuries until the next big comet), but of course they're very small; if they were planet-sized, you'd think they'd severely disturb planetary orbital calculations, so was the existing astronomical data insufficiently precise to prove the absence of such orbital disturbances or was there some other issue?

The argument for the short duration of the human race is also wrong, since we know anatomically modern humans have been around for at least 50,000 years at this point.
What's particularly interesting about his argument is that if he had made it at a randomly chosen point in human history, then he would have correctly concluded the opposite, that the human race was ancient, due to the lack of discernible progress: in fact, he could only have made this argument in a tiny window between the start of the Scientific/Industrial Revolution and the archaeological/geological/evolutionary proof of mankind's antiquity starting around the 1800s, so maybe 400 years or 0.4 millennia; he had to have the bad luck to be born into that exact 0.8% historical window for the argument from progress to be wrong!
Kinda remarkable.
(I wonder if Newton's belief that he was merely rediscovering what the ancients like the Chaldeans or King Solomon knew is connected to this 'recent progress' argument, as it could also be taken as evidence for humanity existing for a long but in constant cycles of rise and fall?)

Interestingly, it seems the 'short human history' argument is not original to Newton, because I came across a version of it in Lucretius's _[On the Nature of Things](!Wikipedia)_:

> Moreover, if heaven and earth never had a beginning or birth, but have existed from everlasting, why have there not been other poets to sing of other events prior to the Theban war and the tragedy of Troy? Why have so many heroic deeds so often been buried in oblivion, instead of flowering somewhere, implanted in eternal memorials of fame? The true explanation, in my judgment, is that our world is in its youth: it was not created long ago, but is of comparatively recent origin. That is why at the present time some arts are still being refined, still being developed. This age has seen many improvements in shipbuilding; it is not long since musicians first molded melodious tunes; our system of philosophy too is a recent invention, and I myself am found to be the very first with the ability to expound it in the language of my country [Latin].
>
> If by chance you believe that all these same things happened before, but that the races of human beings perished in a great conflagration, or that their cities were razed by a mighty convulsion of the world, or that rivers, rapacious after unremitting rains, inundated the earth and submerged towns, there is all the more necessity for you to admit defeat and acknowledge that heaven and earth arc destined to be destroyed.

#### Simplicity is the Price of Reliability

Why should we care about simple systems and long part lifetimes?
Because for many things, the only way to build a reliable system is out of as few and more reliable parts as possible.

[Kevin Kelly](http://kk.org/thetechnium/the-art-of-endl/ "The Art of Endless Upgrades") notes a a lesson from a home-repair story:

> When we first moved into our current house, newly married, I had some caulking to do around the place. I found some silicon caulking that boasted on the tube that it was warranted for 20 years. Cool, I thought. I'll never have to do this again. Twenty years later, what's this? The caulking is staring to fray, disintegrate, fail. I realize now that 20 years is not forever, though it seemed that way before. Now that I am almost 60, I can see very permanent things decay in my own lifetime.

Consider a 20 year lifetime for a part. This may sound like a lot, and just like Kelly, you may intuitively feel that 20 years is close enough to 'forever' as to need little more thought.
But each such part or object cannot be considered in isolation, because you have *many* objects.
On a yearly basis, I photograph all my possessions for backup purposes and to sort through my possessions; despite my best efforts to keep clutter down and to photography multiple objects in the same photo, I find it takes more and more photographs every year, and for 2015, had to take 281 photographs of what is probably at least 10 objects per photo so perhaps 3000+ individual objects (of which each may have many components and parts which can break in exciting & novel ways).
Kevin Kelly notes that in [an inventory of his own household](http://kk.org/thetechnium/the-number-of-s/ "The Number of Species We Use"), there was probably 10,000 objects, surprisingly close to the [Inventory of Henry VIII of England](!Wikipedia)'s crown holdings of 17,810 items (pre-[Consumer Revolution](!Wikipedia)), but far more than a selection of Third World households, who supposedly averaged 127 objects. Self-storage complexes, overflowing garages, hoarders...
Thousands of objects, at a minimum, without including infrastructure: the circuit breakers, the hot water tanks, the faucets, the showerheads, the light bulbs, the fans, the lamps, the switches, the floor & roof boards, the pipes and plumbing, the septic tank, the grinder, the windows - the list is fractal.
Stuff breeds stuff, and [kipple](!Wikipedia) lurks under every corner.

But each of these objects can be a problem. They can be lost, or forgotten, or decay, or they can damage other things. (They can also be mental burdens.)
If an object lasts 20 years or 7305 days, but you have 10000 objects, then on average something will break on a daily basis; worse, on a good 30 days, 3 objects will break simultaneously; on around 16 days, 4 will, and on a handful of days, 6 or more objects will break.^[Modeling it as a binomial in R: `table(sort(rbinom(365, size=10000, prob=1/(20*365.25))))`]
(Fortunately, most objects breaking do not cause *serious* problems, and many objects easily last 20+ years and don't contribute too many failures. But if you've ever wondered why in military histories, it seems like every ship or airplane is half-broken all the time and units routinely are disabled by mechanical problems, this is partially why: many moving parts and objects placed in harsh environments.)
If one object breaking can cause another to break, things get even worse...

I recently ran into an example of this.
I use a Zeo EEG-based sleep tracking device, and it is one of my favorite objects which [I have used to run many self-experiments](/Zeo).
I also record my sleep using an Android smartphone accelerometer-based app, since I know my Zeo will not last forever (in particular, the rechargeable battery is expected to die within a few years) and I will most likely be forced to replace it with an accelerator app.

Sometime in late 2015, my Zeo stopped working.
The headset would not relay any data no matter if it should have been fully charged or not.
I concluded the battery had finally died as I long feared; Zeo Inc closed a long time ago, so there are no replacement headsets (or if some can be found used, they will probably cost \$200+ and be using original batteries as well), and one has to DIY by finding a random rechargeable battery, cracking open the headset, and soldering the new one in, which is all new to me.
While I was getting around to this, the lights went out in my bedroom: they flickered irregularly, began going out for hours at a time, and then finally one day they went out entirely.
This was irritating and made my bedroom difficult to use, but not a big problem, because of the 4 sockets in the bedroom, the socket nearest my bed (the one my smartphone, Zeo, and electric mattress for cold nights were all plugged into)
My landlord lackadaisically began to think about calling an electrician.

I did some work on repairing my broken Zeo: reading up on the topic further, ordering the new battery for \$9, and most tricky, cracking open the headset which turned out to be almost impervious to my prying, even when I put it in a vice, and to my horror, when I finally wiggled a razor blade through the corner and started to crack it open, I had cut too far into it and cut a metal ribbon connecting the circuit board with the metal headband!
I had no idea if I had destroyed it or not, but I hoped that if I superglued it down, the physical connection would be enough for the EEG functionality to work.
Unnerved by that, I didn't cut out the original battery and try to solder in a new battery (likely botching it and destroying the irreplaceable headset); I would do that sometime later.
After about a month of the blackout, my smartphone began having trouble recording through the night, apparently running out of power; I wasn't sure what was going on, but after verifying that the smartphone would properly charge while plugged into my laptop, and while plugged into its charger plugged into a socket in the living room, I concluded it had to be the power strip or loose placement in the power socket in the bedroom.
While wiggling the charger around, the lights abruptly came back on, and I began hearing 'pop' sounds and I noticed simultaneous flashes from behind the faceplate of the power socket.
Immediately yanking everything out, I smelt burnt insulation and feared the worst: an electrical fire inside the walls.
I unplugged everything, flipped the circuit breaker, and after monitoring my bedroom with a fire extinguisher for a few hours, insisted an electrician be called within the week.
He came, and... the whole thing was nothing but a loose wire in the one socket I had thought *was* working!
The poor connection caused it to heat up and burn the insulation, and blocked electricity from flowing 'downstream' to the lamps.
Then the smartphone hadn't been charging because it wasn't getting enough electricity through the charger, and as I quickly verified, the Zeo had been suffering the same problem and was perfectly functional. (The superglue had worked.)

I hadn't known it was possible for digital devices to appear to be turned on and receiving power but only *partial* power as I naturally assumed that if the Zeo turned on and appeared operational then the power socket must be perfectly fine, and I was completely surprised that the one power socket I had verified as working - did it not power everything I had plugged into it? - was both malfunctioning, had been for at least 3 months if not for years before that, and this was the root of multiple apparently unrelated problems.
So one loose wire had caused a cascade of issues eventually costing: the electrician's repair bill, \$9, 3 months of sleep logs, a good deal of trouble on my part, and nearly destroyed an nearly irreplaceable favorite possession.

Such a cascade of issues and odd edge-cases with interacting objects is common.
With enough objects in a system such as a home, at least some objects will always be failing and can trigger interactions with some other objects; all of these interactions cannot be predicted because each of these objects is complex in its own right, and their behaviors must be understood on a superficial and black box level.[^Winner]
Cook gives some principles in his well-known ["How Complex Systems Fail"](http://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf):

> 4. *Complex systems contain changing mixtures of failures latent within them.*
>
>     The complexity of these systems makes it impossible for them to run without multiple flaws being present. Because these are individually insufficient to cause failure they are regarded as minor factors during operations. Eradication of all latent failures is limited primarily by economic cost but also because it is difficult before the fact to see how such failures might contribute to an accident. The failures change constantly because of changing technology, work organization, and efforts to eradicate failures.
> 5. *Complex systems run in degraded mode.*
>
>     A corollary to the preceding point is that complex systems run as broken systems. The system continues to function because it contains so many redundancies and because people can make it function, despite the presence of many flaws. After accident reviews nearly always note that the system has a history of prior "proto-accidents" that nearly generated catastrophe. Arguments that these degraded conditions should have been recognized before the overt accident are usually predicated on nave notions of system performance. System operations are dynamic, with components (organizational, human, technical) failing and being replaced continuously.
> 6. *Catastrophe is always just around the corner.*
>
>     Complex systems possess potential for catastrophic failure. Human practitioners are nearly always in close physical and temporal proximity to these potential failures - disaster can occur at any time and in nearly any place. The potential for catastrophic outcome is a hallmark of complex systems. It is impossible to eliminate the potential for such catastrophic failure; the potential for such failure is always present by the system's own nature.
> 7. *Post-accident attribution accident to a "root cause" is fundamentally wrong.*
>
>     Because overt failure requires multiple faults, there is no isolated 'cause' of an accident. There are multiple contributors to accidents. Each of these is necessary insufficient in itself to create an accident. Only jointly are these causes sufficient to create an accident. Indeed, it is the linking of these causes together that creates the circumstances required for the accident. Thus, no isolation of the 'root cause' of an accident is possible. The evaluations based on such reasoning as 'root cause' do not reflect a technical understanding of the nature of failure but rather the social, cultural need to blame specific, localized forces or events for outcomes.

[^Winner]: Life is too short to understand everything to a meaningful level, as specialization has far outstripped individuals' ability or interest to understand them all, and such division of cognitive labor is required to create the modern world, even if this comes at the cost of any global understanding: _Autonomous Technology: Technics-Out-Of-Control_, Winner 1989:

    > Society is composed of persons who cannot design, build, repair, or even operate most of the devices upon which their lives depend...In the complexity of this world people are confronted with extraordinary events and functions that are literally unintelligible to them. They are unable to give an adequate explanation of man-made phenomena in their immediate experience. They are unable to form a coherent, rational picture of the whole. Under the circumstances, all persons do, and indeed must, accept a great number of things on faith...Their way of understanding is basically religious, rather than scientific; only a small portion of one's everyday experience in the technological society can be made scientific...The plight of members of the technological society can be compared to that of a newborn child. Much of the data that enters its sense does not form coherent wholes. There are many things the child cannot understand or, after it has learned to speak, cannot successfully explain to anyone...Citizens of the modern age in this respect are less fortunate than children. They never escape a fundamental bewilderment in the face of the complex world that their senses report. They are not able to organize all or even very much of this into sensible wholes....

    [Do you know how to draw a bicycle?](https://www.behance.net/gallery/35437979/Velocipedia)

My Zeo accident follows several of these: the Zeo could've failed at any time due to the battery getting too old, insufficient voltage or amperage, electrical surges from lightning strikes, the headband conductivity being destroyed by sweat/dirt, software errors etc.
A failure in a separate object triggered a latent but unknown Zeo design flaw (not telling the user it is *unable* to charge the headset due to insufficient electricity and trying to anyway); this led to further errors.
Root-cause attribution is difficult as none of the causes are individually sufficient, and even now I don't see how I could have done better without being an electrician.

Trying to add in preventive objects or procedures runs the risk of creating even further problems; as [one sysadmin on HN says](https://news.ycombinator.com/item?id=8282923)

> Your fail-safes are themselves the source of faults and failures. I've seen, just to list a few: Load balancers which failed due to software faults (they'd hang and reboot, fortunately fairly quickly, but resulting in ~40 second downtimes), back-up batteries which failed, back-up generators which failed, fire-detection systems which tripped, generator fuel supplies which clogged due to algae growth, power transfers which failed, failover systems which didn't, failover systems which did (when there wasn't a failure to fail over from), backups which weren't, password storage systems which were compromised, RAID systems which weren't redundant (critical drive failures during rebuild or degraded mode, typically), far too many false alerts from notifications systems (a very common problem even outside IT: [on hospital alarms](http://redd.it/1x0p1b)), disaster recovery procedures which were incomplete / out of date / otherwise in error. That's all direct personal experience.

When I was still contributing a bit to the DVCS [darcs](!Wikipedia), I thought then-maintainer Eric Kow's efforts to set up automated tests & buildbots, while well-intended in trying to catch errors in patches & ensure that the code always compiled and software engineering best practices, wound up wasting far more time than it ever saved because the testing infrastructure itself kept crashing and needed constant configuration & upgrades.
Given limited contribution time, dealing with the buildbots did not seem like a good investment.
This also applies to programmers tempted to automate or script stuff on their computers: [how much time are you *really* saving](http://www.xkcd.com/1205 "Is It Worth The Time?") and was it worth the risk that one day a year from now you'll wake up and discover a cron job hasn't been running for months because of some Bash error?
(Due to upgrading to a Shellshock-resistant Bash, in my case, which caused an inscrutable interaction with an exported function name in my Bash aliases file.)

Complexity and automation create a technical debt in the form of the fallout from future unreliability.
"The price of reliability is the pursuit of the utmost simplicity. It is a price which the very rich find most hard to pay."

#### Rationality Heuristic for Bias Detection: Updating Towards the Net Weight of Evidence

> Bias tests look for violations of basic universal properties of rational belief such as subadditivity of probabilities or anchoring on randomly-generated numbers. I propose a new one for the temporal consistency of beliefs: agents who believe that the net evidence for a claim _c_ from _t~1~_ to _t~2~_ is positive or negative must then satisfy the inequalities that _P(c, t~1~)_<_P(c, t~2~)_ & _P(c, t~1~)_>_P(c, t~2~)_, respectively. A failure to update in the direction of the believed net evidence indicates that nonrational reasons are influencing the belief in _c_; the larger the net evidence without directional updates, the more that nonrational reasons are influencing _c_. Extended to a population level, this suggests that a heuristic measurement of the nonrational grounds for belief can be conducted using long-term public opinion surveys of important issues combined with contemporary surveys of estimated net evidence since the start of the opinion surveys to compare historical shifts in public opinion on issues with the net evidence on those issues.

A friend of yours tells you he's worried he saw a snake on his driveway yesterday which he thinks may be a poisonous coral snake rather than a harmless snake; he gives it 50-50 chance - it looked a lot like a coral snake, but he didn't think this state had any poisonous snakes.
You see him the next day and ask him about the snake.
"Terrible news!" he says. "I looked it up on Wikipedia, and turns out, this state *does* have poisonous snakes, coral snakes even."
How unfortunate. So what probability does he think it is a coral snake? His probability must have gone up, after all.
"Oh, 50-50."
What? Is he sure about that?
"Sure I'm sure. I still wonder if it was a coral snake or not..."
Your friend is fond of gambling, so you know he has not misspoken; he knows what a probability is.
You politely end the conversation and conclude that while you have little idea if it was a coral snake or not, you do know your friend is fundamentally not thinking straight on the issue of snakes: he understood that he found net evidence for the snake being a coral snake, but somehow did not update his beliefs in the right direction.
Whatever his thinking process, it is non-rational; perhaps he has herpetophobia and is in denial, or has some reason to lie about this.

It can be hard to decide whether someone's conclusions are irrational because they could have different priors, have different causal models, have been exposed to different evidence, have different preferences, and so on.
But there are a few hard rules for bare minimums of rationality: no contradictions; conjunctions are equally or less likely than any of their conjuncts; disjunctions are equally or more likely than any disjuncts; probabilities of exhaustive sets of claims sum to 1; 0 and 1 are not degrees of belief; and - net evidence for a claim increases the posterior probability of that claim.
(Or to put it another, per Bayes rule $P(A|B) \propto P(A) \cdot P(B|A)$, for arbitrary _P(A)_ and _P()_, if _P(B|A) > 1_ then _P(A|B) > P(A)_; $(P(A|B) \le P(A)) \land (P(A) \gt 1)$ is a contradiction.)
And what applies to coral snakes applies to everything else - if your friend agrees evidence suggests his pool was a bad buy, he should be less optimistic about it than he was when he bought it, and so on.
Your friend might have totally different priors or causal models or life experiences or political affiliations, but whatever they are, he still must make his net evidence and update direction jive.
Updating is not sufficient for rationality (one can still have wrong models which indicate something is net evidence which shouldn't be, or update too much, or be irrational on other matters) and updating doesn't itself show notable rationality (perhaps one was just profoundly ignorant about a topic), but it is necessary.

We can broaden it further beyond individuals.
If someone fails to update their earlier estimate towards their claimed weight of evidence, then they are wrong. What about everyone else?
If you surveyed your neighbors and your friend, they would agree that however much one should believe it was a coral snake, upon learning that coral snakes do in fact live around here, it is terrible news and evidence for the snake being a coral snake.
They might not agree with a starting 50% probability, and might argue about whether the Wikipedia article should matter a lot or a little ("whenever I check a WP article, it's always vandalized"), but they would agree that the evidence is in favor of a coral snake and that the correct increase is definitely not 0% or -5%, and anyone who changes their belief that way is just wrong.
Hence, for your neighborhood as a whole, each person is wrong if they don't change their earlier probability upwards.

Can we broaden it further?
If (for some reason, perhaps because we too suffer from herpetophobia) we have surveys of your neighbors about the risk of snakes on their part of this mortal plane going back decades, then we can employ the same trick: ask them what they think the weight of evidence about coral snakes is, and their current probability, and compare to their old probability.

Can we broaden it further?
There are few long-term surveys of opinions of the same people, so this heuristic is hard to apply. But what applies to your neighborhood should also *generally* apply to populations over time, barring relatively exotic changes in population composition like natural selection for high religiosity priors.
Now we ask everyone what they think, or they think the general population thinks, the net weight of evidence has been. (Somewhat like [Bayesian truth serum](http://nel.mit.edu/bayesian-truth-serum).)
If there is some issue which 100 years ago split the population 50-50 on, and everyone agrees that events/data/research since then have generally favored one side of the issue, and everyone is also meeting bare minimums of rationality, we should see that weight of evidence reflected in proportions shifting towards the winning side.
We definitely do not expect to see surveys reporting the split remains exactly 50-50.
If it does, it suggests that the population is not dealing with the issue rationally but for other reasons like personal advantage or politics or cognitive biases.^[Or possibly we messed up somehow. For example, we've misunderstood the survey questions: terminology or popular understanding could have changed to the point where the position of one side has become different to its position now, in the same way that American political party platforms of 2000 bear scarcely any resemblance to those of 1900 of the same name, or how the Christianity of 500 AD differed drastically from 100 AD.]

These directions do not need to be the exactly same over all time periods or for all issues.
For example, consider the question of whether there is alien life on other planets in the Solar system or in other solar systems in the universe from the period 1500 to 1900, 1900 to 2016, and 1500 to 2016.
Isaac Newton and other natural philosophers speculated about life on other planets and throughout the universe, and I think the net weight of evidence as astronomy and biology progressed was heavily on the possibility of life with an ever-expanding universe to generate life somewhere, and so the direction of belief would have been increasing towards 1900 for life in the Solar system and universe; but then, as progress continued further, there was a drastic reversal of fortune - the canals on Mars were debunked, spectroscopy showed no signatures of life, the launch of space probes showed that Venus was not a garden planet but a sulfuric-rain molten-lead hellhole while Mars was a freeze-dried husk of sand sans life; and after the abrupt extinguishing of hopes for Solar life, Enrico Fermi famously asked 'where are they' with no hints of radio activity or stellar engineering after billions of years even as development of rocketry and space technology demonstrated that advanced alien civilizations could colonize the entire galaxy in merely millions of years.
And who knows, perhaps some clear signal of life will yet be discovered and the weight of evidence will abruptly swing back in favor of life in the universe.
Another example might be behavioral genetics and intelligence tests, for which there is an extraordinary disparity between expert beliefs and the general public's beliefs, and for which an equally extraordinary amount of evidence has been published in the past decade on the role of genetics in individual differences in everything from human evolution over the past few thousand years to the presence of dysgenics to the genetic bases of intelligence/personality/income/violence/health/longevity; surveyed experts would doubtless indicate strong weights of evidence against the long-dominant blank slatism and show accordingly changed beliefs, and a survey of the general public might show little or no weights of evidence and belief shifts - but that is not evidence for strongly nonrational public beliefs because it might simply reflect considerable ignorance about the scientific research, which have been minimally reported on and when reported on, the meaning & implications minimized.
So depending on the time period, question, and group the update might be up or down - but as long as it's *consistent*, that's fine.

An example of an application of the net evidence heuristic might be [cryonics](!Wikipedia).
Many objections were raised to cryonics at the start: religious and dualist objections; cell lysosomes would 'explode' immediately after death, erasing all information before vitrification; personality and memories were encoded in the brain not as stable chemical or biological structures but as complex electrical dynamics which would be erased immediately upon death; cryonics organizations would disappear or would 

#### November 2016 data loss postmortem

> In late November 2016, my Acer laptop broke and also corrupted the encrypted filesystem on the SSD, which apparently due to design decisions is very easily broken. Because I had been changing my backup strategy to use an new encryption key and had been lax about my manual backups, this made the most recent encrypted backups undecryptable as well, causing data loss back at least 2 weeks. I review how all this came to pass despite my careful backups, and my countermeasures: a new higher quality laptop, making backups of encrypted filesystem headers as well as the contents, and buying faster external drive enclosures.
>
> Key lesson: *if you use LUKS-encrypted Linux filesystems, know that they are super-fragile!* You should backup a copy of the LUKS header (by running a command like `sudo cryptsetup luksHeaderBackup /dev/sda5 --header-backup-file luks-header.bin.crypt`) to avoid the header becoming corrupted & all data destroyed.

![Investigating my cascade of laptop and backup problems.](http://imgs.xkcd.com/comics/tv_problems.png "XKCD #1760: 'TV Problems'")

On 26 November 2016, I woke to find my [Acer](!Wikipedia) laptop cold and dead.
The power had not gone out overnight, according to the microwave and the UPS, and the power indicator on the laptop was not on.
I swapped the power cord for a generic replacement power cord I'd gotten from Best Buy when the _first_ laptop power cord had died on me back in May; this made no
Attempts to drain the battery and reset the laptop (this laptop had a hardwired battery) did nothing, and the laptop, then and later, remained totally unresponsive and gave no activity.
It was well and truly dead. My best guess is that the motherboard broke/was fried.
This is an unusual way for a (non-Apple) laptop to die, and has never happened to me before (my problems have always been with the screen or keyboard or hard drive or battery/power supply).
My best guess for why the motherboard died is that the stress of doing deep learning on the GPU finally caught up with it; I thought it was handling it fine because the CPU only occasionally had to be throttled, but the cumulative months of running `char-rnn`/`torch-rnn` for projects like [RNN metadata]() may've been too much and it shorted out or something.

The laptop in question was a [Acer Aspire V17 Nitro Black Edition VN7-791G-792A Gaming Laptop 4th Generation Intel Core i7 4720HQ (2.60GHz) 16GB Memory 1TB HDD 256GB SSD NVIDIA GeForce GTX 960M 4 GB GDDR5 17.3-inch Windows 8.1 64-Bit](http://www.newegg.com/Product/Product.aspx?Item=N82E16834314853) I had bought in 24 July 2015 from Newegg for \$1,215.21; it had a 1 year warranty on it, which had expired.
In any case, even had the laptop still been under warranty, I would've had to buy a new laptop simply because I am leaving on a long trip to England on 10 December, and there was no way I could ship a laptop to an Acer repair center, the entire motherboard replaced (which would probably cost easily \$500+), be shipped back to me, and all my data recopied & OS set up in time for me to leave.
So I had to buy a new
To replace the Acer laptop, I decided to go with a brand known for reliability, good laptop keyboards, power, and Linux support: a [Lenovo ThinkPad P70](https://www.amazon.com/Lenovo-ThinkPad-20ER002KUS-i7-6700HQ-Windows/dp/B0178JBBMG/).
[ThinkPad](!Wikipedia) P70s have the 17-inch screens I love, and I added 32GB [ECC RAM](!Wikipedia) because [Rowhammer](!Wikipedia) is frightening and I like the idea of more reliability, and expedited shipping.
Perhaps because I was buying the day after Black Friday, it cost a lot less than I expected: \$1886.59.
It shipped on 30 November 2016, and UPS soon said it was in Anchorage Alaska, so it should definitely make it by 10 December, and indeed arrived on the evening of 1 December, well ahead of my expectations.
(This suggests that paying \$20 for expedited shipping might not have been *that* necessary, but I think it was the right choice - I didn't want to risk it.)
Had it not arrived in time, my backup plan was to drive to Best Buy, buy a regular laptop, and simply return it afterwards.

I pulled out the 1TB Samsung SSD I'd put in and inserted it into the old half-broken Dell Studio 17 laptop I keep as my backup laptop.
(It's half-broken because about a third of the keyboard is missing or about to break, there is a permanent line of LEDs lit up on the screen, and it's ancient - 4GB RAM, only 2 USB ports etc.)
The drive failed to boot with FS corruption, and then [LUKS](!Wikipedia) decryption with the passphrase failed despite at least 30 tries on my part.
The SSD had been corrupted by the motherboard in its death throes, apparently.
Even though the entire partition was there and even most of the LUKS header was there and could be seen using `cryptsetup luksDump /dev/sda5`, the key was not available.
Reading up, I learned that LUKS-encrypted hard drives have *no* redundancy or protection from any kind of corruption, and this is by design/laziness, claimed to be in the interests of security, to quote the `cryptsetup` man page (emphasis added):

> LUKS header: If the header of a LUKS volume gets damaged, all data is permanently lost unless you have a header-backup. If a key-slot is damaged, it can only be restored from a header-backup or if another active key-slot with known passphrase is undamaged. _Damaging the LUKS header is something people manage to do with surprising frequency_. This risk is the result of a trade-off between security and safety, as LUKS is designed for fast and secure wiping by just overwriting header and key-slot area.

("Did you just tell me to go fuck myself?" "I believe I did, Bob.")

This is BS because the security rationale makes no sense (how many milliseconds would it take to erase a second header stored elsewhere on the partition? has anyone ever in the history of the world been saved from law enforcement or hackers because there was only *one* header rather than two?); I suspect the real reason is that the maintainer admits in bug reports that the current LUKS format doesn't allow for multiple copies ([1](https://gitlab.com/cryptsetup/cryptsetup/issues/19), [2](https://gitlab.com/cryptsetup/cryptsetup/issues/77)), and this is just them engaging in sour grapes about how It's Actually A Good Thing.
I can safely say I had no idea whatsoever that Linux encrypted drives were _this_ fragile and permanent data loss so trivial, or that we are expected to do things like `sudo cryptsetup luksHeaderBackup /dev/sda5 --header-backup-file luks-header.bin.crypt` if we want to avoid it.
Such fragility is itself deeply counter to security as it strongly encourages users to not use encryption at all: they have to choose whether to be mugged by bitflips or by the FBI. What a choice.

A lesson is learned but the damage is irreversible.
This was unfortunate, but OK, I have a good backup situation.
It was well-timed, if anything, as I had been busy on 15 November 2016 improving my backup situation by creating a fresh un-passphrased PGP key X for unattended backups and setting up a [Backblaze](!Wikipedia) B2 account for a remote off-site backup (Backblaze storage is ~10x cheaper than [Amazon S3](!Wikipedia)).
So I had available for recovery of my data:

1. 3 passphrased LUKS-encrypted external [USB-2](!Wikipedia) drives, which I [rsync](!Wikipedia) manually to periodically irregularly at my whim (partially because they are so slow), rotating among them; stored in ziplock bags in a locked fire safe in a separate room. These had full backups as of 1 November, 5 November, and 14 November. Useful but outdated.
2. an incomplete Backblaze full backup, using [duplicity](!Wikipedia "Duplicity (software)") backups encrypted to key X. It was going to take a week or two to do the first full upload, so unsurprisingly it had been interrupted by my Acer laptop's untimely demise, and was of no use; oh well.
2. a plugged-in [USB-3](!Wikipedia) 4TB external drive that I had a cron job which did daily duplicity backups encrypted to X; I could see the new duplicity archives created at midnight, so the incrementals were all there. (This external drive is not passphrased LUKS-encrypted since it's a RAID and I didn't want to risk breaking it and quickly filled it up to the point where I couldn't reformat it for lack of anywhere to copy the contents, so I left it as a NTFS FS and simply made sure that all duplicity archives backed up to it were encrypted; same thing in the end.) Very useful - I could pull my data off it quickly and I would lose nothing more than some background activity since midnight. All I needed was key X to decrypt it.

At this point horror dawned.
My most recent immediately accessible backup was the manual backup drive done on 1*4* November, but I had created key X on 1*5* November.
Copies of key X existed on the SSD - whose master key had been corrupted and could not be decrypted - and on the USB-3 external drive - all of which was encrypted to key X - and on Backblaze - also encrypted to key X - _and nowhere else_.

What happened was that I had simply assumed that my regular backup procedures would migrate key X onto the manual external drives, and then in any disaster, I would simply copy over all the data from one of them, and grab incrementals from Backblaze or the USB-3 external drive.
This was _almost_ right and off only by a day.
But so much of my time was used up with Thanksgiving festivities and writing and [digging in a new Ethernet cable](/WiFi) and shopping that my usual manual backup routine of syncing every 5 days or so fell into abeyance.

So I'd lost 16 days of data.

The timing was really remarkably bad.

The previous night, I had wondered mentally if I spent too much on backups. That morning, staring at my bricked laptop+16 days data loss, I knew I spent too little.

> "What is despair? I have known it - hear my song." --[James Mickens, "The Night Watch"](https://www.usenix.org/system/files/1311_05-08_mickens.pdf)

The 5 stages of backup & data loss grieving:

1. Denial: it'll work if I hold the power button; I must've mistyped the password, LUKS isn't broken; I have incremental backups!
2. Anger: no header file copies at all?! Curse you LUKS developers! If only I had made even one backup more recently!
3. Bargaining: even one backup would be enough, I can survive missing a few days, one of the other drives...
4. Depression: gone, all gone, 14 days of data... I don't even remember what I've lost, why I write things down in order to forget them
5. Acceptance: "vanity of vanities, all is vanity". "Look upon my works ye mighty and despair." _Shoganai_. At least I didn't lose as much as [Shtetsu](!Wikipedia). Let's draw up a list of what's been lost and figuring out how to recreate them.

Thus began my short stay in computing hell...

The data loss was bad enough but the rest added insult to injury.
I wasted almost 2 days in the Ubuntu LiveCD trying to copy the `/dev/sda5` partition off the SSD before I realized that creating a [bzip2](!Wikipedia)-compressed disk image of an encrypted partition was, besides being very slow between the ancient laptop CPU & USB-2 connection, pointless and a huge waste of my time.
Then I tried to install Ubuntu 16.10 on the Dell Studio 17 only to encounter a baffling array of glitches, crashes, reboots, and extremely slow hard drive write speeds, which went away when I reinstalled 15.04, so apparently the Dell Studio 17 either is having internal hardware issues or has become so old that standard Linux distros are incompatible out of the box (which is just peachy) or the Ubuntu release is broken.
15.04 at least works, but the keyboard remained a PITA to use for anything, and I also had to fight issues with the old software in 15.04 (eg my [Xmonad](!Wikipedia) configuration doesn't work, the versions of Xmonad shipped are that ancient; and the Chromium is too old for the Ledger extension, so I can't use my bitcoins without finding someone who's backported Chromium 50 to 15.04...).
And I can't avoid using the broken laptop by plugging in a USB keyboard unless I want to also be unable to plug in any of the USB drives because it only has 2 USB ports and one is used by the Logitech trackball.
Further, Acer's website and support are notoriously bad, and they didn't help at all (after waiting 40 minutes in chat with a thorough description of my problem to get a price quote on a repair, they simply disconnected me).

Data recovery proved tricky.
I wrote down everything I could possibly think of which I had done in those 16 days, got copies of IRC logs for hints as to my activities, synced with the Github mirror of `gwern.net` to recover (most) of my writing during that period, and tried to match up with my offline activities.
I definitely lost 2 weeks of data on some of my self-experiments and other metrics, along with some new music I'd downloaded from [What.cd](!Wikipedia) and whose name I've forgotten, and some Geocities-related `char-RNN` training logs + checkpoints, all of which is sad but not a huge deal.
In retrospect, the data loss doesn't _seem_ to be that bad (although I've spent several days working through it) in part, ironically, for the same reasons that I had slackened on manual backups.
Of course, this assumes I haven't forgotten that I've forgotten having done something important...

So, [post-mortem](!Wikipedia "Root cause analysis") time - one always prepares to fight the last war.
What was the primary cause for this huge waste of time, money, and effort; what other causes worsened it; and how might all be prevented in the future?


Hopefully all this should prevent any recurrence or make dealing with it far less difficult. Nevertheless, it's cost me a ton of time, money, and stress. A lesson is learned but the damage is irreversible.


The ThinkPad arrived fortunately quickly on 1 December 2016.
I immediately began installing & copying over.
Despite the [certification of ThinkPad P70s with Ubuntu 14.04](https://certification.ubuntu.com/certification/hardware/201512-20417/), the install was not super-smooth.
I pulled out the default HDD with Windows on it (which did work when I booted it up to test) and put in my 1TB Samsung SSD, and toggled the graphics in the BIOS to "discrete" rather than "hybrid".
The Ubuntu 16.10 LiveCD wouldn't even boot (I tested the laptop thoroughly with the builtin BIOS tools and the optical disk drive was fine, just like everything else), so I've chucked it (it's now betrayed me twice, with the Dell _and_ ThinkPad so I think the CD is just broken) and I later burned another.

Installing with 15.04... initially went well but on rebooting gave me problems: it would freeze after [GNU GRUB](!Wikipedia) loaded Ubuntu.
After much puzzling, I learned there was a bootloader graphics problem - I could work around it by either using 'rescue' mode to bypass the GRUB splashscreen stuff and simply tell rescue mode to boot as normal, or I could type in the FS encryption password blind and then it would work!
After this, things went much smoother albeit with a ton of waiting on the USB-2 enclosures to sync with the SSD (the new enclosures not yet having arrived).
The audio required an upgrade of the kernel module `snd_hda_intel`, and I experienced problems when I tried to change the `ext4` filesystem mount options (apparently it no longer allows you to change options like `journal=writeback` in `/etc/fstab` as it will throw an error and dump you into a read-only root filesystem on boot...?).
Ubuntu 15.04 turns out to no longer be supported by Ubuntu, where "supported" means "won't even let you upgrade to the next OS version", so I had considerable difficulties figuring out how to tweak `apt` to get it to upgrade to 15.10 so it could then upgrade to 16.10, but eventually that worked out.
Then I began taking advantage of the 32GB of RAM by putting `/tmp/` & Firefox's cache into it.
Very nice.

I then established that Internet, Firefox, IRC, syncing `gwern.net`, torrents (using `rtorrent`), audio, video, [Mnemosyne](!Wikipedia "Mnemosyne (software)") (for [spaced repetition](/Spaced repetition)), backups to external drives & Backblaze, the trackball, email, and some other things are all working without any noticeable problems.
The WiFi works out of the box, surprisingly, not even requiring an `apt-get` installation of a proprietary driver.
Display brightness appears to not be supported based on `dmesg` errors, and I haven't checked laptop suspend yet.
The boot thing is still an issue but not a major one.

Hardware-wise, I am enjoying the ThinkPad so far.
The keyboard is decent with an acceptable layout, the screen is at least as good as (and may be somewhat larger than) the Acer was, the battery & SSD were easy to install and the hardware was not hostile at all with everything labeled, the BIOS is nicely featured, the 32GB RAM is very nice to have, the 4 USB ports save me from port starvation, I appreciate that the Ethernet port is in the _back_ of the laptop, the laptop feels sturdy, the lid overhangs slightly so it's easy to open which is a nice touch, my cat Oolong walking on the touchpad hasn't crashed X yet...
So far so good.

##### External links

- discussion:

    - [Google+](https://plus.google.com/103530621949492999968/posts/Xba6usohMFR)
    - [Twitter](https://twitter.com/gwern/status/802590832532529155)

#### Advanced Chess obituary

As automation and AI advance in any field, it will first find a task impossible, then gradually become capable of doing it at all, then eventually capable of better than many or most humans who try to do something, and then better than the best human.
But improvement does not stop there, as 'better than the best human' may still be worse than 'the best human using the best tool'; so this implies a further level of skill, where no human is able to improve the AI's results at all rather than get in the way or harm it.
We might call these different phases 'subhuman', 'human', 'superhuman', and 'ultrahuman'.

The interesting thing about this distinction is that each level has different practical implications.
At subhuman, the AI is unimportant and used largely in cases where performance doesn't matter much or where a human is unusable for some reason (such as environment).
Taking arithmetic as an example, Pascal's calculator was ingenious but sold only a handful of units and made correspondingly little difference.
Once the human phase is reached, then it may become an economic force to be reckoned with, as it will be better than many humans and have other advantages; this may prompt a global revolution in that field as all the humans adopt the new technology and use it to assist themselves.
A calculator as fast & accurate as the median human at arithmetic will be better than the median human because it is more systematically reliable. Here the mechanical calculator can take off, with de Colmar's Arithmometer selling millions of units. The calculator becomes a 'complement' to a human accountant or clerk, as it double-checks sums and by its reliability helps handle the escalating arithmetic needs of the industrial economy such as double-entry accounting & statistics for small businesses, corporations, researchers etc.
At the superhuman level, no longer does any human do the task on their own except for learning purposes or debugging; those humans now focus on things like when the task should be done or from what perspective it should be described.
The humans using it become more productive and more valuable and employment increases; they do not become unemployed because the accountant still needs to punch in the right numbers to the calculator to figure out the corporation's balance, and the (human) computers are still executing more complex algorithms than the calculator understands even if the calculator is now doing all the arithmetic - a pile of fancy electro-mechanical calculators could not replace the human computers for the Manhattan Project, they needed many people (often women) to handle the full workflow to answer various questions the physicists set up, check the answers at a higher level for sanity, etc.
At the ultrahuman level, the technology becomes autonomous in the sense that a human no longer contributes to it at all, and that occupation disappears.
The calculator, having developed to the level of a programmable digital computer, now fully obsoletes ; the 'computer' is fully unemployed and no longer exists as an occupation. The calculator has gone from 'complement' to 'substitute': it now fully replaces a computer.
No human does arithmetic or square roots for a living, nor do they even double-check the arithmetic results, 'computer' now means exclusively programmable digital computers', and the lower-skilled parts of occupations which formerly involved much arithmetic cease to exist and people are now employed for higher-skilled roles - eg accountants now specialize in international tax evasion rather than cranking through the balance sheet and verifying that all accounts balance to zero.

From the global perspective, the ultrahuman level is the ultimate goal of all technological development: to eliminate the need for any human involvement and allow unlimited expansion and efficiency.
Arithmetic done by a human is expensive, and can be afforded only in limited circumstances like accounting; but as arithmetic becomes cheaper, it allows for ever more complicated and arithmetic-heavy things to be done. FLOP for FLOP, computing a business's solvency in 1500 is far more valuable than calculating the critical mass of an atomic bomb in 1945, which is far more valuable than running agricultural statistics in 1960, which is far more valuable than animating a bouncing icon on your smartphone, but despite these steeply diminishing returns, humanity is vastly better off because arithmetic has become so cheap that unimaginable amounts of it can be used for anything at all.
For the occupants of a particular occupation, on the other hand, things may not be so rosy; forced out of a job, they may have trouble finding another one, particularly if they are old or unexceptional.
For them, the ideal phase was superhuman: the point at which their personal productivity was steeply increased by the technology and they can reap most of the gains, but where it's not *so* good that it replaces them entirely.

Chess has gone through a similar sequence of technological improvement.
Early mechanical automatons could not play a full game of chess at all.
Alan Turing wrote the first computer chess program in 1951, inaugurating the subhuman phase; the subhuman phase could be said to have lasted until Mac Hack Six in 1967 or Chess 4.6 in 1977, so perhaps 26 years.
The human phase saw the first defeat of a chess master in 1981, the first defeat of a grandmaster in 1988, and then as well known, last until the 1997 victory of Deep Blue, for a total of 20 years.
The superhuman phase begun in 1997 still allowed for the top grandmasters to occasionally beat a computer, depending on available hardware (not everyone could afford computing power on par with Deep Blue), rules and timesettings (computers perform much better than humans at short time controls like blitz due to their calculating and tactical advantage in avoiding blunders), how well the human had prepared 'anti-computer tactics', luck etc, but it's suggested that by 2004-2005, chess AIs were definitively superhuman and no human could systematically beat them.

However, the story does not end there. A grandmaster alone couldn't defeat a chess AI, but what about a grandmaster assisted by another chess AI? Kasparov proposed this "advanced chess" variant and the first tournament was run in June 1998; Kasparov only drew with Topalov instead of crushing him, because of Topalov's AI assistance. Both players played better than on their own. Indeed, in 2005-2007, the best advanced chess teams often had humans who weren't all that great at playing chess themselves, but were great at compiling game databases, and watching the chess AIs evaluate moves while intuiting where the AIs were weak and should be overridden. These advanced chess games were likely among the best chess games to ever be played (along with the best correspondence chess games).

As such, advanced chess has been employed (particularly in a rash of books/op-eds ~2013) as an exemplar of what increasing technological development may imply: not technological unemployment, but increasing partnership. The rising tide will lift all ships.

However, if advanced chess is going to be used this way, we should remember that after the superhuman phase, comes the ultrahuman phase, and ask how long the superhuman phase lasts in which 'advanced chess' is possible. Advanced chess players generally admit that at some point humans will cease to be net contributors; when was that?

The subhuman to human phase lasted 26 years, and the human to superhuman phase took 20 years; splitting the difference we might guess that the next phase, superhuman to ultrahuman would take a similar amount of time, 23 years, and that would put the transition point at 2020. Alternately, if each transition takes 6 years less and so the last transition takes 14 years, then it would happen in 2011.

when did man vs machine become impossible?

- Kasparov says the era of human competitiveness was ~10 years, 1994-2004: "Before 1994 and after 2004 these duels held little interest. The computers quickly went from too weak to too strong."
- 2005 Hydra: https://en.chessbase.com/post/adams-vs-hydra-man-0-5-machine-5-5

    Cowen 2013: "The last time a serious public contest was tried, in 2005, Hydra crushed Michael Adams 5.5 to 0.5. A half point indicates a draw, and Adams was lucky to come away with that one draw. In the other games he didn't put up much of a fight, even though he was ranked number seven in the world at the time-among humans."

https://en.wikipedia.org/wiki/Advanced_Chess
"advanced chess"/"cyborg chess"/"centaur chess"/"freestyle chess"

1. first tournament June 1998: a month before, he'd trounced Topalov 4-0. But the centaur play evened the odds. This time, Topalov fought Kasparov to a 3-3 draw. Kasparov:

    > Despite access to the "best of both worlds," my games with Topalov were far from perfect. We were playing on the clock and had little time to consult with our silicon assistants. Still, the results were notable. A month earlier I had defeated the Bulgarian in a match of "regular" rapid chess 4-0. Our advanced chess match ended in a 3-3 draw. My advantage in calculating tactics had been nullified by the machine.

    TODO: Topalov & Kasparov ELOs in 1998
3. Friedel 2000, of the first two advanced chess matches:

    > In the following year [1999] Vishy Anand played against Anatoly Karpov. Both players were assisted during the game by ChessBase 7.0 and the chess engine Hiarcs 7.32. Karpov was quite inexperienced at operating a computer, while Anand happens to be one of the most competent ChessBase users on the planet. The result was that we were witness to an (unplanned) experiment of man and computer vs man. Karpov didn't have a chance and was trounced 5:1 by his opponent. I am convinced that a player like Anand, using a computer to check crucial lines during the game, is playing at a practical level of over 3000 Elo points.

    TODO: Anand & Karpov ELOs in 1999
2. Kramnik 2002 https://en.chessbase.com/post/kramnik-on-advanced-che-and-fritz

    > How about a giant Internet qualification event including amateurs for next year's Advanced Chess?
    >
    > Well, it makes sense. It is clear that with the computer the difference in playing strength is reduced. Normally I can beat a player of 2600 without great difficulty, but if we both have a computer it is already not so easy. At the highest level it is not just about understanding, the very top players are also better at everything - calculation, imagination. With the computer that is no longer so useful.

    TODO: what was Kramnik's ELO in 2002 when he said this?
3. 2005 freestyle tournament: Cowen 2013:

    > ZackS defeated Russian grandmaster Vladimir Dobrov and his very well rated (2,600+) colleague, who of course worked together with the programs. Who was ZackS? Two guys from New Hampshire, Steven Cramton and Zackary Stephen, then rated at the relatively low levels of 1,685 and 1,398, respectively...Anson does not have any formal chess rating, but he estimates his chess skill at about 1,700 or 1,800 rating points, or that of a competent local club player. Nonetheless, he has done very well with his two quad-core laptops at the Freestyle level. Anson and his team would crush any grandmaster in a match. Against other teams, during one span of top-level play, Anson's team scored twenty-three wins against only one defeat (and twenty-seven draws) across four Freestyle tournaments and fifty-one games.

    So in 2005, 2600+computer < ~1550+computer or 1750+computer, implying the best teams had an human-part ELO advantage >1050

    > Dagh Nielsen estimated that the Freestyle teams were at least 300 Elo rating points better than the machines alone (a measurement of players' relative skill levels), although that was a few years ago. Nelson Hernandez estimates a 100-150 rating point advantage, which is like the difference between the number one player in the world and the number seventy-five player.

    So ~2010 ("a few years ago") Nielsen estimated the difference at 300; in 2013, Hernandez at 150.
4. Nickel, 2005: of the freestyle tournament

    > Just to give you clue, I would say that the strength of good Advanced Chess players must be something around Elo 3000. Frederic Friedel was right in his conclusions last year: "The level of play may be the highest ever seen at these time settings. There cannot be a doubt that a human player, even one of the top players in the world, would have no serious chance in such a field."

    The first chess engine, Rybka, reached ELo 3000 by ~2006 http://chessok.com/?p=21214
4. Vasik Rajlich 2009 http://rybkaforum.net/cgi-bin/rybkaforum/topic_show.pl?tid=10960 : the Rybka cluster may be slightly better than any other freestyle team (presumably due to its much greater computing capacity)
4. Ken Regan, 2012: near-perfect play may be ~3600 ELO (http://www.cse.buffalo.edu/~regan/papers/pdf/RMH11b.pdf); with Komodo in 2016 at 3358 ELO and regular ELO improvements at ~50 per year (not just from the computing power doubling; see eg https://en.chessbase.com/post/komodo-8-the-smartphone-vs-desktop-challenge ), that limit will be approached within 5 years, and hence advanced chess obsolete, setting an upper bound of ~2021 chess engines with standard hardware or 2016 chess engines with 32x that Komodo's computing power ((3600-3358)/50) (32x is not even that unreasonable, when Amazon EC2 will rent you a x1.32xlarge instance with 128 CPUs for \$12/hour)
4. Cowen 2013, based on interviewing top Advanced Chess players/teams: "Today, the top Freestyle players fear that the next or maybe even the current generation of programs (e.g., Rybka Cluster) will beat or hold even with the top Freestyle teams....Vasik Rajlich says that, to date, the gap between the programs and the top Freestyle teams has stayed more or less constant. The human element really does add something, at least for the time being, although he too wonders how long this will remain the case."
4. GM Hikaru Nakamura + Rykba (200 Elo weaker on 2008 laptop) vs 2014 Stockfish without opening/endgame books; Nakamura lost 3-1 https://www.chess.com/news/stockfish-outlasts-nakamura-3634
4. InfinityChess 2014: " InfinityChess Freestyle Battle 2014" tournament articles+writeups http://www.infinitychess.com/News/Freestyle%20Chess : general attitude that it's getting very difficult to beat a chess AI without extensive preparations and a surprising number of games decided by [human screwups like mouse misclicks](http://www.infinitychess.com/Page/Public/Article/DefaultArticle.aspx?id=162) (!). Evaluation of engine vs centaur is given in ["The Freestyle Battle 2014: Computer-based Chess with Houdini & Co", Nickel 2014](http://www.infinitychess.com/Page/Public/Article/DefaultArticle.aspx?id=118)

    > A few years ago, between 2005-2008, during the PAL/CSS-Freestyle-Tournaments on the ChessBase server, which offered prizes, this question was hotly debated, and answered with a "yes, but...". The results were in favor of the centaurs despite some occasional spectacular success of the machines. Here preparation played a significant role, because the superiority of the centaurs was more marked in round robin tournaments than in open tournaments with short term pairings. Specific opening choices, time management, structural knowledge, positional feeling, and deep analysis of critical variations in advance (going into the variations) were cornerstones of the centaur-strategy, even though one had to concede that the computers achieved a relatively high number of draws, particularly so when playing with White. Today, at a time when computer developments are rapidly taking place, and a new generation of chess engine has changed the chess world, the question of the role humans play in this battle can no longer be answered that clearly. A lot of chess commentary and video-livestreams at tournaments, in which engines (mostly Houdini) run parallel to the games, sometimes create the impression that the chess engines know it all. What, then, can a human do? However, in reality things look rather different as everybody knows who has ever tried to analyze positions, in which several candidate moves of apparently rather equal value are possible, with the help of a computer knows.
    >
    > ...*Centaurs against pure engines*
    >
    > In the Freestyle Battle 2014 the participants every round can choose whether they want to play as centaurs, which technically means having to enter the moves manually, or whether they let any UCI-engine play automatically (of course with a specifically prepared opening book). 16 of the 30 participants always play as centaurs; another 9 play mainly as centaurs, but in a few cases (when they had other obligations) employed an engine; 3 computer players occasionally tried their luck as centaurs, and only 2 players relied exclusively on the engines. Roughly speaking, 83% of the field are centaurs and 17% pure engine players. The engine players thus more often than not play against centaurs, and a third of all games is played between these two groups. Only in 10 from 265 games did computer programs play against each other.
    >
    > In the competition between centaurs and pure engines, which, however, does not affect the distribution of prizes, the centaurs lead 53,5 to 42,5 after 18 rounds: +24 / =59 / -13, which on average is one point in every ten games (5.5:4.5). In 54 of these 96 games the centaurs played with White, in 42 games they played with Black, and thus they had a certain advantage resulting from the random sequence of the games. This, however, might later be leveled. But the distribution of color is an important factor because the superiority of the centaurs strongly relies on the white pieces - of the 24 wins the centaurs scored against the engines 20 were achieved with White. The engines score 9 wins with White compared to 4 wins with Black. This means that according to the current trend centaurs have a 65 % winning chance with White, but only a 45 % winning chance with Black. This allows one to conclude that the advantage of the centaurs lies mainly in the exploitation of opening advantages, but that it almost vanishes if no opening advantage is gained.

    http://www.infinitychess.com/Page/Public/Article/DefaultArticle.aspx?id=141 "Freestyle Battle 2014: Hours of decision"

    > Engines 70 (46%) : Centaurs 82 (54%)

    Looks like 46% vs 54% translates to an Elo difference of 52 using a 2014 Stockfish Elo estimate of 3247: (3247 + 400*(82-70)) / (82+70)
4. 2015 GM Daniel Naroditsky + Rybka vs Stockfish match: expected score of Stockfish was 0.799, actual score was 3 wins & 1 draw; Naroditsky apparently made no positive contribution (judging from his comments and how the score is consistent with the expected score predicted by the chess AIs' ELOs) but was not experienced at advanced chess
4. Internet Chess Club ran a ["1st Ultimate Chess Championship"](https://www.chessclub.com/ucc) in October 2015 with 8 players. Of the 28 games in [the finals](https://www.chessclub.com/user/bits/pgn/UCC/1/UCC15-finals.pgn), 25 were drawn, so it's difficult to infer anything from the results. Of the 8 players, the top one was human+AI, the middle 6 were ~4 AIs (guessing from their profile pages; "jpsingh1972", "ComputerGOD"?, "Blitz-Masta") and 2 human+AIs ("RayJr", "Bookbuilder"?), and the last place was human+AI ("Gaon"). The winner of the tournament was Alvin Alcala (["ENGINEMASTER"](https://www.chessclub.com/finger/ENGINEMASTER); [as of 2014](http://www.infinitychess.com/Page/Public/Article/DefaultArticle.aspx?id=156 "Portrait of Alvin Alcala") he used Houdini 4/Komodo TCEC/Stockfish) with 1 win & 6 draws. [Bobby Ang asks how](http://www.bworldonline.com/content.php?section=Sports&title=draw-death-not&id=122530 "Draw death? Not!"):

    > Wow! three decisive results out of 28 games. I asked my fellow-admins in the ICC why there were so many draws and they replied that it wasn't for lack of trying. According to the admin who ran the tournament, the observation was that with all the powerful computer hardware and software around winning is basically impossible unless someone goes bonkers or is simply weak and has a bad computer or something.... Alvin's secret is that he built a huge database collection of human games, correspondence chess games, computer engine games and freestyle chess games, and put together several "trees" of opening analysis, similar to the method described by Alexander Kotov in "Think like a Grandmaster". Every move is a branch, and the possible replies to each move is a sub-branch.

    The few participants, small prizes, and high rate of draws suggests that the 'Ultimate Chess Championship' might not be drawing the best advanced chess players or represent the state of the art, but to the extent it does, it suggests that the AIs are now on par with the humans as Alcala won by a single game and almost all were draws.
4. InfinityChess has run a series of 8 monthly "Centaur Weekend Tourney" tournaments in 2015 for selection into a planned 2016/2017 freestyle tournament with human+AI, mixed, and AI-only participation. (The rules encourage either pure human+AI or engine play.) I can't find the final results, but the cumulative rankings of the 30 players after the 7th are available: http://infinitychess.com/Page/Public/Article/DefaultArticle.aspx?id=262 Pure centaurs took rankings #1-3, 9, 12, 18, & 21; while pure engines took 4-7, 10-11, 12, 15-17, 19, 22-27, 29-30; and mixes took 8, 12, 20, & 28. (Many of these rankings are ties, broken by ELO - 19 players have the same score of 3.5 due to 85% of games ending in draws.) Centaur advantages sound like they are down primarily to opening book preparation & finding 'novelties in advance', with the engines playing remarkably well - in earlier tournaments one certainly didn't find pure engines taking up almost the entire middle and threatening to enter the top 3. ELO ratings for InfinityChess's centaur-specific ELO ranking scale, which in this tourney ranges 2300-2686; on this particular ELO scale, centaurs average +100 points with average 2502(65.5) vs 2607(71.4) and a gap of 141 ELO between the best centaur & AI, with an average points of 3.75 vs 3.375 or 0.8 wins vs 0.4. Centaurs won 8 of 70 games, and AIs won 8 of 140 games; a multilevel model estimates a base probability of winning a game as 4.3% and being a centaur roughly doubles that to 8.7%. <!-- elo <- c(2641, 2655, 2686, 2500, 2500, 2500, 2500, 2622, 2476, 2500, 2300, 2560, 2500, 2565, 2643, 2500, 2619, 2646, 2500, 2618, 2666, 2500, 2500, 2450, 2480, 2498, 2496, 2500, 2508, 2500); points <- c(5.0,4.5,4.0,4.0,4.0,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.5,3.0,3.0,3.0,2.5,2.0); centaur <- rep(FALSE, 30); .0centaur[c(1:3, 9, 12, 18, 21, 8, 12, 20,  28)]  <- TRUE; wins <- c(3,2,1,1,1,2,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0); cwt7 <- data.frame(Player=1:30, ELO=elo, Points=points, Wins=wins, Centaur=centaur); t.test(ELO ~ Centaur, data=cwt7) -->

    CWT PGNs: http://infinitychess.com/Download/8th%20Centaur%20Weekend%20Tourney%20(119%20games)_267.zip http://infinitychess.com/Download/7th%20Centaur%20Weekend%20Tourney%20(104%20games)_256.zip http://infinitychess.com/Download/6th%20Centaur%20Weekend%20Tourney%20(118%20Games)_255.zip http://infinitychess.com/Download/5th%20Centaur%20Weekend%20Tourney%20(114%20Games)_254.zip http://infinitychess.com/Download/4th%20Centaur%20Weekend%20Tourney%20(110%20Games)_253.zip http://infinitychess.com/Download/3rd%20Centaur%20Weekend%20Tourney%20(105%20Games)_252.zip http://infinitychess.com/Download/2nd%20Centaur%20Weekend%20Tourney%20(126%20games)_223.zip http://infinitychess.com/Download/1st%20Centaur%20Weekend%20Tourney%20(112%20games)_222.zip



Kramnik 2002 https://en.chessbase.com/post/kramnik-on-advanced-che-and-fritz

> How would you fare with the computer against a player like Leko, Topalov or Anand if they were not using a computer?
>
> I would win, of course, and the other way around also. I cannot give you an exact performance rating, but it makes a huge difference. In classical chess it would probably be less profitable, but even there it makes a serious difference. In one-hour or 30 minute games it is absolutely decisive.
>
> Kasparov said (after the first Advanced Chess match) that Topalov with a computer would crush him without a computer.
>
> Yes, I agree. I never tried it, and I wouldn't like to do so. Maybe somebody else can go for this experiment. I don't know about "crush". It depends on style. I think that my style is so solid that even if someone is playing with a computer I can fight. But only fight and lose with a respectable score.
>
> ...How about a giant Internet qualification event including amateurs for next year's Advanced Chess?
>
> Well, it makes sense. It is clear that with the computer the difference in playing strength is reduced. Normally I can beat a player of 2600 without great difficulty, but if we both have a computer it is already not so easy. At the highest level it is not just about understanding, the very top players are also better at everything - calculation, imagination. With the computer that is no longer so useful.

http://smarterthanyouthink.net/excerpt/ _Smarter Than You Think_, Clive Thompson:

> In June 1998, Kasparov played the first public game of human-computer collaborative chess, which he dubbed "advanced chess," against Veselin Topalov, a top-rated grand master. Each used a regular computer with off-the-shelf chess software and databases of hundreds of thousands of chess games, including some of the best ever played. They considered what moves the computer recommended; they examined historical databases to see if anyone had ever been in a situation like theirs before. Then they used that information to help plan. Each game was limited to sixty minutes, so they didn't have infinite time to consult the machines; they had to work swiftly.
>
> Kasparov found the experience "as disturbing as it was exciting." Freed from the need to rely exclusively on his memory, he was able to focus more on the creative texture of his play. It was, he realized, like learning to be a race-car driver: He had to learn how to drive the computer, as it were-developing a split-second sense of which strategy to enter into the computer for assessment, when to stop an unpromising line of inquiry, and when to accept or ignore the computer's advice. "Just as a good Formula One driver really knows his own car, so did we have to learn the way the computer program worked," he later wrote. Topalov, as it turns out, appeared to be an even better Formula One "thinker" than Kasparov. On purely human terms, Kasparov was a stronger player; a month before, he'd trounced Topalov 4-0. But the centaur play evened the odds. This time, Topalov fought Kasparov to a 3-3 draw.
>
> In 2005, there was a "freestyle" chess tournament in which a team could consist of any number of humans or computers, in any combination. Many teams consisted of chess grand masters who'd won plenty of regular, human-only tournaments, achieving chess scores of 2,500 (out of 3,000). But the winning team didn't include any grand masters at all. It consisted of two young New England men, Steven Cramton and Zackary Stephen (who were comparative amateurs, with chess rankings down around 1,400 to 1,700), and their computers.
>
> Why could these relative amateurs beat chess players with far more experience and raw talent? Because Cramton and Stephen were expert at collaborating with computers. They knew when to rely on human smarts and when to rely on the machine's advice. Working at rapid speed-these games, too, were limited to sixty minutes-they would brainstorm moves, then check to see what the computer thought, while also scouring databases to see if the strategy had occurred in previous games. They used three different computers simultaneously, running five different pieces of software; that way they could cross-check whether different programs agreed on the same move. But they wouldn't simply accept what the machine accepted, nor would they merely mimic old games. They selected moves that were low-rated by the computer if they thought they would rattle their opponents psychologically.
>
> In essence, a new form of chess intelligence was emerging. You could rank the teams like this: (1) a chess grand master was good; (2) a chess grand master playing with a laptop was better. But even that laptop-equipped grand master could be beaten by (3) relative newbies, if the amateurs were extremely skilled at integrating machine assistance. "Human strategic guidance combined with the tactical acuity of a computer," Kasparov concluded, "was overwhelming."
>
> Better yet, it turned out these smart amateurs could even outplay a supercomputer on the level of Deep Blue. One of the entrants that Cramton and Stephen trounced in the freestyle chess tournament was a version of Hydra, the most powerful chess computer in existence at the time; indeed, it was probably faster and stronger than Deep Blue itself. Hydra's owners let it play entirely by itself, using raw logic and speed to fight its opponents. A few days after the advanced chess event, Hydra destroyed the world's seventh-ranked grand master in a man-versus-machine chess tournament.
>
> But Cramton and Stephen beat Hydra. They did it using their own talents and regular Dell and Hewlett-Packard computers, of the type you probably had sitting on your desk in 2005, with software you could buy for sixty dollars. All of which brings us back to our original question here: Which is smarter at chess-humans or computers? Neither. It's the two together, working side by side.

_Average is Over_, Tyler Cowen 2013

> As the programs improved, Freestyle chess circa 2004-2007 favored players who understood very well how the computer programs worked. These individuals did not have to be great chess players and very often they were not, although they were very swift at processing information and figuring out which lines of chess play required a deeper look with the most powerful programs. Today, the top Freestyle players fear that the next or maybe even the current generation of programs (e.g., Rybka Cluster) will beat or hold even with the top Freestyle teams. The programs, playing alone without guidance, may not be so easy for the human to improve upon. If the program's play is close enough to perfection, what room is there for the human partners to add wisdom?
>
> ...A series of Freestyle tournaments was held starting in 2005. In the first tournament, grandmasters played, but the winning trophy was taken by ZackS. In a final round, ZackS defeated Russian grandmaster Vladimir Dobrov and his very well rated (2,600+) colleague, who of course worked together with the programs. Who was ZackS? Two guys from New Hampshire, Steven Cramton and Zackary Stephen, then rated at the relatively low levels of 1,685 and 1,398, respectively. Those ratings would not make them formidable local club players, much less regional champions. But they were the best when it came to aggregating the inputs from different computers. In addition to some formidable hardware, they used the chess software engines Fritz, Shredder, Junior, and Chess Tiger. The ZackS duo operated more like a frantic, octopus-armed techno disc jockey than your typical staid chess player, clutching his hands around his head in tectonic concentration. They understand their programs - and presumably themselves - very, very well.
>
> Anson Williams is another top Freestyle player who doesn't have much of a background in traditional chess. Anson, who lives in London, is a telecommunications engineer and software developer. A slim young man of Afro-Caribbean descent, he loves bowling and Johann Sebastian Bach. Fellow team member Nelson Hernandez describes Anson as laconic, very religious, and dedicated to his craft.
> Anson does not have any formal chess rating, but he estimates his chess skill at about 1,700 or 1,800 rating points, or that of a competent local club player. Nonetheless, he has done very well with his two quad-core laptops at the Freestyle level. Anson and his team would crush any grandmaster in a match. Against other teams, during one span of top-level play, Anson's team scored twenty-three wins against only one defeat (and twenty-seven draws) across four Freestyle tournaments and fifty-one games.
> Along with Anson and Nelson Hernandez, the team is filled out by Yingheng Chen. In her late twenties, she is a graduate from the London School of Economics, not a traditional chess player at all, and now working in finance. She is Anson's girlfriend and has learned the craft from him.
> Nelson Hernandez defended his passion for the game thus:
>> This may sound like easy work compared to OTB [over-the-board] chess but it really isn't when you consider that your opponent can do the same things and thus has a formidable array of resources as well. It is also quite a trick to orchestrate all these things in real time so as to play the best possible chess....
>> My role... is rather specialized. During these tournaments I am minimally involved and spectacularly indolent as I watch Anson demolish his opponents. Between tournaments I am very actively involved in his opening preparation. This is paradoxical, actually, because I am not a chess player. I approach the game entirely from an analytic, computer-oriented point of view.
>
> Anson, when playing, is in perpetual motion, rushing back and forth from one machine to another, as Freestyle chess is, according to team member Nelson, "all about processing as much computer information as rapidly as possible."
> Vasik Rajlich, the programmer of Rybka, considers the top players to be "genetic freaks," though he stresses that he means this in a positive manner; he is a top Freestyle player himself. He sees speed and the rapid processing of information as central to success in Freestyle. In his view, people either have it or they don't. The very best Freestyle players do not necessarily excel at chess and they pick up their Freestyle skills rather rapidly, sometimes within twenty hours of practice. He refers to Dagh Nielsen, one of the top Freestyle players, as operating in a rapid "swirl" during a Freestyle game.
> Some players enter these events using a chess engine only, set on autopilot and not using any additional human aid. These "teams" do not take the top prizes, and they are looked down upon by the more enthusiastic partisans of Freestyle. Dagh Nielsen estimated that the Freestyle teams were at least 300 Elo rating points better than the machines alone (a measurement of players' relative skill levels), although that was a few years ago. Nelson Hernandez estimates a 100-150 rating point advantage, which is like the difference between the number one player in the world and the number seventy-five player.
>
> ...Top American grandmaster Hikaru Nakamura was not a huge hit when he tried Freestyle chess, even though he was working with the programs. His problem? Not enough trust in the machines. He once boasted, "I use my brain, because it's better than Rybka on six out of seven days of the week." He was wrong.
>
> ...This Freestyle model is important because we are going to see more and more examples of it in the world. Don't think of it as an age in which machines are taking over from humanity. After all, the machines embody the principles of man-machine collaboration at their core-even when they are playing alone...Secret teams. Board games. Code names. Does this all sound a little too much like child's play? Could the Freestyle chess model really matter all that much? Am I crazy to think direct man-machine cooperation, focused on making very specific evaluations or completing very specific tasks, will revolutionize much of our economy, including many parts of the service sector? Could it really be a matter of life or death?...What are the broader lessons about the Freestyle approach to working or playing with intelligent machines? They are pretty similar to the broader lessons about labor markets from chapters two and three:
>
> 1. Human-computer teams are the best teams.
> 2. The person working the smart machine doesn't have to be expert in the task at hand.
> 3. Below some critical level of skill, adding a man to the machine will make the team less effective than the machine working alone.
> 4. Knowing one's own limits is more important than it used to be.

["Can A GM And Rybka Beat Stockfish?"](https://www.chess.com/article/view/how-rybka-and-i-tried-to-beat-the-strongest-chess-computer-in-the-world), 15 May 2015, GM Daniel Naroditsky:

> When Tyson wrote to me in May, he had the experiment planned out: I would play a four-game match against Stockfish 5 (currently rated 3290, 13 points above Houdini 4) using the 2008 version of Rybka (rated approximately 3050).

TODO: Naroditsky's ELO

3 losses, 1 draw

3290 vs 3050; 240 ELO point difference https://en.wikipedia.org/wiki/Elo_rating_system
1 / (1 + 10^((3050-3290)/400)) = 0.799
"A player's expected score is his probability of winning plus half his probability of drawing. Thus an expected score of 0.75 could represent a 75% chance of winning, 25% chance of losing, and 0% chance of drawing."
So Stockfish would be expected to score almost exactly as it did against Rybka+Naroditsky (3 victories, 1 loss or tie)


http://marginalrevolution.com/marginalrevolution/2013/11/what-are-humans-still-good-for-the-turning-point-in-freestyle-chess-may-be-approaching.html
http://www.cse.buffalo.edu/~regan/chess/fidelity/FreestyleStudy.html
http://rybkaforum.net/cgi-bin/rybkaforum/topic_show.pl?tid=25469

https://rjlipton.wordpress.com/2012/05/31/chess-knightmare-and-turings-dream/

> My work also hints that the Elo rating of perfect play may be as low as 3600. This is not far-fetched: if Anand could manage to draw a measly two games in a hundred against any perfect player, the mathematics of the rating system ensure that the latter's rating would never rise above 3500, and if Gelfand could do it, 3400. Perfect play on both sides is almost universally believed to produce a draw, even after a few small slips. All this raises a question:

> As Ken Regan says, the present advantage of computers is roughly 400 elo points (which is an immense advantage).

https://en.wikipedia.org/wiki/Magnus_Carlsen peak rating 2882
https://en.wikipedia.org/wiki/Chess_engine "According to one survey,[citation needed], the top engines have been increasing in strength by an average of 67 Elo per year since 1986."
highest chess engine rating: Komodo 3358
1 / (1 + 10^((2882-3358)/400)) = 0.94

another way to put it is that Regan says that perfect play may be ~3600 ELO. Komodo is then 242 points away, and historically chess engines increase at 67 points per year, so perfect play would be reached in 4+ years. by definition, then advanced chess becomes pointless


http://www.nybooks.com/articles/2010/02/11/the-chess-master-and-the-computer/ 2010

> My hopes for a return match with Deep Blue were dashed, unfortunately. IBM had the publicity it wanted and quickly shut down the project. Other chess computing projects around the world also lost their sponsorship. Though I would have liked my chances in a rematch in 1998 if I were better prepared, it was clear then that computer superiority over humans in chess had always been just a matter of time. Today, for \$50 you can buy a home PC program that will crush most grandmasters. In 2003, I played serious matches against two of these programs running on commercially available multiprocessor servers-and, of course, I was playing just one game at a time-and in both cases the score ended in a tie with a win apiece and several draws.
> ...There have been many unintended consequences, both positive and negative, of the rapid proliferation of powerful chess software. Kids love computers and take to them naturally, so it's no surprise that the same is true of the combination of chess and computers. With the introduction of super-powerful software it became possible for a youngster to have a top- level opponent at home instead of needing a professional trainer from an early age. Countries with little by way of chess tradition and few available coaches can now produce prodigies. I am in fact coaching one of them this year, nineteen-year-old Magnus Carlsen, from Norway, where relatively little chess is played.
>
> The heavy use of computer analysis has pushed the game itself in new directions. The machine doesn't care about style or patterns or hundreds of years of established theory. It counts up the values of the chess pieces, analyzes a few billion moves, and counts them up again. (A computer translates each piece and each positional factor into a value in order to reduce the game to numbers it can crunch.) It is entirely free of prejudice and doctrine and this has contributed to the development of players who are almost as free of dogma as the machines with which they train. Increasingly, a move isn't good or bad because it looks that way or because it hasn't been done that way before. It's simply good if it works and bad if it doesn't. Although we still require a strong measure of intuition and logic to play well, humans today are starting to play more like computers.
>
> The availability of millions of games at one's fingertips in a database is also making the game's best players younger and younger. Absorbing the thousands of essential patterns and opening moves used to take many years, a process indicative of Malcolm Gladwell's "10,000 hours to become an expert" theory as expounded in his recent book _Outliers_. (Gladwell's earlier book, _Blink_, rehashed, if more creatively, much of the cognitive psychology material that is re-rehashed in _Chess Metaphors_.) Today's teens, and increasingly pre-teens, can accelerate this process by plugging into a digitized archive of chess information and making full use of the superiority of the young mind to retain it all. In the pre-computer era, teenage grandmasters were rarities and almost always destined to play for the world championship. Bobby Fischer's 1958 record of attaining the grandmaster title at fifteen was broken only in 1991. It has been broken twenty times since then, with the current record holder, Ukrainian Sergey Karjakin, having claimed the highest title at the nearly absurd age of twelve in 2002. Now twenty, Karjakin is among the world's best, but like most of his modern wunderkind peers he's no Fischer, who stood out head and shoulders above his peers - and soon enough above the rest of the chess world as well.
>
> ...This is not to say that I am not interested in the quest for intelligent machines. My many exhibitions with chess computers stemmed from a desire to participate in this grand experiment. It was my luck (perhaps my bad luck) to be the world chess champion during the critical years in which computers challenged, then surpassed, human chess players. Before 1994 and after 2004 these duels held little interest. The computers quickly went from too weak to too strong. But for a span of ten years these contests were fascinating clashes between the computational power of the machines (and, lest we forget, the human wisdom of their programmers) and the intuition and knowledge of the grandmaster.
>
> ...Having a computer partner also meant never having to worry about making a tactical blunder. The computer could project the consequences of each move we considered, pointing out possible outcomes and countermoves we might otherwise have missed. With that taken care of for us, we could concentrate on strategic planning instead of spending so much time on calculations. Human creativity was even more paramount under these conditions. Despite access to the "best of both worlds," my games with Topalov were far from perfect. We were playing on the clock and had little time to consult with our silicon assistants. Still, the results were notable. A month earlier I had defeated the Bulgarian in a match of "regular" rapid chess 4-0. Our advanced chess match ended in a 3-3 draw. My advantage in calculating tactics had been nullified by the machine...This experiment goes unmentioned by Russkin-Gutman, a major omission since it relates so closely to his subject. Even more notable was how the advanced chess experiment continued. In 2005, the online chess-playing site Playchess.com hosted what it called a "freestyle" chess tournament in which anyone could compete in teams with other players or computers. Normally, "anti-cheating" algorithms are employed by online sites to prevent, or at least discourage, players from cheating with computer assistance. (I wonder if these detection algorithms, which employ diagnostic analysis of moves and calculate probabilities, are any less "intelligent" than the playing programs they detect.)
>
> Lured by the substantial prize money, several groups of strong grandmasters working with several computers at the same time entered the competition. At first, the results seemed predictable. The teams of human plus machine dominated even the strongest computers. The chess machine Hydra, which is a chess-specific supercomputer like Deep Blue, was no match for a strong human player using a relatively weak laptop. Human strategic guidance combined with the tactical acuity of a computer was overwhelming.
>
> The surprise came at the conclusion of the event. The winner was revealed to be not a grandmaster with a state-of-the-art PC but a pair of amateur American chess players using three computers at the same time. Their skill at manipulating and "coaching" their computers to look very deeply into positions effectively counteracted the superior chess understanding of their grandmaster opponents and the greater computational power of other participants. Weak human + machine + better process was superior to a strong computer alone and, more remarkably, superior to a strong human + machine + inferior process.

https://en.chessbase.com/post/freestyle-tournament-advice-from-an-expert "Free survival tips for the \$16,000 Freestyle Tournament" Arno Nickel March 2005

> Gary Kasparov, the spiritual father of Advanced Chess, which he proposed in 1998, saw his views confirmed by the 1st Freestyle Tournament. He wrote about this in his New In Chess column (NiC Magazine 5/2005, p. 96f). "At first the results [of the PAL/CSS Freestyle tournament] seemed quite predictable. Even the strongest computers were eliminated by IMs and GMs using relatively weak machines to avoid blunders."
>
> Three semi-finalists were indeed grandmasters, working with computers, but the fourth player, 'ZackS', who eventually won the event, turned out to be two American amateurs, both rated under 1700. Kasparov points out that the ZackS team consistently played the Grunfeld and the Najdorf, using the machine to pick the right lines. "This could be the postmodern way to play the opening," writes Kasparov, "using database statistics and the machine's 'instincts'. And they beat GMs with this technique."
>
> Just to give you clue, I would say that the strength of good Advanced Chess players must be something around Elo 3000. Frederic Friedel was right in his conclusions last year: "The level of play may be the highest ever seen at these time settings. There cannot be a doubt that a human player, even one of the top players in the world, would have no serious chance in such a field."

https://en.chessbase.com/post/a-history-of-cheating-in-chess-4 "A history of cheating in chess (4)" 2000 Frederic Friedel

["Playing Chess With the Devil"](https://rjlipton.wordpress.com/2015/07/28/playing-chess-with-the-devil/), Regan & Lipton:

> Potentially computers can play a decisive role at the very highest levels of chess. This was made very clear to me during the Super GM tournament in Las Palmas in 1997...At this point Kasparov went into a deep think. Jan Timman started to speculate whether White couldn't play the very forceful 20.g4...The game lasted six hours, Anand defended very tenaciously and at around 10 p.m., much to the disappointment of Kasparov, a draw was agreed. When he left the stage Garry spotted me and walked straight over. "I couldn't win it, could I, Fred?" he asked, with a troubled look on his face. It was a bit shocking: the world champion and best player of all times consulting a chess amateur, asking for an evaluation of the game he has just spent six hours on! Naturally Garry wasn't asking me, he was asking Fritz. He knew I would have been following the game with the computer. "Yes, you had a win, Garry. With 20.g4!" My answer vexed him deeply. "But I saw that! It didn't work. How does it work? Show me." He and Anand listened in horror while Juri dictated the critical lines...The next day Garry did an interview with the German magazine Der Spiegel. He spoke about "Advanced Chess", a new concept he has developed, which involves playing games in real time with computer assistance. He used the game against Anand from the previous day to illustrate his point. This is what he had to say: "That game provides us with new arguments for Advanced Chess. If I had had a computer yesterday, I would give you the full line with 20.g4 within five minutes. Maybe less. I would enter g4 and check all the lines. I know where to go. It would give me the confidence to play moves like this. Can you imagine the quality of the games, the brilliancy one could achieve?"
>
> In the time since those remarks there have been two Advanced Chess matches in Len, Spain. In the first Kasparov was unable to defeat Bulgarian GM Veselin Topalov, who made efficient use of Fritz to defend against the world champion. The match ended in a 3:3 draw, although Kasparov had just demolished Topalov 5:1 in a match without computers. In the following year Vishy Anand played against Anatoly Karpov. Both players were assisted during the game by ChessBase 7.0 and the chess engine Hiarcs 7.32. Karpov was quite inexperienced at operating a computer, while Anand happens to be one of the most competent ChessBase users on the planet. The result was that we were witness to an (unplanned) experiment of man and computer vs man. Karpov didn't have a chance and was trounced 5:1 by his opponent. I am convinced that a player like Anand, using a computer to check crucial lines during the game, is playing at a practical level of over 3000 Elo points.

> One-on-one the programs totally dominate the humans-even on laptops programs such as Stockfish and Komodo have Elo ratings well above 3100 whereas the best humans struggle to reach even 2900-but the human+computer "Centaurs" had better results than the computers alone. In the audience were representatives of defense and industrial systems that involve humans and computers. Ken got into freestyle chess not as a player but because of his work on chess cheating-see this for example. Freestyle chess says "go ahead and cheat, and let's see what happens..." The audience was not interested in cheating but rather in how combining humans and computers changes the game. While chess programs are extremely strong players, they may have weaknesses that humans can help avoid. Thus, the whole point of freestyle chess is:
>
>> Are humans + computers > computers alone?
>
> That is the central question. Taken out of the chess context it becomes a vital question as computers move more and more into our jobs and our control systems. The chess context attracts interest because it involves extreme performance that can can be precisely quantified, and at least until recently, the answer has been a clear "Yes."

http://www.infinitychess.com/Page/Public/Article/DefaultArticle.aspx?id=83 centaur-only Elo rankings

http://www.cse.buffalo.edu/~regan/papers/pdf/RBZ14aaai.pdf http://www.cse.buffalo.edu/~regan/chess/fidelity/FreestyleStudy.html

> The PAL/CSS "Freestyle" dataset comprises 3,226 games played in the series of eight tournaments of human-computer tandems sponsored in 2005-2008 by the PAL Group of Abu Dhabi and the German magazine Computer-Schach und Spiele. All of these games were analyzed with Stockfish 4 in the same mode as for CEGT. ...unfortunately no Freestyle events of comparable level and prize funds have been held between 2008 and a tournament begun by InfinityChess.com in February 2014 through April 10.
>
> ...Figure 9 measures that the two CEGT and three PAL/CSS data sources are respectively close to each other, that personal computer engines under similar playing conditions were significantly stronger in 2013 than in 2007-08, and that the human-computer tandems were significantly ahead of the engines playing alone even without aggregating the events together.  The 2-sigma confidence intervals are the empirically-tested "adjusted" ones of (Regan and Haworth 2011); we show them to four digits although the rating values themselves should be rounded to the nearest 5 or 10.

computer: CEGT 2007: 3009
computer CEGT 2008: 2963
computer: CEGT all (2007-2008): 2985
computer: TCEC (2013): 3083
computer: Komodo 2016: 3358
PAL/CSS 2005: 3102
PAL/CSS 2006: 3086
PAL/CSS 2008: 3128
human-computer: PAL/CSS all (2005-2008): Elo 3106


(3083-2985) / (2013-2005.5) = 13 Elo points per year
Komodo 2016 vs CEGT 2007: (3358 - 3009) / (2016-2007) = 38 Elo points per year
PAL/CSS 2005 vs PAL/CSS 2008: (3128-3102)/(2008-2005) = 8.6 Elo points per year

baseline year 2008:
computer trend: 2963 + 38*x
human-computer trend: 3128 + 8.6*x

2963 + 38*x = 3128 + 8.6*x
x=5.6 or 2013.6

or comparing to the 2014 InfiniChess: 52 Elo difference between centaurs and engines
2008 difference: 3128 PAL/CSS vs CEGT 2963: 165 Elo difference
165->52 from 2008->2014 implies the gap is shrinking by 18 Elo points per year; so 52 Elo would take 2.8 years, putting crossover at 2016.8

2007 PAL/CSS group interview http://www.rybkachess.com/docs/freestylers_version_2.htm differences mentioned were 53%, 60%, 75%, 80%, and 100-150 Elo

> Q. It's pretty evident now that a top centaur combination is stronger than an unassisted engine. How would you quantify this difference? By what margin could you win a match against the newest Rybka running on your strongest machine and using Noomen's latest RybkaII.ctg opening book?
>
> Dagh Nielsen: I would not dare to try and quantify the difference in playing level. Personally, if I had an engine running on auto, I would be horrified about the risk that a type of position is entered that the machine simply will not play very well on its own. An additional advantage for the centaur is that he can identify critical situations. Perhaps at some point in a sharp game, the game will essentially be won, drawn or lost within a span of, say, 5 moves. Also, I don't think pure engines can be expected to react properly on tricky novelties, and I am not really aware of any method to ensure that you will be the last one leaving preparation in every single game.
>
> Arno Nickel: Knowing that Rybka plays with its own opening book might be a decisive advantage for the centaur. Further on the time-management of unassisted engines is one of their weaknesses. These two points should guarantee a score of about 66%. But without knowing Rybka's opening book the score should not be more than 60%.
>
> Jiri Dufek: I can predict, that Xakru team under this condition probably scored about 70-80% (match with minimum 12 games).
>
> Nelson Hernandez: Quantifying this difference is difficult to do as a lot depends on the centaur's technique (see #3 in previous paragraph). This is further muddled by the relative strength of each side's opening book, hardware, access to EGTBs etc and other factors previously mentioned. However my general impression is that, all other things being equal, a top centaur has a 100-150 ELO advantage. You can calculate how that translates into a success rate. An indifferent centaur might have no advantage at all.
>
> Nolan Denson: Centaur don't walk blindly into the well know traps. Most programs when following books made for it do not always follow the best line. - 75%
>
> Jochen Rindfleisch: As the most recent tournament has shown, only the best - or most lucky? - centaur teams perform better than a well adjusted engine. So - taking a wild guess now - I would hope for 53% in a Kaputtze/Rybka match, if using identical hardware for analysis.
>
> Nick Carlin: 60-40 is my guess, I suspect that the centaur advantage will come especially in the end game, where human assistance can avoid losses and win games that engines might draw. An example is in some rook endings where the engine will push a pawn to the seventh rank and obtain a draw in a won position.  The missed win is coming through leaving the pawn on the sixth rank and bringing the King up to support its coronation.
>
> Eros Riccio: the advantage(s) of the combination Human(s) + engine(s) is obvious: In general, an engine alone is "stupid". It plays random openings, wastes precious time on forced moves, stubbornly wants to play for the win, sometimes forcing the position and losing, when a draw would have been enough to qualify... so, relying on an automatic engine may be quite risky... a human instead, may control all those things which an engine can't. The only advantage of automatic engine I can see, is with little time left in difficult positions, as there is no risk of mouse slips and losing on time.

#### Uninstallling League of Legends {#zzz}
I've decided to stop playing League of Legends for a while, putting a sober finale to one of my longuest and most extended gaming experience I've ever had. The whole game has became truly bogus and smashed competitive plays at lower elo.
* The elo system is rigged at the point is that it has become almost ludicrous to improve, stacking players forever.
* League's community is one of the most woeful community online games has to offer: from 12 years old to stormy players, the climate operating during games is distastful at every level.

I really hope I can get back at it at a later date, with more confidence to see that Riot Games, the Editor, to have addressed these issues that are killing some of the dedicated ones.